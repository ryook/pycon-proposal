2017,https://pycon.jp/2017/ja/proposals/vote/4/,How to write functions in Python (ja),"Pythonの関数は簡単に書けます。さらに、ポイントや落とし穴を押さえるとより良い関数を書くことができます。この発表では、より良い関数の書き方を解説し、より良いPythonプログラマを目指します。 Pythonで関数を記述する上で押さえるべきポイント、落とし穴を解説し、よりPythonらしい関数を書けるようになること。 # Not Pythonesque, Be PythonicPythonの関数について、『Effective Python』や『Effective Debugging』の翻訳本の査読で得た知識と実際の業務で得たコツを紹介する。* 初めて関数を書くとき（定義、テスト）    * 命名規則    * テスト* 急に要件が増えたとき（リファクタリング、拡張）    * 引数    * 返り値の返し方* 状態のある関数    * クロージャー    * `__call__`を実装したクラス",False
2017,https://pycon.jp/2017/ja/proposals/vote/3/,帰ってきた「PyQtで始めるGUIプログラミング」PyQt5対応改訂版 (ja),6年を経過してなお、SlideShare上でBookmarkが付く、PyCon JP 2011にて紹介したPyQtについての入門セッションが、PyQt5対応で帰ってきます。当時はPyQt4を対象にチュートリアル形式で発表しましたが、今回はPyQt5対応、GUIプログラミングはもちろんのこと、ネットワーク、マルチメディア系なども含むPyQtならではの便利な機能も積極的に紹介します。 Pythonは広範囲に利用できる強力なプログラミング言語です。これまでシステム管理系、Webアプケーション開発、そして現在ではデータ分析やAIプログラミングに利用されるように、その時代に合わせて利用シーンとユーザベースを拡大してきました。バイトコードインタプリタであるPythonがこのような適応性を持っているのは、強力なサードパーティー製拡張モジュールによるものであることに疑いの余地はないでしょう。このセッションでは、普段あまり言及されることのない強力な拡張モジュールであるPyQtを利用したプログラミングの概要を紹介し、これを用いたツール開発を開始できるようになることを目的とします。 PyQtは非常に強力かつ大規模なモジュールの集合として提供されています。QtはGUI分野で著名ですが、それ以外にもネットワーク、各種データコレクション・コンテナ、マルチメディア関連、Webインタフェースなど、多くの便利な機能が備わっています。これらの機能はQtCoreと呼ばれる仕組みにしたがって、統一的なAPIが定義されており、利用方法の基本部分を一旦理解してしまえば、あとは各コンポーネントの固有部分だけを参照しながら利用の幅を広げていける、優れた設計になっています。このセッションでは、QtのPythonバインディングであるPyQtを取り上げ、その機能とプログラミングモデルを紹介・解説し、Qt が持つ強力な機能を活用したツール類の開発をスタートできるように導入部分を提供します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/1/,How to write python NoSQL database driver (ja),Python データーベースドライバーの作り方(NoSQL編） Python でのバイナリーデータの取り扱いに慣れる PyConJP 2016 において  「Python データーベースドライバーの作り方」というタイトルでRDBMS のドライバーを書いた経験を発表しました。[https://gist.github.com/nakagami/bfbe98d62377f3f4554121ab161ae8c9](https://gist.github.com/nakagami/bfbe98d62377f3f4554121ab161ae8c9)その後、RDBMS でないデーターベースのドライバーをいくつか Python で書いたので今回はそれらについて紹介をしながら、そこでやり取りされるデータ（ネットワークプロトコル）の話をします。- Redis [https://github.com/nakagami/toyredis](https://github.com/nakagami/toyredis)- MongoDB [https://github.com/nakagami/nmongo](https://github.com/nakagami/nmongo)- Neo4j [https://github.com/nakagami/minibolt](https://github.com/nakagami/minibolt)- Cassandra [https://github.com/nakagami/minicql](https://github.com/nakagami/minicql)紹介するドライバーの github リポジトリにスターを付けてくれると私が喜びます。,False
2017,https://pycon.jp/2017/ja/proposals/vote/6/,PuLPとPandasを活用した組合せ最適化 (ja),最適化のライブラリとデータ分析のライブラリを組合せることにより、わかりやすいモデルを作成できます。実際に進行中のプロジェクトを例にして、具体的な手法を見ていきます。 ・組合せ最適化が社会でどのように役立っているか ・わかりやすい組合せ最適化のモデルの作り方 社会の課題を数学で解決する数理最適化を紹介する。  2つの課題に対し、数理モデルを説明し、Pythonモデルを解説する。  課題1) 輸送最適化問題  ・倉庫から工場へ製品を輸送する費用の最小化を行う  課題2) 小学校の時間割作成  ・様々な条件を満たす、小学校の時間割を作成する  Pandasを用いることで、最適化のモデルが理解しやすくなることを具体例を用いて説明する。,False
2017,https://pycon.jp/2017/ja/proposals/vote/116/,Python におけるドメイン駆動設計(戦術面)の勘どころ (ja),ウェブ開発やロボット開発でドメイン駆動設計を適用してきた体験を元に、Python でドメイン駆動設計を実践する際によくある課題や失敗、役に立つ設計やライブラリ、考え方についてお話します。 Python 開発にドメイン駆動設計を適用する場合に、すぐに役立つ実践パターンが学べます。 ウェブ開発やロボット開発でドメイン駆動設計を適用してきた体験を元に、Python でドメイン駆動設計を実践する際によくある課題や失敗、役に立つ設計やライブラリ、考え方についてお話します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/5/,ベンリに使おう変数アノテーション - typing.pyとの楽しいお付き合い (ja),変数アノテーションを実行時に読む方法から、実用的な活かし方までを解説。 (English slides + Japanese speech) 変数アノテーションを実行時に読む方法から、実際のプログラムで使う方法、またPythonにおけるinstanceやtype、classの関係性を知る Python 3.6から、変数定義に型を付ける変数アノテーションが使えるようになった。「アノテーション」というと、mypyやPyCharmのような静的型チェッカーが読んでくれたり、開発者間の意思疎通に使えたりと便利だが、実行時には無視される言わば飾りのようなものというイメージを持ちがちである。しかしこれはただの飾りではない。我々の書くコードにおいても便利な道具として活用できる、今あらためて注目したい言語仕様…それが変数アノテーションである。本発表では、はじめに - 変数のアノテーションを読む方法からスタートし、 - 標準ライブラリ `typing.py` で定義された型の取扱い方 - Generic型・Built-in型・Union（直和）型・その他の判別方法といった重要なポイントを解説した後、Go言語ライクな洗練されたJSON読み書き処理を実現するラッパーの実装までを解説する。なお、本発表で紹介するJSON読み込みラッパーを実装したもの（書き込みは未実装）は[すでに公開されている](https://github.com/puhitaku/typedmarshal)ため、興味のある方はそちらも参照されたい。-> [puhitaku/typedmarshal - https://github.com/puhitaku/typedmarshal](https://github.com/puhitaku/typedmarshal),False
2017,https://pycon.jp/2017/ja/proposals/vote/92/,はじめてのPython学習でつまずいてしまうところ (ja),PyQ（ https://pyq.jp ）というオンラインPython学習サービスの問題開発とサポートに従事しています。リリースまでの間、プログラム未経験の方々に問題を解いてもらい、沢山のフィードバックをいただきました。その時に詰まってしまった箇所と、どのように説明したらわかっていただけたかを紹介します。 Pythonの初級部分でよく出た質問をまとめました。プログラム未経験の方に教える時に、どのような解説をするとわかっていただけたかをみなさんと共有したいです。 1年前どのような方針で初学者の方にPythonを教えるべきか考え、問題を作成しました。 完全にPython初学者のモニターさんにわからない箇所はすぐに質問してくださいとお願いし、3ヶ月に及ぶプログラミング学習を行っていただきました。 どのような方針で問題を作ったか、もらった質問、それに対する回答を紹介します。その後の問題修正についてなどお話します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/117/,How (and Why) We Speak in Unicode (en),"A brief history of ascii, encoding, and unicode and how unicode and character encoding is done both wrong and right with examples in python 2.7 and 3.x. 1. To educate about the history and cultural significance of ascii, unicode, and encodings such as UTF-8, UTF-16, and Latin-1.2. To educate how to understand and use encoding in modern python. Since the Tower of Babel, humanity has spoken in different languages and fumbled to translate from one to another. When it came time to write and display those languages in software, those fumbles continued with inconsistent encodings and standards.Then came Unicode. A standard that answers the needs to multiple encodings while solving the issues of translating between them. Today we can ""shrug"", ""肩をすくめる"", "" ¯\\\_(ツ)_/¯"", and ""🤷🏽‍♀️"" all in the same sentence thanks to Unicode.This talk is about the history of encoding and unicode, and how we reason the differences between the two with examples in python.",False
2017,https://pycon.jp/2017/ja/proposals/vote/115/,Pythonで無駄な仕事を減らしたい、そして同僚の無駄な仕事も減らしたかった (ja),職場の面倒な作業の解消になぜPythonを使うのか、どうやって作ったスクリプトを同僚に使ってもらったのか。理解あるプログラマもいねぇ、予算もねぇ、そんな状況でどのようにして職場全体への展開を行っていたか。PythonとOSSのGitHubクローンのGitBucketを使って行った施策について失敗・成功含めてご紹介します。 インフラエンジニアを中心とした、いわゆる日本的企業の中での業務改善活動の一例を紹介させていただきます。これからPythonを学ばれようとしている方には、業務改善の一例を紹介し学習のきっかけとしていただければと思います。また、すでに触られている方にはさらなる社内展開に向けてのイメージを持っていただければと思います。 Pythonを使った業務改善と、それをどのようにして社内で展開していったかを紹介します。WEB系企業等、技術に理解がある会社ではなくいわゆるSIerに近い会社では**作ってみた**だけではなかなか同僚に利用してもらえません。そのような会社に於いて同僚にも利用してもらうため、OSSのGitホスティングサービスであるGitBucketでソースを公開し、また導入のハードルを減らすために自作にアップデートコマンドを同梱することで展開を進めていきました。基本的にはコストがかからない形での仕組みづくりになっているためとりあえず試してもらえればと思います。,False
2017,https://pycon.jp/2017/ja/proposals/vote/27/,実例による Python メタプログラミング入門 (ja),"実例として、Python にブロックの終端を示す  ""end"" キーワードを導入するという、非実用的ながら楽しい問題を取り上げ、これを実際にメタプログラミングで解決する方法を示し、その際に理解が必要になる Python の言語機能について解説します。 Python におけるメタプログラミングの手法を学びます。 実例として、Python にブロックの終端を示す  ""end"" キーワードを導入するという、非実用的ながら楽しい問題を取り上げ、これを実際にメタプログラミングで解決する方法を示し、その際に理解が必要になる Python の言語機能について解説します。扱う予定のトピックは以下の通りです。* モジュールの import の仕組み* CPython のスタックフレームへのアクセス方法* CPython のバイトコードへのアクセス方法* Python ソースコードの AST へのアクセス方法",False
2017,https://pycon.jp/2017/ja/proposals/vote/26/,Python web applicationの構成を考える (ja),Pythonでweb applicationを作ろうと考えたとき、サーバソフトウェアはどうするのか、フレームワークは何を使うのか、データベースはどれを選ぶのか、OSは何を使うのかなどなど、その構成には非常に多くの選択肢があります。現状、どのような構成で作るのがわかりやすいのか、メリット・デメリットを交えながらお話しします。 PHPやPerlでweb applicationを作る場合に比べ、Pythonでweb applicationを作る場合は選択肢やはまりどころが多い。これからPythonでweb applicationを作ってみようと思っている人が、どのような構成で始めればよいのかという指針を得られます。 PHPやPerlでweb applicationを作成する場合に比べ、Pythonでweb applicationを作成する場合、なかなか単純には話がすすみません。特に、これからweb application開発に挑戦してみようとする初心者の場合、サーバはどうすれば良いのか、フレームワークはどうすれば良いのかなど、知識の無い中で選ばなければいけない物がたくさんあります。そこで、初めてPythonでweb applicationを作る場合に、どういった構成で始めれば良いのか、選択肢毎のメリット・デメリットを交えながらお話しします。,False
2017,https://pycon.jp/2017/ja/proposals/vote/111/,The theory of Serverless development by Python (理論から学ぶPythonによるサーバレス開発) (ja),昨今、注目を集めるServerlessについて、現状最もPythonの利用が盛んであるAWS Lambdaと関連サービスを例として、その考え方やFaaSの動作原理に触れながら、それらの特性を踏まえた設計・実装方法をご紹介していきます。 Serverlessとは何か、どのように利用していくか、設計・実装上のポイントなどを知ることが出来ます Serverlessに至るクラウド業界の流れや開発における要求の変化などから始まり、Serverlessを支える各種サービスの概要および、AWS Lambdaを始めとしたFaaS(Function as a Service)の技術的バックグラウンドと基本動作原理を踏まえながら、どのような考え方でServerlessと向き合っていくか、Serverlessに適したアプリケーションの特性やその設計方法、また、実際にPythonで開発で留意するポイントなどを時間の許す限りお話していきたいと思います。## Outline(予定)- Serverlessとは何か- Serverlessを支えるFunctional Services  - Functional Servicesとは  - 選定や利用におけるポイント- FaaS(Function as a Service)の技術的バックグラウンドと基本動作原理  - コンテナ  - イベント駆動- 設計におけるポイント  - サービスの選び方・組み合わせ  - 同期と非同期  - データストアの選定と設計  - 監視やアプリケーションの運用に関する設計- 実装におけるポイント  - 基本実装  - 性能向上のポイント  - テスト- その他  - CI/CD  - Serverlessにすべきかどうかの判断基準など,False
2017,https://pycon.jp/2017/ja/proposals/vote/154/,Mezzanine+Cartridgeで作るECサイト入門 (ja),"MezzanineはDjango製のCMSで、Wordpressのような機能を提供してくれる優れものです。CartridgeはMezzanineの上に更に載せることでショッピングカート機能を簡単につけることができるplag-inです。この2つを利用し、ECサイトの基本的な機能や実装する上での注意点などを中心に、お手軽にECサイトを開発する方法を解説します。 ECサイト開発のポイント、Mezzanine, Cartridgeの特徴・使い方を学べる MezzanineはDjango製のCMSで、Wordpressのような機能を提供してくれる優れものです。CartridgeはMezzanineの上に更に載せることでショッピングカート機能を簡単につけることができるplag-inです。この2つを利用し、ECサイトの基本的な機能や実装する上での注意点などを中心に、お手軽にECサイトを開発する方法を解説します。",False
2017,https://pycon.jp/2017/ja/proposals/vote/118/,"A Guide to Exponentiation, and how it effects Machine Learning (en)","Exponentiation is the gotcha of math operators. Be it square or square root, exp, log, tanh, or the complex roots of unity, ** aka ^ has it's work cut out. Ints, floats, fractions, matrices, complex, and zero don't play nice.. Precision, accuracy, and performance aspects of Python and Julia can be contrasted by following ** from code down to the bits. Machine Learning aspects considered.  Mathematics of exponentiation and why data science cares (nearest neighbors and neural network activation functions, e.g logit/logistic)Calculation, precision, storage, representation and graphing all need to be considered, especially at scale. ** is prone to underflow and overflow with possibly dire consequences. It can even take longer to display the result of an exponentiation than to calculate it! Speed does matter when billions of calculations are being done.A very brief recap language and version differencesThe CPython implementation of exponentiation is fairly convoluted n informative. Ironically, it is sometimes possible to successfully perform exponentiation with integers when floats will fail. As an operator becomes the function pow or ipow. Numerically help is needed at edge cases and so: exp -> expm1, sqrt -> hypot, log -> log1p, factorial -> gamma and Stirling, and why recursion is evil (especially for Fibonacci where an exponential comes to the rescue), and how caching might be useful.In addition, numpy provides exp and exp2, ldexp and frexp, log2, logaddexp and logaddexp2, square and power, and each with interesting use cases.Did someone say matrix exponentiation? numpy.linalg.matrix_power scipy.linalg has expm, expm2, expm3, exmp_cond, expm_frechet.Cryptography uses lots of modular exponentiation and tricks apply here.Solving the simple equation: xy == yx is challenging for both rational and real solutions. Graphing can help, with it's challenges of logarithmic scaling. We'll be examining the decimal, fractions, mpmath, bignum, and sympy modules for insights, workarounds and new gotchas. Mathematics of exponentiation and why data science cares (nearest neighbors and neural network activation functions, e.g logit/logistic)Calculation, precision, storage, representation and graphing all need to be considered, especially at scale. ** is prone to underflow and overflow with possibly dire consequences. It can even take longer to display the result of an exponentiation than to calculate it! Speed does matter when billions of calculations are being done.A very brief recap language and version differencesThe CPython implementation of exponentiation is fairly convoluted n informative. Ironically, it is sometimes possible to successfully perform exponentiation with integers when floats will fail. As an operator becomes the function pow or ipow. Numerically help is needed at edge cases and so: exp -> expm1, sqrt -> hypot, log -> log1p, factorial -> gamma and Stirling, and why recursion is evil (especially for Fibonacci where an exponential comes to the rescue), and how caching might be useful.In addition, numpy provides exp and exp2, ldexp and frexp, log2, logaddexp and logaddexp2, square and power, and each with interesting use cases.Did someone say matrix exponentiation? numpy.linalg.matrix_power scipy.linalg has expm, expm2, expm3, exmp_cond, expm_frechet.Cryptography uses lots of modular exponentiation and tricks apply here.Solving the simple equation: x* * y == y* *x is challenging for both rational and real solutions. Graphing can help, with it's challenges of logarithmic scaling. We'll be examining the decimal, fractions, mpmath, bignum, and sympy modules for insights, workarounds and new gotchas.",False
2017,https://pycon.jp/2017/ja/proposals/vote/94/,Python for Linux System Administration: best practices for CLI (en),"We will discuss best practices for python programs that are used from the CLI by system administrators and devops. By looking at the expectations of system administration (service, logging, monitoring), we will see how we can implement it in Python. Best practices for CLI:- Input (standard input, argument parsing)- Output (standard output, logs)Introduction to linux system administration needs:- Running application as a service- Monitoring Python is a popular replacement to bash scripts on linux servers.In system administration, it is used both for applications that provide services, and for scripted tools.The context of this talk will be the following environment: - Linux distribution with bash or equivalent - Python 2.7 or 3.x- A modern init system (systemd or equivalent functionalities)In this talk, we will see how sysadmins and devops interact with programs using CLI (Command Line Interface), and how to design a CLI-friendly program. We will see two types of programs and their way of interactions: application running on the background, and tools doing a one-time action.For tools, we will look at argument parsing and the standard input for the user input, as well as the standard output and the log management.For python application running in the background, we will look at how we can package an application as a service, and how it can be controlled from the service manager.We will also look at what output that kind of program can provide (status, logs).",False
2017,https://pycon.jp/2017/ja/proposals/vote/112/,pyramid_servicesを使ったサービス化 (ja),pyramid_servicesはpyramidが内部で利用しているzope.interfaceのregistryにサービスを登録するためのライブラリです。このpyramid_servicesの利用方法やどういった処理をどのようにサービス化していくと良いのか解説します。 実際のサービスモジュールの切り分けなどを含めたpyramid_servicesの使い方を学べるはずです - pyramid webフレームワーク- zope.interface- utilityとadapter- registryを意識したpyramid addon- pyramid_services- サービスとは？- サービスにしないほうがいいもの,False
2017,https://pycon.jp/2017/ja/proposals/vote/159/,SREエンジニアがJupyter＋BigQueryでデータ分析基盤をDev＆Opsする話 (ja),Python（JupyterNotebook）とBigQueryを用いたモニタリング基盤・データ分析基盤の開発・運用について話します。多様な部署・職種が関わる大企業において、どのような目的・用途・制約が与えられ、そのためにどのようなシステム・プロセスを採用したのか共有します。 プロダクト開発におけるデータ活用の指針・参考事例を持ち帰っていただきます。少しでも多くの開発現場がPythonという武器を手に、データを最大限に活用し、世の中に良いプロダクトを提供する助けになれたらと思っています。 # ゴール1. 定常モニタリング：日々のKPI管理を安定化させる2. アドホック分析：Build・Measure・Learnを回す# トピック現場ならではの泥臭い話をたっぷりお届けします。- モニタリング編  - 非技術者が作った重厚長大なExcelシートをいかに置き換えるか  - 「時間を掛けて作ったが誰にも使われない」悲劇をいかに回避するか  - モニタリング内容の正しさをいかにテストするか- アドホック編  - 分析経験のないエンジニアチームでいかにデータスキルを装着させるか  - システム開発の現場で仮説検証プロセスをいかに組み込むか# アーキテクチャDWHとしてBigQueryに集約 → Python on EC2 + Jenkins で加工 → 各部署の使いやすいツールに出力- マーケティング部署の見るExcelシート- ディレクション部署が見るRedash- 開発部署/データ分析部署が見るJupyter- 運用部署に届くSlack通知# Pythonスクリプトの構成（モニタリング）日夜改善を続けているので発表時には変わる可能性あり- Controller  - Jenkinsが叩くためのエンドポイント  - 処理の全体像を記述 → Facadeとして他処理を呼び出す- Enum  - Python3.4〜のEnumを活用  - Dictonary型で値を管理  - 使いやすくするメソッドを基底クラスに集約- Model  - 値はpandasで整形  - 中間・最終指標を擬似DataTransterObject(DTO)として代入する  - DTOをI/Fする際にはPython3.5〜のTypeHintsを活用  - DTOはBigQueryに反映して、プログラム上ではステートレスに保つ- Query  - Python3.6〜のf-stringsを活用してクエリを書く  - メソッド切り出し → 値の引き渡しによるスコープ制御  - BigQueryとのシンプルなR/Wに特化- Interface  - 入出力先に応じた接続・変換処理に特化（例：週次売上をmatplotlibで可視化）  - .envから環境変数を読み込む（例：Slack連携で本番・開発でチャンネルを分ける）開発・運用のお試し実行としてはローカルのJupyterNotebookを活用します。実運用PythonのEnumなどをインポートした上で、検証したい処理のみ差し替えます。もともとJupyterのみで完結する予定でしたが、定期実行やクラス構成検討で生Pythonの方が都合が良いことが分かりました。# Jupyter on Github の活用（アドホック分析）- ローカル環境：メインモニタでJupyterNotebook、隣のモニタでアプリケーション本体のソースコード- 分析結果はGithubのプレビュー画面で関係者に共有- コードの差分は全てコミット履歴で管理,False
2017,https://pycon.jp/2017/ja/proposals/vote/10/,Pythonで作るAnsibleモジュール入門 (ja),AnsibleはPythonで記述されたアプリケーションとシステムの導入を容易にする非常にシンプルなIT自動化プラットフォームです。PyConに参加される方々の中でも、インフラを何らかの形で扱う人は居るはずです。今回は任意のAnsibleモジュールをPyhtonで作成していきたいと思います。 Ansibleとは構成管理ツールです。 資料はまだない,False
2017,https://pycon.jp/2017/ja/proposals/vote/11/,Building Your Own Stock Analysis System (en),"People have been talking about how easy it is to use python to analyze data.  However, there are still difficulties for beginners to get a sense of how to use it.  In this talk, I will go through a simple stock analysis system and break the entry barrier for the beginners. The attendees should be able to easily build their own stock analysis system after the talk. In this talk, I will introduce how to do quantitative analysis on stocks.  The talk will include how to get the stock information from the web, how to choose stocks quantitatively, and backtest your strategies.  I will use US stock market as an example.   First, pandas or finsymbols will be used to get the company information.  Next, pandas or quandl will be used to get the historical stock prices.  Regarding the screening strategies for the stocks, I will use volatility as an example.  After that, I will show the atendees how to back test their strategies by using sharpe ratio, maximum drawdown, etc. ",False
2017,https://pycon.jp/2017/ja/proposals/vote/144/,機械学習におけるデータの再現性について (ja),"チームで機械学習のタスクに取り組む際、過去の自分や他人が利用したデータの再現をするのに苦労する事があります。本トークでは、データの再現性が低下する原因について体系的に解説し、akagi というオープンソースのPython ライブラリを利用してデータの再現性を高める方法について紹介します。 Pythonを利用した機械学習に取り組む際に生じるデータの再現性について、その問題と原因を体系的に理解するためのヒントが得られます。予め経験と課題意識のある人にとっては、その解決策の一例を提案されることにより、データ再現性におけるより深い洞察を得られます。 機械学習のタスクに取り組む場合において、データの再現性が損なわれる場面があり、その 原因は多岐にわたります。- データの再現性は時間とともに低下します。過去に誰かが用意したデータを利用しようとする際に、データの入手方法がドキュメント化されておらず、入力データを再現できない場合があります。最悪の場合、丁寧に加工されたデータは本人のローカル環境にしか存在しないかもしれません。- またデータの入手先が多岐にわたり(MySQL, Google Cloud Storage/Amazon S3, ローカルのファイル、Spreadsheet等)、それらの入手先からデータを集め、実験用のデータに落とし込むのが難しいという問題もあります。これは特にエンタープライズアプリケーションを運用する企業などにおいて顕著です。- さらにデータの実体が動的である場合(RDS等)、アクセスするタイミングによってデータの内容が変化する場合があり、固定のデータが欲しい場合に問題となります。一方で例えば深層学習系のフレームワークには、著名なデータセットについて、それをダウンロードし、numpy等の扱いやすい形式に整形する独自のAPIが用意されていることがあり、どこからでも全く同じデータを入手する(再現する)ことが出来ます。これらの抽象度の高いAPIはデータの再現性を極大に高めており、フレームワークのExampleスクリプトで実行する場合や、モデルの動作を検証するためのダミーデータとして利用する場合などにおいて高い有用性を持ちます。しかしながらその抽象度の高さ故に、このようなAPIをデータセット毎に用意してやる必要があるという問題が有り、実際にそのようなAPIでサポートされるデータセットは少なく(MNIST, CIFAR-10, IMDB等)、現実世界のタスクにおける有用性は高くありません。本トークではまず、これらの問題の根本的な原因を突き止める為に、データの抽象度について機械学習の文脈でそれを捉える方法を提案します。その為に、データを機械学習に利用する際、実際にデータをモデルに与えるまでの段階を`Loading`と`Processing`という2つのレイヤに分割します。それぞれ- Loading: データをダウンロードする- Processing: 適切な形式に加工／分割し、モデルに与えるという操作を表します。そして、2つ以上の異なるタスクに取り組む際、これらのレイヤそれぞれにおいて導入される多様性に量的差がある事をtensorflowの`maybe_download`等の例を元に指摘し、`Loading`レイヤを抽象化出来る事を示します。また、そのような抽象化を行うPythonライブラリとしてのakagiを利用する事で、「データを実行可能なPythonスクリプトで表現」し、再現性を高められることを示します。また本トークでは、データの固定についても触れます。データの固定とは、例えば""http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz""のようなURLを用意する事です。MNISTデータセットの内容は、このURLを参照する限り(かつサーバー上のその内容が変更されない限り)変化しません。一方でRDB等のデータソースについては、同じクエリでも得られる結果がコンテキストにより変化します。その場合、固定されたデータが欲しい場合は静的なファイル(.csv)に保存するなどの工夫が必要になります。akagiではAmazon S3をバックエンドとしたデータの固定機構を利用し、この問題に対処しています。akagiの開発には、上記のような課題の設定が現場の状況に合致していることが必要となります。データの再現性という限定された課題を挟んで情報交換するとともに、参加者の皆様がこの課題を認識／理解する助けになれば幸いです。",False
2017,https://pycon.jp/2017/ja/proposals/vote/14/,The essence of Django ORM (en),"The world is swimming in articles and books about Django and Django ORM itself. All this sources cover only basic use cases of Django ORM comparing to real world examples.This talk is dedicated to the proposition to crash test Django ORM 80% of functionalities in 20% of the time. They can write concise, effective, and idiomatic backend code with Django ORM and SQL with PostgreSQL. The world is swimming in articles and books about Django and Django ORM itself. All this sources cover only basic use cases of Django ORM comparing to real world examples.This talk is dedicated to the proposition to crash test Django ORM 80% of functionalities in 20% of the time.  Also I’m going to show the important features and capabilities of Django ORM compare to most of SQL features. Traditionally, ORM is presented in a “bottom-up” fashion, but I would like to propose approach, on the other hand, is “top-down”. We begin by identifying a rather short list of standard questions, or precisely types of questions, that are often asked of relations databases. We then show how these standard questions are posted in Django ORM, introducing and motivating the use of its capabilities and features as they become relevant. ",False
2017,https://pycon.jp/2017/ja/proposals/vote/13/,Shin Recognize Godzilla (ja),みんな大好きなPythonとみんな大好きな「ゴジラ」の画像を使ってディープラーニングしよう！というお話です。データセットは用意されていないので、集めることからスタートします。経験した人はわかると思いますが本当に大変で、色々な手法を使って集め、加工などの前処理を行って初めて学習させることができます。その一連の流れとゴジラ（画像）をディープラーニングさせるとどうなるのか？のお話をします。 畳み込みニューラルネットの基礎知識の理解。Pythonを使ってデータの収集と画像加工についてのヒントを得ることができる。 # 話すこと1. データ収集2. データ加工3. データ水増し4. 機械学習（ディープラーニング）5. デモ※すべてPythonを使います 画像の収集の手法、機械学習を行うまでの前処理や収取のtipsのようなこと、またディープラーニングのパラメータチューニングのお話をします。**時間に余裕があればゴジラの紹介もします（有名なはずなので、いらないかも**）## 使用するライブラリ（主に）* Numpy* Selenium* OpenCV* Keras* Tensorflow,False
2017,https://pycon.jp/2017/ja/proposals/vote/145/,いにしえのDjangoをアップグレード (ja),Webフレームワークでは必ず存在するアップグレード。今回はいしにえのDjangoからのアップグレードについてご紹介いたします。古いDjangoのバージョンから最新のDjangoへのアップグレードに必要な手順、問題点、解決方法についてです。 Djangoのアップグレード方法・考え方について知っていただくことが目的。 Webフレームワークでは必ず存在するアップグレード。今回はいしにえのDjangoからのアップグレードについてご紹介いたします。古いDjangoのバージョンから最新のDjangoへのアップグレードに必要な手順、問題点、解決方法についてです。1. Djangoとは2. 背景3. アップグレードの流れ4. 問題点5. 解決方法6. まとめ,False
2017,https://pycon.jp/2017/ja/proposals/vote/17/,Developing REST API using  Python and Bottle Framework and Usecases (en),"REST is the architectural style of the API and it is next big thing after SOAP messaging. Bottle is the simple, proven and one of the most popular frameworks for web development. We are using Bottle framework with REST APIs for python web programming. You will learn...How to developer website using Python Bottle framework?How do REST APIs work?How is REST API difference from SOAP architecture? REST is the architectural style of the API and it is next big thing after SOAP messaging.  Use of the REST APIs is growing enormously and many big firms are moving with REST APIs. As recently Facebook has moved to RESTFul APIs and deprecated many of its old APIs.Bottle is the simple, proven and one of the most popular frameworks for web development. We can do man amazing thing using Bottle framework with REST APIs for python web programming.",False
2017,https://pycon.jp/2017/ja/proposals/vote/19/,野球を科学する技術〜Pythonを用いた統計ライブラリ作成と分析基盤構築 (ja),"野球の統計モデルをPythonで実現する方法について紹介します.スクレイピング(Scrapy,Beautifulsoup),データパイプライン(Airflow),可視化ツール(Jupyter,Redashなど)を駆使し,野球統計学「セイバーメトリクス」を用いた野球選手の評価方法および,野球統計パッケージ(自作)について解説いたします. 野球統計学「セイバーメトリクス」によるプロ野球選手の評価方法と実現方法.データパイプラインを用いてデータ収集から可視化まで一気通貫で行えるプロダクトを作るためのノウハウ(Docker + GCPを使う予定です), Scrapy,Airflowなど,Pythonライブラリの用い方.野球統計パッケージ(自作)の紹介および関連する野球統計モデルの紹介. # 野球を科学するための技術〜Pythonではじめるセイバーメトリクスと分析基盤構築## Python使いとして今年(2017)1月から新しいチーム(Retty)にJOIN後,Luigiなどを用いたPythonベースのデータパイプラインを用いる機会が増えました.データ収集から前処理・分析および可視化という割と誰でも興味ある・やりそうなテーマではありますが,案外実践事例が無いのでは?と思い,私がライフワークとしている野球で試して公開してみることにしました.このセッションではPyCon JP 2015で発表した「[野球Hack!〜Pythonを用いたデータ分析と可視化][1]」で発表した「オレオレ野球分析基盤」をScrapy,Airflow,Redashなどを用いて構築した時のノウハウを紹介いたします.## セイバーメトリシャン(野球統計学ファン)としてまた,セイバーメトリクス(野球統計学)にて,現在主流として取り扱われている以下の概念と指標についての紹介いたします(Pythonでの算出方法を含める).* Linear Weights/Run Value(得点期待値と得点価値)* wOBA* wRAAこれらを野球統計学の概念および,野球統計パッケージ(自作)を含め紹介いたします.## 参考資料* [ScrapyとRedashではじめる野球統計学(PyCon mini Kumamoto2017発表資料)][2]## Keyword* Webスクレイピング(Scrapy,Beautifulsoup)* データパイプライン(Airflow)* 分析と可視化(Jupyter,Redash)* Cloud・インフラ(GCP,Dockerほか)* セイバーメトリクス(RC,wOBA,wRAA,DIPS, FIP, WARほか)  [1]: https://pycon.jp/2015/ja/schedule/presentation/67/  [2]: https://speakerdeck.com/shinyorke/scrapytoredashdehazimeruye-qiu-tong-ji-xue-number-pyconkuma",False
2017,https://pycon.jp/2017/ja/proposals/vote/16/,Industrial Test Automation with Asyncio (en),"In this talk we'll explore how to use the new Python 3 asyncio library to automate hardware-software integration tests for industrial automation. We will find out how asynchronous coroutines can simplify and speed up distributed tests in real world environments, and what possible difficulties to watch out for. Learn how Asyncio can be used for more than HTTP APIs. Find out how Industrial Automation + Python 3 rocks. And last but not least: Serious companies use Python to do serious embedded development and testing. # Industrial Test Automation with AsyncioMost internet examples of Asyncio cover writing faster HTTP services. The new `async` and `await` syntax has proven to make complex concurrent tasks easier to write and easier to reason about. Two great examples for libraries that support asyncio are `aiohttp` and `uvloop`.  In this talk on the other hand, we will explore a completely different way of leveraging Python's new asynchronous capabilities and find out how to orchestrate complex hardware based software integration tests.Having used Python and Asyncio in a embedded development project for a big railway signaling manufacturer, I was able to leverage Asyncio to better model tests that involve multiple machines used in typical redundant tests in industrial automation. Not only are test cases easier to set up, since they would otherwise typically involve a lot of multithreaded code, test results are easier to analyze and it is possible to retrieve test results from test hardware much faster.Overall, the resulting increase in testing speed makes the develop/test feedback loop faster and can produce more reliable and reproducible results during hardware and software certification processes. The cost saving potential for companies is enormous.",False
2017,https://pycon.jp/2017/ja/proposals/vote/22/,タイプヒント (ja),Type Hintsに関連するPEPの解説、及び、その周辺ツールによって得られる利点について話します。特にPython2からPython3への移行をするときに使えるテクニックについても共有します。 Type Hintsについて学び、IDEからのサポートの受け方、mypyをつかったCIの仕方などを知ってもらいます。特にチーム開発などに関連する話をしたいと思っています。 タイプヒントはPython3から追加されたものです。これを追加することで補完が便利になったり、コードが読みやすくなるなどたくさんのいいことがあります。是非、明日からの開発に役立てていただきたいです。,False
2017,https://pycon.jp/2017/ja/proposals/vote/23/,探検Pyramid (ja),"Pyramid Webフレームワークはアプリケーションのコンポーネント構成を実行中に確認できるintrospectionという仕組みがあります。introspectionを使ってカスタムコンポーネントでintrospectionに登録したり、実行時情報からopen apiのスキーマ情報を作成する方法を解説します。 Pyramidの内部構造とコンポーネント作成の流れを説明し、それらを活用するためのメタデータであるintrospectionの使い方を説明します。 -  Pyramidのコンポーネントアーキテクチャ-  contextとrequest.registry- zope.interfaceとAdapter Registry- pyramid.config.Configurator- introspectable- view, routes のintrospection- open api への応用",False
2017,https://pycon.jp/2017/ja/proposals/vote/20/,Pythonとパッケージングと私 (ja),Pythonのパッケージング周辺技術やPEPアップデートの解説 Pythonパッケージングの仕組みを知り、依存ライブラリの管理やデプロイなどを効率化する Pythonでパッケージを作成したり利用したりするためのツール群の説明と、パッケージング周りのPEPの更新から今後どのようにツールが更新されそうなのか解説します。また、はまりがちなWindowsでのC拡張モジュールの利用についてtipsを紹介します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/25/,SQLの錬金術師ー既存データベースに立ち向かう (ja),既存データベーススキーマがある状態からの開発においてSQLAlchemyでどのようにクラスにマッピングしていくかのtipsを紹介します。 代理キーなしの複合主キーや外部キーなしの関連などイレギュラーなスキーマを取り扱う方法を学んでいただけると思います。  - declarative_baseによるモデル定義 - スキーマ情報の取り出し - sqlsoupでアドホックにテーブル情報を扱う - 複合主キーのマッピング - 外部キーなしで関連を定義する - 外部キー定義にさらに条件をつけた関連マッピング,False
2017,https://pycon.jp/2017/ja/proposals/vote/36/,How to use Python Requests with simulation trading sites (ja),"Stock trading using algorithms has become far more accessible to programmers. However managing deals, orders and trades is another key part to trading. In this talk I will demonstrate methods of how to use HTTP POST and GET commands to interface to a sandbox trading site.スライドの内容は英語で、トークは日本語で開催する予定です。 By the end of this talk, you will learn the following:-What a deal management system is-The complexity to build a system-How to find and connect to sandbox trading systems-Learn basic skills to automatically login to a site using the Requests library to POST and GET data Having a system to manage your orders, positions and track your profits and losses is another important part to doing online trading. Here I will discuss the importance of having a deal management system and the complexities of building such a system. We will instead learn how to turn a sandbox trading system into a trading simulator by using HTTP GET and POST requests.This is a very safe and fast way to start learning how to trade without needing to have a paid brokerage account or use systems that cost money. For this talk, I will focus on using a free sandbox site and the concept of how to build your own python API / Application",False
2017,https://pycon.jp/2017/ja/proposals/vote/18/,実践 SQLAlchemy (ja),SQLAlchemy は Python の代表的なオブジェクト・リレーショナルマッパー (ORM) の一つです。SQLAlchemy を使うことで MySQL や SQLite といったリレーショナル・データベース (RDB) を扱いやすくなります。このセッションでは、データの永続化についての基礎知識から SQLAlchemy の実践的な扱い方までをご紹介します。 このセッションを通して SQLAlchemy を使って RDB にデータを永続化するための方法を知ることができます。また、SQLAlchemy を扱う上での Tips や注意すべきポイント、便利な周辺ライブラリについても理解が深まります。 プログラミングにおいて、データの永続化は色々な場面で必要になります。例えばそれは Web サイトでユーザが入力した内容だったり、あるいはインターネットからスクレイピングしてきた情報かもしれません。そんなデータの永続化ですが、具体的なやり方としては様々な方法が考えられるでしょう。中でも MySQL や SQLite といったリレーショナル・データベース (RDB) はデータを永続化する上で最も一般的なやり方と言えます。本セッションでご紹介するのは、そんな RDB を扱いやすくする SQLAlchemy というオブジェクト・リレーショナルマッパー (ORM) です。SQLAlchemy は採用実績も豊富にあり、多機能で直感的な操作が可能な Python の代表的な ORM の一つです。ORM を通して RDB を扱うことで、一般にインピーダンスミスマッチと呼ばれる問題を解消できます。しかしながら、SQLAlchemy は多機能であるが故にとっつきにくいと感じられる面もあります。また、日本語の資料が豊富にあるとは言えず、扱う上で注意を要するポイントも随所に存在しています。本セッションでは、データの永続化に関する基礎知識から、SQLAlchemy のより実践的なテクニックまでを体系的にご紹介します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/146/,Building A Wrapper API: The case for abstraction (en),"Suppose that an API already exists for a service you use. Now suppose that you want to make a tool that only uses a subset of that API to make it easier to perform certain tasks in that API. This is a wrapper API: A way to simplify or abstract API functions in a way that your tool doesn't have a lot of repetitive code, and that makes it easy for others to build on or contribute to your tool. Learn what a wrapper API is, when and why to create one, how to write tests for it, document it, and publish it. In this talk I'll describe how I got started creating wrapper APIs for Zabbix and Elasticsearch, and how the latter led to the tool called Elasticsearch Curator. I'll cover topics such as:* Design: Am I reinventing the wheel, or is this really going to be a useful endeavor?* Classes: When to use them, or whether it makes sense to just stick with a function.* Keep moving forward: Your code will always evolve, and your skills improve with use. How to make usable code now, and allow yourself space to improve over time, rather than be stuck in ""impostor syndrome"" purgatory.* Documentation: How to ensure that others will know how to find your documentation, and be able to understand it so they can use your API* Project layout and file separation* Testing: Why you need both unit and integration tests. How to use Travis CI and GitHub to accomplish this* Publication: PyPI, RPM/DEB and other concerns.",False
2017,https://pycon.jp/2017/ja/proposals/vote/114/,PyCharmを使いこなそう (ja),PyCharmはJetBrains社が開発しているPython向け統合開発環境です。このトークでは、PyCharmを使ったPythonアプリケーションの開発の具体的な流れ(環境構築、デバッグ、UT、VCS連携...etc)について話します。また無料で使えるCommunity版と有償のProfessional版の違いやTipsもデモを交えて紹介します。 PyCharmの基本的な使い方を知り、実際の開発現場での開発フローを知ることができます。また、JetBrains社はIntelliJ IDEAやAndroid Studioをはじめ他言語向けIDEも幅広く提供しているため、Python使いに限らずとも使える情報も提供します。 PyCharmはとても高機能なPython向け統合開発環境です。このトークでは、デモを交えながら実際にPyCharmで新規プロジェクトを作成するところからはじめ、そのアプリケーションのデバッグ方法、ユニットテストの実行方法、gitでのブランチ切り替えなど、実際の開発で必要になることを一通り紹介します。また、PyCharmには無料で使えるCommunity版と有償のProfessional版があります。Professional版ではWebフレームワークのサポートやDockerサポートなどCommunity版にはない便利な機能が多く含まれているのでそちらも合わせて説明します。また、私はもともとPythonはVimで書いていたのですが、前職で同じJetBrains社製のAndroid Studioを用いてAndroidアプリ開発を行っていた経験から、Python開発でもPyCharmをメインに使うようになりました。基本的な部分は共通しているものも多いため、自身の経験を踏まえ、JetBrains社製のIDEで横断的に使えるTipsも紹介していきます。,False
2017,https://pycon.jp/2017/ja/proposals/vote/120/,Secrets of a WSGI master. (en),"The mod\_wsgi ecosystem has grown in recent years, adding easier ways to deploy Python web applications, and tools for deploying to containerised environments. This talk will highlight new or not well known capabilities of mod\_wsgi. It will also provide tips on how to configure mod\_wsgi to ensure it is setup properly, will perform well and can automatically recover from problems that can occur. If a beginner, they will learn why mod_wsgi is still a good option for deploying a Python web applications. If they are an old time user of mod_wsgi, they will find out about all the features they probably didn't know existed, and so revisit their current Python web application deployment and make it even better. The WSGI (Web Server Gateway Interface) specification for hosting Python web applications was created in 2003. Measured in Internet time, it is ancient. The oldest main stream implementation of the WSGI specification is mod\_wsgi, for the Apache HTTPD server and it is 10 years old.WSGI is starting to be regarded as not up to the job, with technologies such as HTTP/2, web sockets and async dispatching being the way forward. Reality is that WSGI will be around for quite some time yet and for the majority of use cases is more than adequate.The real problem is not that we need to move to these new technologies, but that we aren't using the current WSGI servers to their best advantage. Moving to a new set of technologies will not necessarily make things better and will only create a new set of problems you have to solve.As one of the oldest WSGI server implementations, Apache and mod\_wsgi may be regarded as boring and not cool, but it is still the most stable option for hosting WSGI applications available. It also hasn't been sitting still, with a considerable amount of development work being done on mod\_wsgi in the last few years to make it even more robust and easier to use in a development environment as well as production, including in containerised environments.In this talk you will learn about many features of mod\_wsgi which you probably didn't even know existed, features which can help towards ensuring your Python web application deployment performs to its best, is secure, and has a low maintenance burden.Topics which will be covered include:* Easy deployment of Python web applications using mod\_wsgi-express.* Integration of mod\_wsgi-express with a Django web application.* Using mod\_wsgi-express in a development environment.* How to make use of mod\_wsgi-express in a production environment.* Using mod\_wsgi-express in a containerised runtime environment.* Ensuring consistency between development and production environments using warpdrive.* Using mod_wsgi-express to bootstrap a system Apache installation for hosting WSGI applications.* Why you should be using daemon mode of mod\_wsgi and not embedded mode.* How to properly associate mod\_wsgi with a Python virtual environment.* Building a robust deployment that can recover from misbehaving application code, backend services, or request overloading.* Using hooks provided by mod\_wsgi to monitor the performance of your Python web application.If you are a beginner, come learn why mod\_wsgi is still a good option for deploying your Python web applications. If you are an old time user of mod\_wsgi, find out about all the features you probably didn't know existed, revisit your current Python web application deployment and make it even better.",False
2017,https://pycon.jp/2017/ja/proposals/vote/12/,PyQt/PySideの落とし穴 - 脱初心者のための実践開発 (ja),"PyQt/PySideは、PythonのGUIツールキットとしては圧倒的なシェアを獲得しています。そんなPyQt/PySideですが沢山の落とし穴があります、実務を通じて踏み抜いた落とし穴を回避する方法をお教えします。 PyQt/PySideで""当たり前""のGUIアプリケーションを作成するには、あまりにも情報が少なく落とし穴が多いのが現状です。必ずハマる落とし穴を回避して、当たり前の事が当たり前にできる知識が身につきます。また、PyInstallerを用いたPyQt5のexe化、AppVeyorによる継続的インテグレーションによるExe供給ができるようになります。 PyQt/PySideを使った開発で必ず落ちるであろう落とし穴を説明します。以下のコードな何の変哲もないi18n(国際化)を設定したアプリケーションのコードです。    app = QApplication(sys.argv)        translator = QTranslator()    translator.load(""translate/qt_ja.qm"")    app.installTranslator(translator)        app.exec_()上のコードをリファクタリングして、i18nのコードを関数にまとめました。    def setup_translator(app):        translator = QTranslator()        translator.load(""translate/qt_ja.qm"")        app.installTranslator(translator)    app = QApplication(sys.argv)    setup_translator(app)    app.exec_()なんと単純に関数化しただけでi18nが動作しなくなります！エラーも出ないため原因の特定も難しいでしょう。この落とし穴はほんの一例です。他にも多数の落とし穴が存在します、知らなければ意味不明なバグに延々と悩まされるでしょう。本セッションでは、このような落とし穴にはまらないように実務を通じて得た知識を紹介します。- QtオブジェクトとPythonオブジェクトの寿命の違いから生じる落とし穴- 例外の落とし穴- StyleSheetの落とし穴- Qtのマルチスレッド/マルチプロセスの落とし穴- PySideとPyQtのマルチ開発- PyInstallerによるexe化と AppVeyourによる継続的インテグレーション",False
2017,https://pycon.jp/2017/ja/proposals/vote/28/,Python 2/3 互換なコードの書き方 (ja),Python 2 は 2020 年にメンテナンス終了が予定されていますが、現時点では依然として多くの場所で Python 2 が使われ続けています。そのため、主にライブラリ開発者は、Python 2/3 の両方で動くコードを書く必要性に直面することがあります。本トークでは、Python 2/3 互換なコードを書くべきケースはどのような場合なのか、そしてそのようなコードを書かざるを得なくなった場合、どのような選択肢があるのかをお話しします。 Python 2/3 互換なコードを書くべきケースはどのような場合なのか、そしてそのようなコードを書かざるを得なくなった場合、どのような選択肢があるのかを学びます。 TBD,False
2017,https://pycon.jp/2017/ja/proposals/vote/29/,Years with Python (en),"This talk is more likely to be technical retrospection in a manner of speaking but also intended to give insight into how the Python ecosystem interacts in a company. In this talk, I’ll cover what I did to get things done, what mistakes I made and how I make up for the mistakes. Also, I’ll share the actual case of using Python in Korea. As a software engineer, as an active member of the community, and as a decision maker of a company, I have spent a lot of time with Python which is including engineering, community build up, hiring, and choosing the right technologies.In this talk, I’ll cover what I did to get things done, what mistakes I made and how I make up for the mistakes. Also, I’ll share the actual case of using Python in Korea.",False
2017,https://pycon.jp/2017/ja/proposals/vote/30/,PyParseをANTLR v4で置き換えてparserの性能を向上 (ja),検索APIで検索クエリのparserがボトルネックになっていました。当初利用していたPyParseをANTLR v4で置き換えることで性能が5倍になり大きく改善しました。性能改善の流れをPyParseとANTLR v4の比較を交えて紹介します。 ANTLRの基本的な使い方、性能改善のケーススタディに基づくノウハウを共有すること 検索クエリのパース処理が検索APIのボトルネックとなっていました。検索APIの構成の紹介をした上で、ボトルネック解消のために検討したことを共有します。検索クエリのパース処理を高速化したことが性能改善に大きく寄与しました。元々はPyParseを用いて実装していたところをANTLR v4で置き換えることで高速化を実現しました。両ライブラリの使い方、特徴の紹介と比較を行います。,False
2017,https://pycon.jp/2017/ja/proposals/vote/121/,Power up your app with effective search api (en),"In this talk, I'd like to share my experience about integrating Elasticsearch to our Django app and the data relationship between Elasticsearch and your main database. Moreover, rather than only using Elasticsearch as a efficient filtering tool, how can we leverage the power of Elasticsearch to improve users search experience. Based on my learning path, I found it quite interesting to dive into this powerful tool, improve search experience without having strong knowledge about information retrieval. Hope the audiences will get some ideas about:- Easily build up filtering and sorting functions without worrying about table design and database indexes- Optimizing search experience for Mandarin text- what are the interesting use cases by using Elasticsearch Thanks to the amazing work of Open Source developers, building application is no longer a pain start point by using open source frameworks like Django, Flask. While it's easy to reach a point where your application do rely on a search function over certain amount of information, you may start indexing fields in your database where search can be performed. Databases can be good enough for filtering or simple string matching, but they are not a good candidate when you need advanced search functionality.In this talk, I'd like to share my experience about integrating Elasticsearch to our Django app and the data relationship between Elasticsearch and your main database. Moreover, rather than only using Elasticsearch as a efficient filtering tool, how can we leverage the power of Elasticsearch to improve users search experience.",False
2017,https://pycon.jp/2017/ja/proposals/vote/133/,Understanding Serverless Architecture (en),"This talk introduces the concept of serverless architecture and the art of scaling them. The audience will leave with a good understanding of serverless systems and architecture, and also with a knowledge of how to spin up serverless pipelines in AWS. **This is what my team have done while building the serverless architecture:**We wanted to build a serverless data pipeline for coding medical charts using NLP.  However, we didn't want it to be real-time (which is most serverless systems). So, we used a queue and a  monitoring system's (AWS CloudWatch) alarms to pull off the serverless, batch processing pipeline.Next, we wanted to make it a serverless, batch, distributed pipeline. So, we made use of Ansible and made the Master-Workers architecture. However, AWS Lambda has a time-limit of 5 minutes. But our entire NLP pipeline flow takes 30 minutes to complete. So, we stumbled upon an idea wherein we create a Master server via Lambda and run Ansible in nohup mode. And then, we learned some very important lessons while doing nohup monitoring.Now, we realized that Ansible can terminate the workers once the tasks are completed, but we want to delete the master also. So, we again built the Ansible playbook such that the Master kills itself once the workers are terminated. Also, we built a serverless API for querying the results of the data-pipeline, using AWS Lambda and API Gateway. And, all this have to be built keeping in mind the HIPAA compliance, which means that the data needs to be encrypted both at rest, and in motion.So, along the way of building this complete architecture, we experienced a lot of gotcha moments, failures, huge wins and pitfalls which taught us very important lessons. This talk would very briefly explain those bullet-points after the hands-on demo.This talk would include a quick spin up of a simpler serverless Lambda function in AWS [hands-on], and explain the concept of serverless, and the gotchas and pitfalls to keep in mind while opting for a serverless architecture",False
2017,https://pycon.jp/2017/ja/proposals/vote/134/,Running Dask in the Cloud (en),"Is pandas running out of memory?  Have you wanted to more easily process large scale data?dask may be what your looking for.Here I'll introduce the dask framework and talk about how you can get it running in the cloud Define what is DaskExplain why you'd want to use it.Learn how to get started setting up a dask distributed computing cluster Dask is a general purpose Spark-like big data computing framework that allows you to take advantage of Numpy/Pandas/Scikit-learn level complex algorithms, written in Pure Python.This talk provides a brief introduction of dask and focuses on how a dask cluster can be setup in the cloud to get you started with migrating your pandas/numpy workflows to more easily work with and process larger datasets.",False
2017,https://pycon.jp/2017/ja/proposals/vote/95/,slackbotによるライフハック (ja),生活でのマンネリやムダを排除するために、pythonでSlackのbotを作り、いろんなwebサービスやガジェットに連携してライフハックしました。さらに毎日の行動を自動で生成し、ふりかえりもできたので、その結果によって自分自身にどんな変化が起きたのかについても共有します。 PythonとBotやWeb APIを使ってライフハックがしたいふりかえりによって昨日の自分よりもレベルアップがしたい 最初はchatbotの概要と環境構築について説明を行ってから、実際にライフハックをした内容とふりかえりについて紹介します。最後にまとめと今後の展開についても紹介します。* モチベーション* chatbotの説明と環境構築 * チャットやchatbotについて * Slackbotの環境構築方法* ライフハックについて * 食事や健康のライフハック * お金のライフハック * 読書のライフハック * 家族とのライフハック * 趣味とのライフハック* ふりかえりについて * ふりかえりとは * ふりかえりの自動生成 * 自動ふりかえりの効果* まとめと今後の課題について * まとめ * 今後の課題,False
2017,https://pycon.jp/2017/ja/proposals/vote/123/,ローカル環境でDockerをドカドカつかう (ja),みなさん、Dockerは使っていますか？Dockerはデータベースなど、開発環境の一部として使う場合にも非常に便利です。ローカルの開発環境の一部としてDockerコンテナを使うメリットや、コンテナをコマンドの様に活用する方法などをお話しします。 Dockerを使うことで、ローカルの開発環境を手軽にそろえることができます。データベースコンテナなどを使用して、開発環境をそろえる方法を学ぶことができます。 みなさん、Dockerは使っていますか？今やDockerはコンテナ技術の代名詞となりました。Dockerを使うことで、データベースやテスト環境などを手軽に作ることができます。また、コンテナはPythonの実行環境としても利用できます。このセッションでは、Dockerコンテナを開発環境の一部として利用することで得られるメリット、コンテナをコマンドの様に使用する方法などをお話しします！,False
2017,https://pycon.jp/2017/ja/proposals/vote/136/,Understanding the mystery of Neural Networks using Keras (en),"Neural Networks aren't as complicated as they seem to be. Anyone who can write a few lines of Python code can easily whip up an elegant model to predict text or recognise speech using Neural Networks with the help of the Keras library. I will go through how any beginner can use Keras to start their journey as Neural Network pros. Attendees of this talk will develop an understanding of how neural networks work and how they can implement these complex models with simplicity using the Keras library in Python. This presentation will go over how Neural Networks work. I will talk about the differences between RNNs, LSTMs etc and help you learn when to use which type of network. After a brief theoretical introduction to the domain, I will move on to actually implementing Neural Networks in Python using Keras. I will take it right from setting up Keras on your machine all the way to analysing the accuracy of your model. You will learn about the different backends that Keras works with. You will be exposed to implementing the different models Keras has to offer. I will talk about Optimizer functions like Stochastic Gradient Descent, RMS Prop etc. You will learn about loss functions like Cross Entropy loss. You will see how to modify your Keras model on the fly based on the results of the Neural Network (how accurate it is). We will do a demo of a basic neural network on a sample data set where attendees of this talk will get hands on experience with fine tuning the parameters of a neural network to optimize results. With the help of Keras you will be able to jump right into developing complex models using Neural Networks without the hassle of all the boiler plate code you would need without a handy library like Keras.",False
2017,https://pycon.jp/2017/ja/proposals/vote/122/,Java経験者からの僕から見えたPythonとNumpyの世界で大切なこと (ja),Javaを10年書き続け、Python経験は0に等しかった僕がひょんなことから転職し、Python/Numpy/luigiを使ったデータ系の開発に携わりました。Numpyは今やデータ系では外すことのできないライブラリです。しかし、なまじコンピュータプログラミングの知識を持っていると「速く書こうとしてかえって遅くなる」といったような罠を踏みます。私の場合は更に「Pythonの経験が乏しい」というハンデまであります。そんな僕がこれらをどのようにして学び、無事開発を終えることができたのか。他言語の経験は何が役立ち何が役立たなかったか、Numpyを使うときは何を気をつけなければいけなかったか、実際の業務での経験を踏まえて話したいと思います。 他言語での経験はどれくらいPythonで役立つのか、逆に、何を捨てるべきなのか知ることができる。Python経験0の僕が短期間で実案件で活動できたことを知り、元気づけられる。従来のエンジニアだからこそはまりやすいNumpyの癖を知ることができる。 - 私のこれまでの経歴- レコメンドエンジンの理論の概要（協調フィルタリングとはなにか）- Numpyとは- luigiとは- これまでの業務経験で非常に役立ったこと- 役立たなかったこと- チームの人が作った実装のパフォーマンスチューニングをしようとするもかえって遅くなったのはなぜか- Java使いから見たPythonの特徴- Webエンジニアの常識の何を捨てなければならないか- 開発する時は何から手を付けたのか- テストはどのようにしたのか、何はテストできないのか- どうやって学んだのか,False
2017,https://pycon.jp/2017/ja/proposals/vote/147/,Pythonでざっくり学ぶUnixプロセス  (ja),Unixプロセスの基礎的な部分をPythonを通して紹介します。多くのプログラマが経験するであろうUnix系のシステムでどのようにプログラムが動いてるかを簡単なPythonコードで交えながら、初心者の方でもわかるようにお話します。  Pythonに限らず、Unix系システムでプログラムがどのように動いてるのか学ぶことができます。 プログラミングを学び始めた頃に、プロセス、ファイルディスクリプタ、システムコールなどのUnix系システムの用語に戸惑ったことはありませんか？本セッションでは、Unix系システムでプログラムを動かす単位となるプロセスについて概要を説明するとともに、その周辺知識を広く浅くPythonコードを交えながら話します。- プロセスとは? - ファイルディスクリプタ - システムコール - シグナル- Pythonからどう見える?- Webサーバはどう動いてる?- 理解を助ける便利なツール大まかに上記のようなトピックを主軸に、Python初心者やプログラム初心者を対象にお話します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/135/,Using machine learning to try and predict taxi availability (en),"In this talk we will use the taxi availability data from Singapore to learn how we can predict taxi availability with machine learning, and also discuss how such information might be used to help consumers and taxi companies The audience can expect to get the following from this talk - 1) See how we can apply machine learning to real life data2) Get an idea on the issues faced and lessons learnt3) Get pointers to discuss how a consumer or taxi company might use such predictions for their benefit Taxi's nowadays are equipped with devices which an provide their location very accurately. These can be used to get a snapshot of taxi availability at any point of time. The Singapore government provides an open API which can give us a snapshot of the taxi availability in the form of the taxi locations across Singapore. This is very useful to get a current snapshot of the data.By querying the API at periodic intervals we can build a picture of how the availability changes across Singapore. These changes will include various variables like drivers moving around looking for riders, taxis getting hired, taxis dropping off people etc.  If we analyze the data and apply machine learning to these data snapshots taken over a few days, we can try and predict the taxi availability at any location for a given time of the day.The information which we learn from such an analysis can be combined with other data sets like weather, rider demand, any news events etc and understand or predict how people will move across the city. This can be very useful for consumers, taxi companies and even government. Such systems probably already exist at the major taxi and ride sharing companies. So this talk will focus on the following aspects to enable the audience to learn more about these systems - - Data collection - Processing data to a format we can use - Identifying the parameters that we can learn/analyze from the data - Provide a few example of the analysis - Present the results - A brief discussion on how the data can be used in conjunction with other data setsAt the end of the talk the audience can use the slides and information as a reference in case they want to perform such analysis on their own or use it for learning. We can also learn from the comments of any audience members who have worked on such systems and who may want to add some information during the Q&A section.",False
2017,https://pycon.jp/2017/ja/proposals/vote/31/,Pillow と Amazon API Gateway / Amazon Lambda を使ったサーバーレスな動的画像変換 API の作り方 (ja),Amazon Lambda は Amazon API Gateway を通じ、 HTTP リクエストに応答して Python のプログラムを実行することができます。これらと画像処理ライブラリ Pillow を用いたプログラムを組み合わせて実現可能な HTTP リクエスト毎に動的に画像変換を行う API の作り方を紹介します。 - Amazon API Gateway / Amazon Lambda において Python で実現可能な内容の理解を深める - Python で WEB API を作る際の事例を知る Amazon Lambda は Amazon API Gateway を通じ、 HTTP リクエストに応答して Python のプログラムを実行することができます。一方で、Python では画像処理ライブラリ Pillow を用いることで基本的な画像処理を手軽に行うことができます。これらを組み合わせることで HTTP リクエスト毎に URL パスやクエリパラメータに応じた画像変換を行い、その結果を返す API を作ることができます。本トークではこのようなシステムを作るにあたって必要になる、以下の基本的な知識と Tips を 実例を交えながら紹介します。- Amazon API Gateway でのバイナリの扱いについて- Amazon Lambda で利用する Python パッケージの作り方- production/staging などのステージ管理- 効率的なデプロイ- 内部で発生したエラーのハンドリングetc,False
2017,https://pycon.jp/2017/ja/proposals/vote/33/,An Introduction to web scraping using Python (en),"Want to learn how to scrape the web (and / or organized data sets and APIs) for content? This talk will give you the building blocks (and code) to begin your own scraping adventures. We will review basic data scraping, API usage, form submission as well as how to scrape pesky bits like Javascript-usage for DOM manipulation. - What/Why Web Scraping - Scraping vs APIs- Useful libraries available- Which library to use for which job- What is Scrapy Framework- When and when not to use scrapy or which particular framework- Legalities and ethics Web scraping is a technique for gathering data or information on web pages. You could revisit your favorite web site every time it updates for new information. Or you could write a web scraper to have it do it for you!Want to learn how to scrape the web (and / or organized data sets and APIs) for content? This talk will give you the building blocks (and code) to begin your own scraping adventures. We will review basic data scraping, API usage, form submission as well as how to scrape pesky bits like Javascript-usage for DOM manipulation.Besides looking at how websites are put together, we will also discuss the ethics and legalities of scraping. What is legal? How can you be a friendly scraper, so that the administrator of the website you are scraping won’t try to shut you down?Comparison between different libraries like bs4 vs lxml vs re would also be there. I'd also point the directions in which people can make decisions on which library to use for a particular task. Finally towards the end, I'll speak about the scrapy framework, its features and how we can write a simple scrpaer in scrapy.1. [BeautifulSoup][1]2. [lxml][2]3. [re][3]4. [scrapy][4][1]: http://www.crummy.com/software/BeautifulSoup/bs4/doc/[2]: http://lxml.de/index.html#documentation[3]: https://docs.python.org/2/library/re.html[4]: http://doc.scrapy.org/en/1.0/",False
2017,https://pycon.jp/2017/ja/proposals/vote/34/,Attack of Pythons : Gotchas and Landmines in Python (en),"This talk will be about common pitfalls (termed warts/landmines) that people face using Python programming language. Learn about common pitfalls every Python programmer faces in the beginning and also learn how to tackle them in the best way possible. You'll also learn a lot of Python tricks and tips ! Python may be one of the simplest and most flexible programming languages out there, but it is still a programming language. It still has syntax, datatypes, and some occasional dark corners. Python ""warts"" are things for which people have criticised Python, typically aspects of the language or mechanisms of its implementation, because such aspects either expose certain surprising inconsistencies, are regarded as omissions, or cause irritation for parts of the community in some sense.This talk will be about common pitfalls (termed warts/landmines) that people face using Python programming language.",False
2017,https://pycon.jp/2017/ja/proposals/vote/35/,PythonとRを行ったり来たり (ja),データ分析ツールとしてPythonとRはそれぞれの強みがあります。このtalkではPythonとRを両方使って分析を進めるための，いくつかのデータ交換手段を紹介します。 CSV経由以外のPythonとRでデータ交換をする方法 データ分析ツールとしてPythonとRはそれぞれの強みがあります。このtalkではPythonとRを両方使って分析を進めるための，いくつかのデータ交換手段を紹介します。以下のデータ交換手段やデータ形式を紹介する予定です。 - CSV - Excel - rmagic - RData - feather,False
2017,https://pycon.jp/2017/ja/proposals/vote/137/,Rage Against The Learning Machine (en),"Machine learning is ranked numero uno in Gartner’s Top 10 Strategic Technology Trends for 2017. In recent years, it has received a lot of attention and already revolutionized many areas from finance to image recognition to transportation.  Working examples in Python and couple of tools such as word2vec (Google) and fasttext (facebook), might be demoed to prove some love/points. This session, aims to make machine learning simple, fun and accessible to anyone.  Machine learning is ranked numero uno in Gartner’s Top 10 Strategic Technology Trends for 2017. In recent years, it has received a lot of attention and already revolutionized many areas from finance to image recognition to transportation. This talk, aims to make machine learning simple, fun and accessible to anyone. Working examples in Python and couple of tools such as word2vec (Google) and fasttext (facebook), might be demoed to prove some love/points.",False
2017,https://pycon.jp/2017/ja/proposals/vote/38/,トピックモデルと自然言語解析における前処理の効果 (ja),自然言語を扱うことになった際に、いくつかの考えるポイントがあると思います。前処理をどうしたらいいのか(そのままTerm Frequencyでベクトル化していいのか、IDFも考慮すべきなのか、BM25を用いるべきなのか)という問題があります。これらを実際のデータを用いてそれぞれどのような結果に結びつくか、ということと、トピックモデルを使った言語解析について発表したいと思います。 自然言語処理をする際の一連の処理及びその効果、トピック抽出の方法 自然言語を扱うことになった際に、前処理は自然言語処理において大きな結果を左右する項目の一つになります。そのまま単語の出現回数を使うべきなのか、特定の分野にのみ頻出するような単語の値が大きくなってほしいのか、それとも文書長も考慮したようなベクトル化がいいのか、など様々な単語のベクトル表現が存在します。更に「コンピュータ」と「コンピューター」など表記揺れや表記ミスなど様々な要因で表記が異なることは多々あります。本発表ではこれらの問題についての対処法を述べると共に、前処理と結果との関連について発表したいと考えています。,False
2017,https://pycon.jp/2017/ja/proposals/vote/32/,Python と Email ヘッダ (ja),"こんにち、クラウドメールサービスの普及により、""Email"" という形式のデータを我々が直接扱う機会は少なくなりました。しかしながら、もしも「Email を Python で扱わなくてはならない」としたら、どのような方法や課題が存在するのでしょうか。このセッションでは、とある事情から Python で Email を処理しなくてはならなくなった「わたし」が実際に遭遇したさまざまな困難と課題、そしてそれらの解決方法についてのアウトプットを行います。Email や Email ヘッダの仕様について、あらためて ""フォロー"" してみてはいかがですか？ このセッションをつうじて、Email にまつわる RFC や、Email ヘッダの正しい仕様についての理解を深められます。あわせて現実に存在する Email を扱うことの難しさと Python での課題の解決方法について知り、ナレッジを持ち帰ることができます。 *""Email""* は身近な存在ですが、じつにさまざまなプロトコル や RFC の積み重ねにより成り立っている歴史と伝統のある情報技術です。こんにち、クラウドメールサービスの普及により、Email という形式のデータを我々が直接扱う機会は少なくなりました。しかしながら、もしも「**Email を Python で扱わなくてはならない**」としたら、どのような方法や課題が存在するのでしょうか。メールヘッダの種類やフォーマット、メールアドレスに利用できる文字列は [RFC  2822](https://www.ietf.org/rfc/rfc2822.txt) および [RFC 5322](https://www.ietf.org/rfc/rfc5322.txt) で定められています。Python の標準ライブラリに存在する [email](https://docs.python.org/3/library/email-examples.html) は優秀な Email / Email ヘッダ パーサーとして利用できます。これらの使用や技術を採用すれば、問題無く Email を扱えるように思われます。しかし、現実にはさまざまな Email の実装が存在し、一筋縄にはいきません。文字コードの問題もあります。添付ファイルの形式や、添付ファイルのファイル名の形式にも注意が必要です。Email ヘッダを正しく理解しないと、BCC アドレスを適切に指定することもままなりません。このセッションでは、とある事情から Python で Email を処理しなくてはならなくなった「わたし」が実際に遭遇したさまざまな困難と課題、そしてそれらの解決方法についてのアウトプットを行います。Python と Email の関係、Email ヘッダの仕様について、あらためて **""フォロー""** してみてはいかがですか？",False
2017,https://pycon.jp/2017/ja/proposals/vote/97/,Pythonによるプログラミング研修 ― 社内新人教育における実践報告 (ja),昨年度より、私の所属する部署（株式会社インサイトテクノロジー 札幌開発センター）においては、新人教育におけるプログラミング研修を全てPythonを使って行っていますが、どんな教材を使ってどのように研修を行い、どのような成果が得られたのか、またその過程において得られた知見や問題点について、企業内で教育を担当している方々にとって直接お役に立てるようなお話をしたいと考えています。 機械学習、特にDeep Learning関連の製品やサービスを提供する上で避けて通れないのがPythonによるプログラミングですが、まだまだ日本国内ではPythonによるプログラミング研修を行っている企業は少ないようです。この講演により、参加者はプログラミング研修におけるPythonの利点および問題点を把握し、新人教育や中堅エンジニアの再教育に直接役に立つ知見が得られることでしょう。 昨年度より、私の所属する部署（株式会社インサイトテクノロジー 札幌開発センター）においては、新人教育におけるプログラミング研修を全てPythonを使って行っていますが、どんな教材を使ってどのように研修を行い、どのような成果が得られたのか、またその過程において得られた知見や問題点について、企業内で教育を担当している方々にとって直接お役に立てるようなお話をしたいと考えています。,False
2017,https://pycon.jp/2017/ja/proposals/vote/138/,The Malcolm Effect : A Tribute (en),"4 years ago, I managed to work with Mr Malcolm Tredinnick as his Devops Engineer. In fact, his name was one of the reason why I left government linked companies to join few other superhumans, in one of the most exciting journey of my life. This talk will try to share some thoughts, knowledge and ideas, from our short Kuala Lumpur-Sydney relationship with Malcolm Tredinnick.  This talk will try to share some thoughts, knowledge and ideas, from our short Kuala Lumpur-Sydney relationship with Malcolm Tredinnick. 4 years ago, I managed to work with Mr Malcolm Tredinnick as his Devops Engineer, at a Sydney based Digital Communication company. In fact, his name was one of the reason why I left government linked companies to join few other superhumans, in one of the most exciting journey of my life. I believe that those who had worked with Malcolm knows, and definitely appreciate every minutes spent with him. But, god loves him more. Our master-apprentice session could only be lasted about 4 months, before he left us all. This talk will try to share some thoughts, knowledge and ideas, from our short Kuala Lumpur-Sydney relationship with Malcolm Tredinnick. ",False
2017,https://pycon.jp/2017/ja/proposals/vote/40/,Kivyによるアプリケーション開発のすすめ (ja),KivyはPythonのGUIライブラリーとして海外では積極的に取り上げられていますが日本語での使い方は少ないです。今回はKivyでどういったアプリケーションが作成できるかや、また日本語環境による特有のはまりどころを紹介します。 kivyでどんなものを開発できるかを知ることができます。デスクトップ、モバイルでスタンドアローンアプリをPythonのみで開発してリリースすることができます。 KivyはクロスプラットフォームGUIライブラリーで2017年現在も積極的な開発がなされておりPythonの公式ドキュメントでも[グラフィックユーザインタフェース FAQ](https://docs.python.jp/3/faq/gui.html#kivy)の項目で紹介されています。しかしながら日本語でのまとまった解説が少なく、具体的にどんなものが作成できるか、どのように作成したらいいかの情報が少ないのが現状です。本公演は実際にどのようなアプリケーションをどうしたら作成できるかを紹介します。紹介する内容は以下の通りです。コードと内容の説明をしますがハンズオンは行わない予定です。+ 基本的な画面の作成方法+ 文字、画像の表示方法+ ボタンの使用方法+ ボタンとレイアウトを使用：電卓アプリケーションの作成方法の紹介+ タイマー機能を使用：時計アプリケーション、ストップウォッチアプリケーションの作成方法の紹介+ グラフ描画機能：グラフの作成方法の紹介+ ネットワークとの連携：webAPIとの連携方法の紹介+ デスクトップアプリへのexe化の方法+ モバイルアプリ(android/ios)の作成方法+ Kivyの強みと弱みの紹介+ 日本語入力、表示の問題と対応方法  参考：有志による[公式サイト](https://kivy.org/docs/)の日本語翻訳サイト：[https://pyky.github.io/kivy-doc-ja/](https://pyky.github.io/kivy-doc-ja/),False
2017,https://pycon.jp/2017/ja/proposals/vote/155/,Pythonにおけるデバッグ手法 (ja),1ファイルのスクリプトから、様々なフレームワークを利用したコード、様々なところでバグは発生します。それらのバグの解消を速やかに行えるように、デバッグのツールやノウハウが進化しています。デバッグ手法の意義について学び、また各種デバッグツールとその使い方を解説します。また実業務で発生する実際のデバッグしたい環境を想定したデバッグ手法の解説、デバッグに関する失敗談を解説します。 デバッグについて学ぶ意義、デバッグのツールや使い方や、実際にデバッグを行う際のノウハウ、失敗事例からバッドプラクティスを学びます。 様々なところでバグは発生します。それらのバグの解消を速やかに行えるように、デバッグのツールやノウハウが進化しています。このセッションではデバッグの意義から、実際に使えるノウハウやバッドプラクティスを解説します。以下の内容について解説します。- デバッグの意味- デバッグする-  デバッグでよく用いられるツールのメリット/デメリット    ipdb/bpdb/pudb/pycharmの紹介- 実業務の様々な環境を想定したデバッグのノウハウ- デバッグ失敗談から学ぶバッドプラクティス,False
2017,https://pycon.jp/2017/ja/proposals/vote/9/,Pythonで大量データ処理！PySparkを用いたデータ分析のきほん (ja),昨今では大量データの分析や機械学習のニーズが増えています。Apache Sparkは、高度なCPUやメモリ利用の効率化が行われた並列分散処理フレームワークとして、ビッグデータアーキテクトやデータサイエンティストの中で最も注目を浴びるプロダクトの一つとなっています。しかしながら、Sparkをきちんと扱うためには、分散処理における勘所を理解する必要があります。本発表では、Sparkをこれから始めるという人も、すでに導入しているという人にもわかりやすく、Sparkのアーキテクチャを説明します。また、PySparkを用いた分析基盤の開発と実運用を通してのはまりどころを紹介します。 PySparkを用いたビッグデータ処理や機械学習について学ぶことができます。 # SparkについてSparkは、pandasで扱うことが難しい、数GB以上といった大量データの処理を行うのに適したライブラリです。1つのライブラリでリスト処理やSQLライクな処理を行うことができ、機械学習やストリーム処理を行うAPIも用意されています。米国で毎年行われているSparkSummitでは、今年の参加人数は3000人を超え、ディープラーニングの事例なども多数報告されています。SparkにはPython以外に、Scala、Java、R等のインタフェースがありますが、データサイエンス場面での利用により、PySparkユーザの割合が拡大しています。講演前半においては、大量データをどうやって効率的に処理するのか、そのアーキテクチャを紐解いて解説します。# RettyにおけるPySpark事例についてRettyでは、ユーザ数が急増しており、日々拡大するデータに対する複雑な分析に対応するため、次世代分析基盤を構築しました。この際、PySparkを用いたETL処理でスケーラブルなアーキテクチャを実現しています。講演後半においては、* なぜPySparkを選んだのか* スケーラブルなアーキテクチャを実現するための工夫* 時系列データの複雑な処理に関するノウハウ* パフォーマンス・チューニングのノウハウなど、Rettyの分析基盤におけるPySparkを用いた開発と実運用を通しての知見をお伝えします。,False
2017,https://pycon.jp/2017/ja/proposals/vote/148/,Pythonをとりまく並行/非同期の話 (ja),Pythonにおける並行/非同期処理を広く浅くまとめて紹介します。並行/非同期処理を理解するためには色々なトピックに触れる必要がありなかなか理解するまでに時間がかかるケースがあると思います。本セッションでは並行/非同期処理に触れる際に頻出するトピックをPythonコード/ライブラリの話を交えながら、紹介していきます。 - Pythonをとりまく並行/非同期処理の全体像を掴むことができます。- 全体像を掴むことで、ユースケースに応じて適切な手法を選ぶことができるようになります。 Pythonで並行処理や非同期処理を学ぼうとした時に、様々なトピックが頻出して理解するのが難しいと感じたことはありませんか？本セッションではPythonが言語として備える機能や、ライブラリとして提供される並行処理や非同期処理について、基本的な知識、昔からある手法や、2017年時点でホットなトピックを、広く浅くまとめて紹介したいと思います。大まかに下記のトピックを中心に話します。- 並行処理とは- 非同期処理とは- Pythonではどのような機能があるか- ライブラリはどのようなものがあるか- どのようなユースケースで使うと良いか,False
2017,https://pycon.jp/2017/ja/proposals/vote/98/,Pythonで実現する4コマ漫画の分析・評論 2017 (ja),前年は4コマ漫画を対象にコマ毎の画像を切り出す手法までについて発表しました。今年はそれらの画像からデータを抽出して分析・評論を行うまでを発表したいと思います。 Pythonを用いての画像処理、機械学習、分析の手法をいかに自分のやりたいことに役立てるか。4コマ漫画の分析・評論についての知識。 - 前回（コマ毎の切り取りまで）の発表：https://www.slideshare.net/esuji/30-41-66333316- コードのあるリポジトリ：https://github.com/esuji5/yonkoma2data#### 導入日本に限らず海外でも漫画表現として用いられる「4コマ(yonkoma)」。近年、アニメ原作に選ばれる数も増えており、その技術解析・分析・評論を行い、読者・漫画家・編集者を含めてその成果を共有することは文化的に大きな価値があると言えます。本Talkでは、その序論として4コマ漫画のデータとして扱う手順並びに、統計や機械学習を用いた分析をPythonで一気通貫に行う手法を紹介します。#### なぜ4コマ漫画なのか- ある程度決まった形式の表現なので機械的に処理しやすい- WEB上で目にすることの多い漫画表現、アニメ原作に選出される頻度が多くなってきた等の文化的価値#### 分析結果をどう使うか- カメラワーク、キャラクター配置、会話パターンのレコメンド- 4コマ漫画をラフネームレベルで自動生成- フリー素材のイラストを用いて配置することで自動生成もできるかも？#### データを集める- それぞれのコマに切り出す(前回までで完了)- コマ上に配置されたデータ  - 人物    - アニメ顔検出器で正面の顔は結構取れる    - ↑をベースに発展させた検出器をdlibを用いて作成    - 検出後、人物の分類はCNNで行う  - 吹き出し    - Google Cloud Vision APIでセリフをOCR。縦書きの日本語も認識可能      - 関係ない線も文字と認識されてゴミになるので取り除く    - OCRの結果を元に吹き出し領域を検出し、そのコマにおけるセリフの順番を決める- データの活用  - データを分析する    - 人物配置・カメラワークの分析    - 人物配置のパターン    - イマジナリーライン超えの率    - 会話内容の分析    - 頻出語を調べる    - 会話遷移のパターン      - 誰が話を振り、誰が話を広げ、誰がツッコむのか      - ギャグを言う、受けない、ディスコミュニケーション  - セリフ、人物、状況から該当するコマを検索するシステムがあるといいよねえ（優先度低い）,False
2017,https://pycon.jp/2017/ja/proposals/vote/124/,A peek into Pyglet - the interactive GUI library for Python (en),"Pyglet is an open source, object-oriented, event-driven, cross-platform, windowing and multimedia library for Python which helps to create games and GUI applications. Its features include full multimedia support, mouse and keyboard hooks, OpenGL extensibility and much more.Attend this talk to learn how Pyglet can simplify your GUI and Game programming experience. In this talk, the audience will learn the following things.1. Scope of current GUI libraries in Python.2. Good basic knowledge of Pyglet library. This will enable them to create their next GUI game in minutes.  Do you find building GUIs in Python difficult? Games are certainly impossible, right? Well worry no more, meet **Pyglet**, the object-oriented, event-driven, cross-platform, windowing and multimedia library for Python. It can be used to create games and other visually rich applications. And that too without depending on external dependencies like Qt, GTK+ or Tk. Just `pip install pyglet`, include it in your project and you are good to go. It also has good multimedia support featuring videos, audios and images. So whatever the interface you have in mind, Pyglet can create it.But wait, don't be fooled by its simplicity. Pyglet is very modular and is built using OpenGL under the hood. It comes with mouse and keyboard hooks and has an asynchronous event framework. So it is a simple as well as powerful tool for your next interactive GUI project.Attend this talk to learn more about Pyglet and its goodness.",False
2017,https://pycon.jp/2017/ja/proposals/vote/45/,pytest入門 (ja),pythonは標準ライブラリにunittestモジュールを持っているため追加のテストツールを利用せずとも自動テストを利用できます。pytestはunittestに比べ、楽にテストを書くことができ豊富なプラグインを利用できるテストツールです。pytestの利点欠点から実践的な利用方法を解説します。 pytestを利用したテストを書けるようになる pytestを使ったユニットテストの方法やおすすめのディレクトリ構成、プラグインの利用など,False
2017,https://pycon.jp/2017/ja/proposals/vote/46/,Pythonで作ったアプリケーションの一般向け配布を模索する (ja),py2exeやcx_Freeze、PyInstallerといったアプリケーションをパッケージングするツールを紹介します。なお、当方Macの環境は無いのでiOSとMacOS向けについては解説しません。 py2exeやcx_Freeze、PyInstallerの使い方。Pythonで作ったアプリケーションの配布方法。 Pythonで作ったアプリケーションを動かすには、ランタイム環境が必要になります。py2exeやcx_Freeze、PyInstallerを使ってアプリケーションをパッケージングすると、利用者は簡単に利用できます。,False
2017,https://pycon.jp/2017/ja/proposals/vote/139/,A Django Tale  (en),"Django is a high-level python framework which encourages rapid development, clean and pragmatic design focusing on automation which is widely supported with many deployment options. The talk would be all about a tale of building a  budding  crowd sourcing platform which was built  using the Django framework. The attendees would get close to the Django framework and know the use cases of the framework.Along with this they would know about a crowd sourcing platform which was built using Django. Developers are constantly searching for the best. They seek the best language to code in, the best tools to use, and they are always looking for what is at the forefront of development. Django as the web framework of your project is the best way to turn your idea into business reality. Perhaps you’ve heard of Instagram, Pinterest, Disqus, OpenStack and BitBucket that use the Django framework. **You could attend the talk to know more about Django, its applications and a tale of  a crows sourcing platform built using Django.**",False
2017,https://pycon.jp/2017/ja/proposals/vote/59/,脱写経のススメ　〜自分のライフログデータで遊んでみよう〜 (ja),"初心者といえども，いつまでも書籍の写経だけでは飽きてしまいます。もっと身近なデータで楽しくデータ分析や地図情報プログラミングを学んでみたい！そんな動機で，スマホのライフログアプリMovesのサーバーからAPIで取得したライフログデータを使って遊んでみました。簡単なデータ分析や地図上への位置データの表示を楽しく学びます。初心者にはハードルの高いAPI経由のデータ取得にも挑戦しています。 以下の3点をお伝えします。1)APIで提供されるデータを取得して，pandasを使って簡単なデータ分析と可視化を行う方法(bottle, configparse, requests, pandas使用)。2)緯度，経度の情報を元に地図上にプロットする方法(foliumパッケージ使用)。3)すべてが用意されている写経と異なり，自分でやりたいことを模索するときに壁を乗り越えるコツ ## 脱写経のススメ初心者のうちは，参考書を買ってきて，まずはそこに書いてあるコードを写経して学びますよね。最初のうちは必要なことだと思いますが，私は飽きっぽいので写経だけだとつまらなくなってしまいます。だって，アヤメの花の長さと幅のデータ，とか昔のアメ車の燃費データとか，興味ありませんから。それに，写経したときは動いていたスクリプトも，いざ自分で書こうとするとうまくいかない，なんてこともよくあります。やりたいことがあるなら，身近なデータを使って分析してみるほうが100倍(当社比)楽しいと思います。## でもまだ修行中身近なデータ，といえば業務で使っているデータが思い当たります。でも本格的に仕事で使うことができるほどPythonが書けるようになったわけじゃないし，納期が迫ってたりするとのんびり調べながらってわけにもいきません。それに，コンプライアンスがうるさい昨今，業務データの持ち出しは厳しく禁じられてますので，自宅に持って帰って週末いじってみるなんてこともできません。そもそも趣味なのに，仕事のデータを分析するなんて楽しくないですよね。## 自分に一番近いデータ業務データより自分に近いデータ，それはライフログデータです。スマホを常に持ち歩くようになり，ライフログアプリを使っていれば自然とデータがたまります。万歩計のカウンターを毎日メモしていたのはそんなに昔のことじゃない気がしますが，便利な世の中になりました。自分自身のデータですから，100年も前のアヤメや20世紀の自動車データなんかよりよっぽど興味が持てますよね。私が使っているのはMovesというアプリです(iOS,Android両方あります)。このアプリはサーバー上にログデータが蓄積されていて，APIを使ってデータを取り出すことができます。今回はこのデータを使った簡単な分析と，せっかく位置情報も入っているので地図上への位置情報の表示にも挑戦しました。## 立ちはだかる壁初心者にとって，初めてやることはすべて壁に感じます。今回の壁は，- APIを使ったサーバーからのデータ取得- CSV形式じゃなく，JSON形式のデータの分析- 緯度経度の情報を使って地図上に表示の3つです。これらの壁をどのように乗り越えたのか，できるだけわかりやすく説明します。どちらかというとコードの中身よりも，どこが困ったのか，そういうときどうやって解決したのかについてのお話を中心にする予定です。具体的なソースコードはGitHubで公開します。",False
2017,https://pycon.jp/2017/ja/proposals/vote/99/,Python Prompt toolkitで作るサクサク動くインタラクティブCLI入門 (ja),Python Prompt toolkitはJonathan Slenders氏により作られたPythonで強力なインタラクティブCLIを実現するためのライブラリです。このトークでは、Python Prompt toolkitの活用事例と、このライブラリを使ってサクサク動くインタラクティブCLIツールを作るための実装ポイントをご紹介します。 Python Prompt toolkitを使ってみなさんがお作りになる、もしくは既存のコマンドライン（CLI）ツールをインタラクティブでクールなものにするための方法をご理解いただくことを目的としております。 [Python Prompt toolkit][1]はJonathan Slenders氏により作られたPythonで強力なインタラクティブCLIを実現するためのライブラリです。このトークでは、Python Prompt toolkitの活用事例と、このライブラリを使ってサクサク動くインタラクティブCLIツールを作るための実装ポイントをご紹介します。  [1]: https://github.com/jonathanslenders/python-prompt-toolkit,False
2017,https://pycon.jp/2017/ja/proposals/vote/126/,Pythonの本気！RaspberryPiやEdisonを使ったIoTシステムの構築 (ja),IoTの仕組みをPythonで殆ど構築できるので、その実践と紹介を。・サーバー機能 Flask・ネットワーク機能 Requests・データーDB　SQLite・数値計算　NumPy・ハードウェア制御　akilibほぼPythonのみで完成したIoTシステムが構築できます。 IoTの仕組みをPythonで殆ど構築できるので、その実践と紹介を。・サーバー機能 Flask・ネットワーク機能 Requests・データーDB　SQLite・数値計算　NumPy・ハードウェア制御　akilibほぼPythonのみで完成したIoTシステムが構築できます。これらライブラリをどのように組み合わせ、どのようにシステムを構築していくか、気になるポイントを詳しく紹介する予定です！！お楽しみに！---------------------------------------- IoTの仕組みをPythonで殆ど構築できるので、その実践と紹介を。・サーバー機能 Flask・ネットワーク機能 Requests・データーDB　SQLite・数値計算　NumPy・ハードウェア制御　akilibほぼPythonのみで完成したIoTシステムが構築できます。これらライブラリをどのように組み合わせ、どのようにシステムを構築していくか、気になるポイントを詳しく紹介する予定です！！お楽しみに！----------------------------------------,False
2017,https://pycon.jp/2017/ja/proposals/vote/140/,Pythonを支える技術: プロトコル編 (ja),"Pythonには文字列やリストなど、長さをもつオブジェクトがあります。長さを手に入れるには、文字列なら、name.length ではなく len(name) のようにlen関数を使います。len関数は引数に渡されたオブジェクトと通信して、オブジェクトの長さを手に入れています。このときの通信の決まりをプロトコルと呼びます。if文がオブジェクトの真偽値を手に入れるのも、for文がオブジェクトからイテレートするオブジェクトを手に入れるのも、プロトコルで決まっています。この発表では、PEP-544で定義されているPythonのプロトコルについて説明し、自分で実装するときにどう実装すればlen()やif文やfor文に適合したオブジェクトを表現できるのか説明します。 Pythonのオブジェクトの動作がどうやって決まっているのかを把握してもらうために、具体的な実装と動作を紹介します。その背景には、Pythonのプロトコルという概念があります。本トークではPEP-544で定義されているプロトコルの概念を説明し、その応用例を紹介します。 - Pythonデータ型再入門- `len()`, `if`, `for` に適合したオブジェクトの実装例- PEP-544 プロトコル: データ型が長さを返す、条件式に使える、繰り返し可能、のような動作を満たす条件- collection.abc を使ったプロトコル実装の強制- TypeHints によるプロトコルチェック- まとめ",False
2017,https://pycon.jp/2017/ja/proposals/vote/149/,Pythonのデバッグ/プロファイルリング (ja),Pythonでデバッグやプロファイリングをする一般的な方法について紹介します。普段Pythonコードを書いてハマった時、より効率よく改善したい時に、使える手段/ライブラリを広く浅く紹介します。なるべく初心者にもわかり易いようにPythonコードを交えながら話します。 Pythonプログラムを修正/改善する時に役立つツール/方法を一通り知ることができます。 Python で デバッグ、プロファイリングをする時に、一度は触るであろうpdbやcProfileを皮切りに効率良くデバッグ、プロファイリングをするための方法、より良いツール/ライブラリ群の紹介をしたいと思います。主に扱うトピックは以下のようなことを考えてます。- デバッグ&プロファイリング- ボトルネックを見つける- パフォーマンス改善Tips- ライブラリ&ツールの紹介,False
2017,https://pycon.jp/2017/ja/proposals/vote/163/,AWS APIGateway + Python Lambda + NEologdで作るサーバレス日本語形態素解析API (ja),PythonとMeCab+NEologdを使用した日本語形態素解析環境は様々な場面で広く利用されていますが，容量などサイズも比較的大きくなりがちで，サーバレス環境のような気軽な環境で動作させることは容易ではありません．本トークではAWSのサーバレスサービスであるLambdaを使用して日本語形態素解析環境をPython + MeCab/NEologd + AWS Lambdaで作成する際に有効なTipsを中心に紹介します． 自然言語処理を行う上で重要な日本語形態素解析基盤をPythonとサーバレスアーキテクチャの組み合わせで構築する手法を学びます． PythonとMeCab+NEologdを使用した日本語形態素解析環境は自然言語処理やテキストマイニングなど様々な場面で広く利用されています．一方でこれらの環境を開発環境と合わせて使用するためにはOSや文字コードなどの依存関係でセットアップやデプロイが面倒なことも多く，Docker化しても辞書のサイズが大きくコンテナが肥大化していくなど，簡単な日本語解析を行えれば良いようなケースにおいても比較的大きなリソースが必要とされます．このようなケースにおいては，必要な機能をAPI化してできるだけ安価に利用できる環境をAWSのサーバレスサービスであるLambdaような環境に用意することが理想的ですが，AWS Lambdaでは容量の制限も厳しくMeCab+NEologd環境はそのままでは実行することができません．本トークではこのような問題をPython実装で補いつつMecab + NEologdの良いところ取りをしたLambda API環境を構築する方法を紹介します．,False
2017,https://pycon.jp/2017/ja/proposals/vote/100/,AWSのマネージドサービスを活用したPython Web Applicationの開発 (ja),AWSのマネージメントサービスを活用することで、スケーラブルなシステムを簡単に展開することができます。とある新聞社のニュースアプリ開発事例を元に、AWSの様々なマネージドサービス（Lambda、API Gateway、ECS等）で展開可能なPython Web Applicationの開発についてご紹介します。 Pythonで書いたWeb Applicationの開発、AWSへのデプロイと運用のノウハウ Pythonで書いたコードは、AWSの様々なマネージメントサービスで動かすことができます。こうしたAWSのマネージドサービスを活用することで、スケーラブルなシステムを、最小限の費用で簡単に構築することが可能となります。しかし、マネージドサービス上にPythonのアプリケーションを展開するためには、AWSの知識と、ちょっとした工夫が必要となります。本セッションでは、とある新聞社のニュースアプリ開発の実例を交えながら、以下の点について紹介していきます。- 成果物のパッケージングとデプロイ- WSGIアプリをAPI Gateway / Lambdaで動かすためには- Pythonで書いたバッチ処理（記事の自動編集・ユーザーログ分析等）をLambda / ECSで動かすためには- エラーログの収集、死活監視、メトリックス分析,False
2017,https://pycon.jp/2017/ja/proposals/vote/57/,データ可視化ツールの選び方と学び方 (ja),Pythonには matplotlibなどさまざまなデータ可視化のためのライブラリがあります。しかし、可視化ツールは学習コストが高いことから、ウェブ上にあるコードを参考になんとなく書いているという方が多いのではないでしょうか。本発表では、目的にあった可視化ツールの選択法を提案したうえで、代表的な可視化ツールである matplolib での描画のポイントや、Bokehなどを使ったその他の可視化の方法について紹介します。  Python の可視化ツールは数多くありますが、どのツールを選択するのがよいのかという情報はほとんどありません。また、可視化ツールは独自のルールが多く、思いどおりのグラフを描画するためには学習コストがかかります。多くの方はネット上に見られるサンプルを参考に、手探りでコードを書いているのではないでしょうか。本発表では目的に合った可視化ツールの見つけ方、Python らしい可視化のためのコードの書き方、Jupyter Notebookを使ったいくつかの可視化ツールの基礎を紹介します。 ## 1. 可視化ツールの選び方     * 多様な可視化ツール* どの可視化ツールを選んだいいの？可視化ツールの選び方    ## 2. matplotlibをしっかり使いこなす   * 5分で理解する matplotlib による可視化の基本   * matplotlibを使った可視化    ## 3. そのほかの可視化ツール    ### pandas    * 一行でここまで可視化できる   * pandasで凝ったグラフを描く   ### Seaborn   * 見た目を整えるだけのパッケージだと思っていませんか？   *  Seabornの機能を活用する    ### Bokeh    * 5分で理解する Bokeh による可視化の基本   * Bokehならではの機能紹介    ,False
2017,https://pycon.jp/2017/ja/proposals/vote/54/,len()関数がオブジェクトの長さを手にいれる仕組み (ja),Pythonには文字列やリストなど、長さをもつオブジェクトがあります。長さを手に入れるには、文字列なら、name.length ではなく len(name) のようにlen関数を使います。len関数はどうやってnameに入っているオブジェクトの長さを手に入れているのでしょうか。if文にはTrue/Falseとなる条件式を指定しますが、それだけでなく文字や数字、自分で作ったデータ型も渡せます。if文はどうやって与えられたオブジェクトがTrueなのかFalseなのかを手に入れているのでしょうか。この発表では、Pythonのプログラムがどうやって必要な情報を手に入れているのか、また、自分で実装するときにどう実装すればlen()やif文やfor文に指定できるのかを説明します。 Pythonのオブジェクトの動作がどうやって決まっているのかを把握してもらうために、具体的な実装と動作を紹介します。その背景には、Pythonのプロトコルという概念があります。しかし、本トークでは難しい概念は置いといて、Python初級者に分かりやすい言葉で仕組みを説明していきます。そして、中級者へとステップアップする足がかりとなる情報をお伝えします。 - 脱初心者、のためのデータ型入門- `len()` はどうやってオブジェクトの長さを手に入れているのか- `if` はどうやって指定したオブジェクトを使っているのか- `for` はどうやって指定したオブジェクトを使っているのか- データ型が長さを返す、条件式に使える、繰り返し可能、のような動作を満たすための条件、プロトコル- まとめ,False
2017,https://pycon.jp/2017/ja/proposals/vote/150/,KerasのステートフルなRNNを使って自動演奏できないか実験してみた (ja),最近、GoogleがMagentaを始めとして、人工知能で作曲する試みが出てきたが、その学習元および出力はmidiという楽譜ファイルであることがほとんどである。今回、音の波形そのものを入力及び出力にした自動演奏機を、最近人気の機械学習フレームワークKerasのstateful RNN を使って実装できないか検証した。 ステートフルなRNNのKerasでの実装方法を知る。Pythonで音声の波形を編集する方法がわかる。 自動作曲プロジェクトが最近盛んになってきていて、特にGoogle のMagentaプロジェクトが有名です。しかし、これらのプロジェクトで使用されているのは、大体がmidiファイルという楽譜ファイルであり、録音した音楽のような「音そのもの」を入力したりするのはあまり見かけません。今回、音そのものを入力とした機械学習型の自動演奏機を、最近人気の機械学習フレームワークであるKerasを使って実装してみました。学習に使用する音声ファイルはmac上で流れる音楽を録音し、Pythonでネイティブに取り扱えるwavファイルに変換したものを使います。このwavファイルをさらに一定時間ごとにフーリエ変換し、周波数スペクトルの時間変遷を入力データとして用意します。次に自動演奏機のモデルをKerasを使って形成します。ここではKerasが用意している[stateful な RNN](https://keras.io/ja/getting-started/faq/#stateful-rnn)を利用することを試みます。statefulなRNNは一度入力があって内部状態が変更された後、その状態を初期状態として、次の入力に対する処理を行います。これにより、不定期間の曲調などを内部状態に取り込めるのではないかと期待しています。学習は至って簡単で、入力として各音声データのある瞬間の周波数スペクトルを使い、出力の教師データとしてその次の瞬間の周波数スペクトルを使用します。これによって得られるモデルは、現在のスペクトルを入力として次のスペクトルを作り、またそのスペクトルを使って次の次のスペクトルを作る...というジェネレータになります。最後に、このモデルを利用して実際に自動演奏機を動作させてみて、しっかり演奏できているのかを確かめます。現在ではそれほど大きな成果は出ておらず、辛うじてリズムを刻んでいるような雰囲気が感じられ、その他はノイズの塊のような状況になっており、実験や改善を繰り返す必要があります。,False
2017,https://pycon.jp/2017/ja/proposals/vote/164/,ScrapyではじめるWebスクレイピング入門 (ja),"Webページからデータを抜き出すWebスクレイピングはメジャーなソフトウェア技術となってきています。Pythonで作られたWebスクレイピングフレーム Scrapy を使用して、本格的なスクレイピング用を行う手順を、ステップ・バイ・ステップで解説します。 Webスクレイピングフレームワーク「Scrapy」の概要と基本的な使い方を理解します。スクレイピングのプログラムを書いていく過程を見ていくことによって、Webサイトを調査してコードを書いていく具体的な流れを学ぶことができます。 Webサイトから情報を収集するWebスクレイピングの技術は、データを収集するためにはかかせない技術になっています。このトークでは Python 製のWebスクレイピングフレームワークScrapyを使用して、Webサイトからデータを抜き出す手順についてステップ・バイ・ステップで解説します。* scrapyのアーキテクチャ* スクレイピングプロジェクトの作成→デフォルトで設定すべき項目* Webページから単一の情報を取り出す* Webページからデータを抜き出すための情報を調査する(Scrapy shell)* 取り出した情報をCSV, JSONファイルに保存する* 複数のページから情報を取り出す* Scrapyのスクレイピング機能のまとめ* Scrapyのその他の機能",False
2017,https://pycon.jp/2017/ja/proposals/vote/96/,飲食店さんをささえる技術〜LuigiとTreasure Dataではじめるデータ集計基盤運用 (ja),"Rettyではユーザーさん(fan)とお店さん(飲食店)をつなげ,広めるサービスとしてFRM(Fan Relationship Management)というサービスを運用しており,日々お店さんとユーザーさんの架け橋となる情報を提供しています.このセッションではデータ集計基盤として活用している「Luigi」「Treasure Data」を中心に,構築から運用のノウハウについてご紹介いたします. luigiの仕組み. 一日数百万〜数千万レコードのデータを裁く基盤の運用・構築ノウハウ.to BサービスにおけるWebおよびbatchのプロダクトの運用・構築ノウハウ. # 飲食店さんをささえる技術〜LuigiとTreasure Dataではじめるデータ集計基盤運用## あらすじユーザーさん(fan)とお店さん(飲食店)をつなげ,広めるサービスとしてFRM(Fan Relationship Management)というサービスを運用しております.FRMサービスはお店さん向けのダッシュボード(予約・広告管理),社内のバックオフィスや営業が用いるツールなど,大小様々なプロダクトで構成されていますが,その中の基盤の一つとして,luigiとTreasure Dataを元に構築した大規模データ集計基盤があります.このセッションでは,* Rettyにおけるデータ集計基盤の背景と歴史* Luigi/Treasure Data/AWS(ECS, Elastic Beanstalk)に関するノウハウ* 運用で良かった所・カイゼンしたいところと中心に,「実運用を通じて得た学び」を紹介したいと思います.## Keyword* データパイプライン(Luigi)* Cloud・インフラ(Treasure Data,AWS)* Python(とPHP/node)を用いたto Bサービス運用のノウハウ",False
2017,https://pycon.jp/2017/ja/proposals/vote/49/,Pythonによる文章自動生成入門！Python ✖︎ 自然言語処理 ✖︎ ディープラーニング (ja),◇Pythonを用いて、文書を自動生成する主要な３つの方法をお伝えします。１）マルコフ連鎖、 ２）自動要約、 ３）ディープラーニング（RNN/ LSTM）◇特にKeras/ Tensorflowによる文章自動生成はPythonで数行で簡易的に書けます。◇文章などのデータを非構造化データと呼びますが、身近でデータ収集がしやすく、個人の趣味やちょっとした研究に適しています。 ◇ディープラーニングの発展で、画像分類の精度向上が大幅に向上しました。◇画像分類だけでなく、自然言語や音声にも適用できます。◇今回は、Pythonを用いて自然言語処理の面からディープラーニングの有用性を検証してみたいと思います。 ◇今や非構造化データは溢れていて、最も入手しやすい資源です。これを次の3つの手法で、文章生成し、有用な利用方法を提案して見たいと思います。１）マルコフ連鎖、 ２）自動要約、 ３）ディープラーニング（RNN/ LSTM）◇用途として、例えば、亡くなったおじいちゃん、おばあちゃんの手紙があれば、その文面から、おじいちゃんやおばあちゃんのメールが届くなどの利用が考えられます。つらい時や悲しい時、励ましの文章が届くアプリなどの開発につなげられます！◇海外ではNatural Language Generation(NLG)として、コンペティションの大会が開催されていて、実はホットなテーマなんです！ー流れー◇１）−３）の理論的な説明➡︎コードの説明➡︎実演◇OUTPUT: １）マルコフ連鎖 ２）自動要約、 ３）ディープラーニング（RNN/ LSTM）◇FOLLOW: ポスターセッション（文章自動生成を体験しよう！知ろう！）でも詳しく説明いたします！※データ量: どのくらいの文字データがあれば、どのくらいの文章が生成されるのかについてのベンチマーク的な情報もお伝えしたいと思います。,False
2017,https://pycon.jp/2017/ja/proposals/vote/60/,Python: Super Power for Blended Learning (en),"I've been doing research in edtech and blended learning for two years and find out python is great super power in this area, in this talk I want to share how to design and implementing blended learning and how we can use python for data mining. - Design and Implementing Education Technology and Blended Learning- Extending EdTech with Python- Data Mining in Blended Learning I've been doing research in edtech and blended learning for two years and find out python is great super power in this area, in this talk I want to share how to design and implementing blended learning and how we can use python for data mining.Educational Technology is growing and going to disrupt online business nowadays. To be effective, educational technologies must be designed based on what we know about how people learn and of course development under open source umbrella will be more effective.Through this session, participants will learn the problem and concept how to design and develop the blended learning technology by using open source technologies, the idea of collaborative learning and active learning.In the last decades, the power of data mining and analytics has transformed instruction in blended learning. Increasingly, large-scale data is available on student learning and interaction online. Much of this data represents student behavior. This has allowed researchers to model and track many elements of student learning that were not previously feasible at scale: engagement, affect, collaborative skill, and robust learning. In turn, these models can be used in prediction of long-term student outcomes, and to analyze the factors driving long-term success of students, I will share how we use python as the super power in this field.",False
2017,https://pycon.jp/2017/ja/proposals/vote/101/,良いコードを書けてますか？Flake8を使ってPythonコードをチェックしよう (ja),Flake8はPythonコードのLintツールです。定期的に実行することでより適切なコードになるように指摘してくれます。チーム開発で取り入れることでメンテナンス性を高めたり、生産性向上につなげられます。このセッションではSideCIの中津川がFlake8の基本的な使い方とプラグインによる拡張方法、実際に使ってくれているチームなどのお話をします。 Flake8の知識。コードレビューの重要性と、Pythonプロジェクトにおける導入方法。 企業内でPythonを使って開発を行っているところは増えていますが、その際に個々人のスキルセットによってコードの品質がまばらになるのは避けたいでしょう。それらは負の遺産になって生産性低下につながってしまいます。改善策として知られているのがコードレビューになります。複数人の目を通すことによってコードの品質を向上させることができます。問題点としては大きなコスト（開発時間の10〜30%とも言われます）がかかることです。そこでお勧めなのがLintツールによるコードチェックです。これによってよくある一般的な指摘は自動化できます。Lintツールによって標準的な品質にした上で、次に人力などによるコードレビューを行ったとしても作業負荷が小さくなります。Pythonで知られているLintツールがFlake8です。SideCIはFlake8に対応しており、日々Pythonコードをチェックしています。このセッションではFlake8の基本的な使い方（ローカルでの使い方）やプラグインによる拡張方法、実際に Flake8をチーム開発で使っている事例などを紹介します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/50/,Experience Replay Bufferの実装 (ja),昨今の強化学習アルゴリズムでは学習の安定化のためにExperience Replay Buffer(ERB)が使われる。ERBにはエージェントが経験した環境や行動データを蓄積し、それらをランダムに抜き出して学習に利用する。ERBのサイズは数百万オーダに至るため、メモリを圧迫せずかつ高速にデータを取り出す仕組みが必要となる。本発表ではThreadingを用いた実装を紹介する。 Experience Replay Bufferをナイーブに実装すると、ハイスペックなPCを使用していない限りは学習の低速化やメモリオーバフローを引き起こす。本発表を見ることで、聴講者は一般的なラップトップPC場においても Deep Q Networkのようなアルゴリズムを用いた強化学習を動作させる音ができる。 　近年提案されている Deep Q Networkベースの強化学習手法ではExperience Replay Buffer(ERB)が使用されている。　ERBにはエージェントが経験した環境やそれに対する行動、得られた報酬などを蓄積しておき、価値関数をERBからランダムに抽出した経験に基づき学習させる。ERBに蓄積される経験の数は数十万から数百万オーダに至ることから、ERBのサイズがPCのメモリ容量を上回ることが容易にありえる。　そこで、本発表では一般的なラップトップPC上でも十分に動作するERBの実装について述べる。,False
2017,https://pycon.jp/2017/ja/proposals/vote/47/,仮想通貨ネットワーク上にDjangoサーバーを立てる (ja),仮想通貨（bitcoinなど）の内部で用いられているブロックチェーンという仕組みを使うと、記録が改ざんできないデータベースを作ることができます。このデータベース上にDjangoサーバーを立てて、サーバのプログラムやユーザーからの投稿などの改ざんや検閲ができないWebサービスを作ります。 非中央集権的なインターネットの可能性を感じることができます。 今回は、Ethereumという仮想通貨プラットフォーム上にサーバーを立てたいと思います。ふつうに、Djangoサーバーを立てた場合、こうなると思います。クライアント  ↓　　　↑  プロバイダー  ↓　　　↑  ApacheなどのWebサーバー  ↓　　　↑  Djangoアプリケーションしかし、今回考えるのは、このような形です。クライアント  ↓　　　↑  擬似Webサーバー   ↓　　　↑  プロバイダー  ↓　　　↑  Ethereum上にアップロードされたDjangoアプリケーション  「プロバイダー」が「Webサーバー」よりも下に来ています。つまり、クライアントのOSの中で、Djangoアプリケーションを動かしています。このようにサーバサイドの処理を全てクライアントサイドでやってしまおう、さらにプログラム本体もp2pネットワーク上にあげてしまおう、というのが今回の企画です。つまり、サーバという概念すら消えて、インターネットそのものがすっぽりクライアント（ノード）同士が繋がるネットワークの中に入ってしまうのです！,False
2017,https://pycon.jp/2017/ja/proposals/vote/51/,すぐ始められる、Pythonの環境構築 (ja),Pythonを学習したり、システムに組み込んだりする際の環境構築のベストプラクティスを紹介します。 Pythonが標準で準備している環境構築方法を知ってもらい、他の方法との比較ができるようなる Python実装系紹介、バージョンの選択、標準のインストール方法、仮想環境、パッケージインストール、エディタの選択を説明します。なお、この講演は、PyCon mini Kumamoto (2017年4月)にて発表した内容の再公演となります。(スライド: https://speakerdeck.com/terapyon/sugushi-merareru-pythonfalsehuan-jing-gou-zhu),False
2017,https://pycon.jp/2017/ja/proposals/vote/52/,JVM上で動くPython処理系cafebabepyの実装 (ja),cafebabepyというJVM上で動くPython3処理系を実装しています。Python3の言語仕様に悪戦苦闘し、どのように実装していったのかをお話します。言語実装の楽しさ、そしてPythonの合理的な言語設計、どのように動いているかについてを共有をしたいと考えております。cafebabeとはJavaクラスファイルのマジックナンバーです。JVM上で動くJythonは2.7(2015年)で更新が止まっているため、じゃあ作るか！というのが実装している理由です。 Pythonの言語仕様について何気なく使っているものがどのような仕組みで動いているかを知ることが出来る。言語処理系の作り方の一例を知ることができる。 cafebabepyというJVM上で動くPython3処理系の実装----------Python処理系を実装していく中で、おっ！おっ？と思ったことを基本に実装の流れを説明していきます。 - PythonのAST(抽象構文木)を作成する話。 - ASTから実際に処理を実行する話。 - Pythonのちょっとだけ深い言語仕様。 - PythonとJavaの境界線の実装。 - 実装における躓きポイント。,False
2017,https://pycon.jp/2017/ja/proposals/vote/56/,世界でいちばん簡単なSphinx入門 (ja),Sphinxは非常に強力なPython製のドキュメンテーションツールです。Pythonだけではなく、LinuxカーネルやCakePHPなど他言語の著名なOSSのドキュメントに使用されています。しかし、導入に際してreStructuredTextの壁、設定の煩雑さなどが原因で二の足を踏んでいる方が多いと思います。本セッションではマークダウンとGUIを使い、世界でいちばん簡単なSphinx入門をお教えします。 Sphinxで一番最初にぶつかるreStructuredTextの壁をマークダウンで回避し、Sphinxの便利な機能を体験してもらう。 Sphinxを始めるに当たって一番の障壁となるのはreStructuredTextでしょう。GitHubをはじめ多くのWebサービスがマークダウンを採用しています、すでにマークダウンで書かれたドキュメント資産があるという方もいるでしょう。しかし、SphixでマークダウンとreStructuredTextが併用できたら便利だと思いませんか？Sphinxをはじめようとしたけど`conf.py`の設定が難しくて断念した方はいませんか？もし、GUIから数回のマウスクリックでSphinxのドキュメントが作成できたら便利だと思いませんか？本セッションでは、GUIとマークダウンを使った世界でいちばん簡単なSphinxの使い方をお教えします。,False
2017,https://pycon.jp/2017/ja/proposals/vote/102/,Pythonで学ぶオブジェクト指向 (ja),オブジェクト指向の基礎知識から、Pythonによるオブジェクト指向の取り扱い方法を提言します。オブジェクト？クラス？インスタンス？といったオブジェクト指向の基礎知識から、実際にコードを書く方法、オブジェクト指向で書くことの利点などをお話します。 Pythonでオブジェクト指向なコードを書けるようになる。オブジェクト指向の理解が深まる。オブジェクト指向のメリット/デメリットを知る。 ### 話す内容 (暫定)* オブジェクト指向とはなにか？    - オブジェクト指向で出てくる用語の話    - オブジェクト指向の考え方* Pythonでオブジェクト指向なコードを書くには    - 普通に書いたサンプルコードを提示します。    - サンプルコードをオブジェクト指向で書き直していきます。    - 普通のコードとオブジェクト指向のコードを比較します。* オブジェクト指向のメリット/デメリット    - 先ほどのコードから、メリットの説明をします。    - 先ほどのコードから、デメリットの説明をします。* 考察    - 話した全体の内容から、考察します。### 技術* Python 3.6,False
2017,https://pycon.jp/2017/ja/proposals/vote/127/,PythonではじめるCT画像再構成 (ja),CT(Computed Tomography)はX線などを用いて物体内部を画像化する技術です。このセッションではCTの仕組みと画像再構成法を題材にして、Numpy/Scipyによる数値計算手法を紹介します。前提知識は基本的な行列計算のみです。 Numpy/Scipyを用いた具体的な数値計算の応用例(CT画像再構成)を紹介します。Numpy/Scipyの使用方法、および画像再構成の基本的な仕組みを知ることができます。従来C++などで作成していた部分を、どのようにすればPythonに置き換えられるかの１例を学べます。 CT(Computed Tomography)のプログラムは従来C言語やC++などで作成されていましたが、研究レベルではNumpyを利用することでPythonによる実装も可能となってきました。Numpyのおかげで、高速かつC++よりもはるかに可読性の高いプログラムを作ることができます。また、matplotlibを用いれば可視化が容易にでき、試行錯誤の多い研究用途ではPythonは便利なツールです。本セッションでは、その１例を紹介します。 本セッションの構成は次の通りです。- CT画像再構成の原理を解説(予定10分)- Numpy/Scipyによる実装方法の解説(予定10分)- その他、質疑応答(残り時間)本セッションでは次のような人を参加者を想定しています- CT画像再構成の基本的な仕組みに興味がある- Numpyを用いた具体的な数値計算の応用を知りたい- C++からの移行について知りたい,False
2017,https://pycon.jp/2017/ja/proposals/vote/141/,Python Test大全 (ja),"Pythonには公式から3rd-partyまで様々なtesting flameworkが存在します. 本セッションでは, 実際に仕事で使う場合での各testing flameworkの特徴や使い所について 解説していきます. 業務レベルでのPython programming testについてのノウハウ・tipsを学べる. 本セッションではPython programming で良く使われる testing flamework について実務レベルで解説していきます. - unittest- doctest- pytest- nose",False
2017,https://pycon.jp/2017/ja/proposals/vote/151/,ギリギリ昭和のエンジニアが語る和暦処理 (ja),あなたの生年月日、年号で出せますか？和暦は明治6年(西暦1873年)からグレゴリオ暦が導入されたこともあり、1873年1月1日から使うことが可能です。今回Pythonでこれを実現する仕組みの紹介とその中で得られた日付処理のポイントについて話します。 Pythonの日付処理、タイムゾーン変換について学ぶ 本セッションではPythonの日付処理でよく使用される以下のモジュールを使用して、実装してみた日付処理について解説していきます。- 標準モジュール- python-dateutil- pytx,False
2017,https://pycon.jp/2017/ja/proposals/vote/156/,NumPy道場 (ja),Pythonのサイエンススタックの全てのビルディングブロックになっているNumPy．そのNumPyの使いこなしについて，基本とテクニックをお教えいたします．NumPyアレイの基本から，ベクトル化，ネットワーク分散化，さらにはGPUによる高速化まで，コードの実例を提示しながらポイントポイントをわかりやすく解説します． 参加者はNumPyの基本と技術を学び，Pythonサイエンススタックを使いこなす基礎を得ることができます． 深層学習を含んだ機械学習，そしてデータ解析・統計解析を行うデータサイエンス，更には物理学や気象学，分子生物学などの科学研究の分野で，科学計算におけるPythonライブラリの発展は留まるところを知りません．そのほとんどの科学計算Pythonライブラリの礎となっているライブラリが[NumPy][1]です．NumPyを使いこなせないとPythonサイエンススタックを使いこなせないと言っても過言ではありません．NumPyを使うにはNumPyアレイが使いこなせないといけません．NumPyアレイは，スライス・リシェイプ・ブロードキャスティングなど，いくつかのポイントとなるテクニックの取得が必要です．また，計算速度とメモリ利用効率のパフォーマンスのためには，計算コード全体をベクトル化することが必要です．更には，計算コードをベクトル化する前には，解こうとする問題をベクトル化する必要があります．以上のように，NumPyアレイの性質及び，コードと問題のベクトル化が理解できていないと，NumPyを使うことは難しいことになります．本トークでは，NumPyアレイとベクトル化について，わかりやすい実例をPythonコードで示しかつ実行しながら，解説いたします．さらには高速化のテクニック，NumPyアレイのネットワーク分散化，GPUによる並列化などの発展的話題についても解説します．  [1]: http://www.numpy.org/,False
2017,https://pycon.jp/2017/ja/proposals/vote/55/,Building a Customized Personal Assistant with Python (en),"What if we could combine bots, machine learning, artificial intelligence, and open source projects for building a personalized assistant in a simple way with Python? In this talk, we will discuss the infinite possibilities of personalized assistant features which could be developed for assisting daily lives. The attendees should be able to build their own personalized assistant for assisting daily lives. In addition, the attendees will learn new insights of infinite possibilities in bots & artificial intelligence technology. For a lot of people, personal assistant often sounds overkill and it often does not satisfy our daily necessities. But, what if, we can develop a customized personal assistant to make our life much easier? As a first-timer expatriate in non-English speaking country, I realized a simple personal assistant has a lot of merits. From learning new foreign language, receiving train status notification, and generating personal entertainment system, we can actually build them easily in Python. This talk will introduce how we can build them with available tools out there and how we can customize personal assistant based on our own daily activities.Project link on Github: [freedomofkeima/messenger-maid-chan](https://github.com/freedomofkeima/messenger-maid-chan)  Medium blog, featured in Chatbot's Life: [Building Personalized Maid Assistant for Dummies](https://chatbotslife.com/building-personalized-maid-assistant-for-dummies-23ce7196fa35)",False
2017,https://pycon.jp/2017/ja/proposals/vote/61/,Pythonistaで始めるiOSプロトタイプ開発 (ja),Pythonは様々なところで使えるプログラミング言語ですが実はiOS上でも使えます。それがPythonistaです。PythonistaはiOS上で統合開発環境として単体で開発をおこなうことができ、さらにiOSの機能も呼び出すことができます。このトークではこのiOSの機能を呼び出すことにフォーカスし、どこまでプロトタイピングとして遊びつくせるのかを現時点までの実績をベースに発表します。 PythonはWeb開発やデータ分析、機械学習だけではない、もっと色んな使い方をしていいんだという新たな視点 Pythonista本体と同梱モジュールの簡単な紹介から、iOSの機能をラップしたPythonista独自のモジュール、さらにモジュールで実装していない機能をランタイムでiOSから呼び出すobjc_utilの使い方までを説明する。そのあとそれらの機能を使ってどんなプロトタイピングができるのかサンプルをいくつか紹介する。,False
2017,https://pycon.jp/2017/ja/proposals/vote/128/,PythonとAWSLambdaによるアスキーアート自動生成ウェブアプリ作成 (ja),PythonとAWS(EC2/Lambda/S3/ServerLessFramework)を利用してウェブアプリを作る事例紹介を行います。今回は趣味で作ってみたアスキーアート自動生成ウェブアプリを題材とします。当日はその他に機械学習や深層学習を利用したウェブアプリ作成の実践的な紹介ができればと思います。 AWS Lambdaを利用したウェブアプリの作り方の概要をつかんでもらう。AWSLambdaで深層学習を利用方法を学ぶ。Python(Django)、JS(Angular)の連携方法について学ぶ。動的計画法を利用したAsciiArt自動生成法のについて学ぶ。 PythonとAWS(EC2/Lambda/S3/ServerLessFramework等)を利用してウェブアプリを作る事例紹介を行います。今回は趣味で作ってみたアスキーアート自動生成ウェブアプリを題材とします。当日はその他に機械学習や深層学習を利用したウェブアプリ作成の実践的な紹介ができればと思います。,False
2017,https://pycon.jp/2017/ja/proposals/vote/62/,Datadogを用いたPythonアプリケーション監視 (ja),これまでモノタロウを支えるサービスの監視には、インフラ視点により、nagios/cactiを使って行われてきました。最近はインフラ視点の監視の上に、アプリケーション開発者視点の監視も加えて行っています。アプリケーション監視には、Pythonとの親和性の高い監視プラットフォームDatadogを用いています。このセッションでは、モノタロウにおけるDatadogを用いたPythonアプリケーション監視についてご紹介します。 異なる視点の監視を行う理由、Datadogの導入方法、ワークメトリクスとリソースメトリクス、Datadog APMを用いたパフォーマンス監視 これまでモノタロウを支えるサービスの監視には、インフラ視点により、nagios/cactiを使って行われてきました。最近はインフラ視点の監視の上に、アプリケーション開発者視点の監視も加えて行っています。アプリケーション監視には、Pythonとの親和性の高い監視プラットフォームDatadogを用いています。このセッションでは、モノタロウにおけるDatadogを用いたPythonアプリケーション監視についてご紹介します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/53/,メタクラスでクラス定義を操ろう (ja),Pythonのクラスはオブジェクトとして参照できるtypeクラス(メタクラス)のインスタンスとなっています。typeを拡張した別のメタクラスを作成して特殊なクラス定義をしてみましょう。 クラスをオブジェクトとして扱うことやクラスの実行時定義、特殊なクラス定義の方法を知る。メタクラスを利用したライブラリがどのように作成されているか知る。 pythonのクラスは変数に代入できるオブジェクトです。オブジェクトなのでもちろん型を持っています。クラスはtypeクラスのインスタンスとなっていて、実際にclass構文なしでクラスを生成できてしまいます。この発表ではtypeを利用したclass構文をハックや実際に使われているメタクラスの活用法を紹介します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/8/,Pythonで始めるデータマネジメント (ja),今回紹介するツール「Hecatoncheir: The Data Stewardship Studio」はデータマネジメントのためのOSSのツールです。このツールを使うことでデータ管理業務のタスクを容易に実施することができるようになります。この発表では、なぜこのツールが必要だったのか、どのように実現したのか、どのように使っているかについて紹介します。また、独学しながら（ほぼ初めて）Pythonで開発する中で、うまくいったところ、ハマったところ、乗り越えてきたところなど、自分なりに学んだところや感じたところを共有したいと思います。 データ分析業務で必要とされるデータマネジメントのタスクを理解できる。ツールを使ってデータマネジメントを実施することができるようになる。 Pythonがデータ分析と相性がいいのは皆さんご存知の通りですが、実際のデータ分析の現場では、データ分析以外にもデータの調査、チェック、共有など、さまざまなデータマネジメント（管理）業務が発生します。この業務は、大量かつ多様なデータを扱わなければならないビッグデータの環境では非常に大きな負荷になります。また、データの活用のためには、データそのものだけではなく、データや分析に関連するナレッジの共有も必要になってきます。これらの手間のかかるデータ管理のタスクを省力化・自動化しつつ、分析に必要なナレッジを共有するためのツールをPythonで開発しました。本ツールはOSSとして公開されています（5月公開）。* [Hecatoncheir: The Data Stewardship Studio](https://github.com/snaga/Hecatoncheir)* [ Hecatoncheir: The Data Stewardship Studio 0.8を公開しました](http://pgsqldeepdive.blogspot.jp/2017/05/hecatoncheir-data-stewardship-studio.html)本セッションでは、なぜDBエンジニアがデータマネジメントツールを開発する必要があったのか、どのようなことができるのか、実際にどのように使われているのか、などについてご紹介します。また、（ほぼ初めて）Pythonで開発するようになって1年ほど経ちました。今振り返って、うまくいったところ、ハマったところ、乗り越えてきたところなど、自分なりに学んだところや感じたところを共有したいと思います。,False
2017,https://pycon.jp/2017/ja/proposals/vote/65/,Pythonと機械学習によるWebセキュリティの自動化 (ja),Webアプリの脆弱性を見つけ出すWebアプリ診断。この業務は、セキュリティエンジニアのスキルと経験に大きく依存するため、長年、人手不足が叫ばれています。また、その業務の特殊さ故に、自動化は難しいとも言われてきました。本セッションでは、このWebアプリ診断業務を「Python+機械学習」を駆使して自動化を試みた際のテクニックやノウハウなどをお話いたします。 Pythonと機械学習を使用したセキュリティ自動化のテクニックや注意点を知ることができる。 日本ではWebアプリケーションの脆弱性を見つけ出す業務（Webアプリ診断）に係る人材が不足しています。私は人材不足を解消する一つの方法として、「**Python**」と「**機械学習**」に着目し、Webアプリ診断の「**自動化**」を進めています。本セッションでは、Webアプリ診断にとって重要となる、以下の二つのタスクを自動化するテクニックと注意点をデモンストレーションを交えながらお話します。 1. **Webアプリのクローリング**Webアプリに潜む脆弱性を網羅的に見つけるために、何十～何千のページを人間がクローリングする必要があります。クローリングでは、入力フォームに適切な値を入力したり、ログインが必要なWebアプリの場合はログインアカウントを作成した後にログインするなどの複雑な操作が必要です。[https://www.youtube.com/watch?v=aXw3vgXbl1U][1] 1. **脆弱性の検出**クローリングができたら次は脆弱性を見つける必要があります。ここでは、従来の脆弱性スキャナのように多くの検査シグネチャを投げまくるのではなく、Webアプリの挙動を観察した上で、より効率よく（熟練したセキュリティエンジニアのように）脆弱性を見つけ出します。[https://www.youtube.com/watch?v=N5d9oM0NcM0][2]  [1]: https://www.youtube.com/watch?v=aXw3vgXbl1U  [2]: https://www.youtube.com/watch?v=N5d9oM0NcM0,False
2017,https://pycon.jp/2017/ja/proposals/vote/64/,Correct 1000 exams with a bit of OpenCV (en),"It may look like a difficult task, but auto-correct a lot of multiple choice tests  with Python can be very easy! With that objective in mind, we will learn how images are stored in a computer, how to work with arrays in Numpy, how to plot graphics easily with  MatPlotLib an a couple of useful functions of computer vision from OpenCV. - Learn how an RGB image is a combination of 3 matrices and how easy is to understand and work with a gray scale image in a single matrix.- Learn the basics of matrix and array creation and manipulation with Numpy.- Learn the basic interface of Matplotlib (pyplot) to make and save basic graphs.- Learn how to blur, detect contours and apply thresholds to images with OpenCV. The objective of the talk is to demonstrate the use of some iconic libraries, tied through the plot thread of achieving easy test analysis. **Numpy** and **MatPlotLib** are two of the most important libraries in scientific Python, used for working with arrays and graphic representations respectively. As a gray scale image in the end is nothing but a matrix , we can use basic image manipulation to show how to work with them with **Numpy**, like using slicing and numpy functions.The easiest interface for **Matplotlib**, called **Pyplot**, is very useful to display a lot of graphs and plots with very little work. We can show an easy example of this by plotting detected contours over the image.We can also show some of the most basics functions of **OpenCV**,  one of the most important softwares for computer vision, which has a comfortable Python binding. We will use the blur, threshold and detect contours functions.If there were enough time, we could also take a look to the **openpyxl** library, which we can use to export or import data through excel sheets.It may be important to explicitly note that, due to the limited amount of time available, **no library can be deeply explored**, and little or nothing will be live-coding demo. However, all the code necessary for generating all intermediate images will be provided through a Jupyter notebook via **GitHub**.",False
2017,https://pycon.jp/2017/ja/proposals/vote/103/,人工知能時代の音楽制作への招待 - Google Magentaとセッションをしよう - (ja),近年ディープラーニングの手法は様々な分野で大きなインパクトをもたらしていますが、この技術を音楽制作をはじめとしたアートに活用しよう、というプロジェクトがGoogle Magentaになります。 本セッションでは、このGoogle Magentaについてその仕組みの基礎的な所から解説を行い、実際に音楽を学習させ、生成させて、セッションを行うところまでの手順を紹介します。 ・機械学習を音声に適用する際の、基本的な手法が理解できます・理解した内容を元に、実際のPythonアプリケーションを動かしてみることができます * 音楽に対し機械学習を適用する手法  * 音声を直接扱うアプローチ  *  音符を扱うアプローチ  * Google Magentaのアプローチ* Google Magentaの仕組み  * Magentaにとっての「音」と「音楽生成」  * Neural Network: シンプルなモデルによる、「音」の予測  * RNN: 「メロディー」をとらえた生成への挑戦  * LSTM: より長いメロディーの理解への挑戦  * Lookback: 「リピート」への着目 * Attention: 「キーポイント」をとらえたメロディーの生成へ*  実際に試すための手順,False
2017,https://pycon.jp/2017/ja/proposals/vote/70/,Data Driven Decisions Using PyPI Statistics (en),"Open source library developers have an analytics problem. What platform do my users run on? How many users do I have? Is it okay to drop support for something? How do you answer questions like this when people are embedding your software as a component of their own product? Fortunately, PyPI has a little-known service that you can leverage to answer questions like these. Learn how to harness it! Attendees will learn to use PyPI's statistics database to look up information relevant to their library and make informed decisions. Open source library developers have an analytics problem. What platform do my users run on? How many users do I have? Is it okay to drop support for something? How do you answer questions like this when people are embedding your software as a component of their own product? Fortunately, PyPI has a little-known service that you can leverage to answer questions like these. We'll walk through setting up your computer to access the BigQuery statistics, check out a variety of queries, and discuss some limitations.",False
2017,https://pycon.jp/2017/ja/proposals/vote/67/,カノジョのつくり方に学ぶSlack Bot開発 (ja),カノジョのつくり方、すなわち、人と対話をするBotの開発方法をご紹介します。対話Botを作成するには多くの壁や工夫ポイントがあります。今回はデータが少ないことが１つの壁でした。オリジナルのBotを作成するには、どのような手順が必要か、Deep Learningと自然言語処理を組み合わせてどうするか、カノジョ特有の工夫点をSlack Bot作成を通じて、ご紹介します。 特にサンプルコードを動かしたけど次に何をしたいのかわからない人を対象としております。例えば、ChainerでMNISTは動かしたけど、次は？、機械学習のスクリプトをかいてみたけど、他にどのようなことができるのでしょうか？、また、実際にどう使えば面白いことができますか？と考えている人です。その他、以下のことが本発表から得られます。①自然言語処理やDeep Learningを使ったBot構築の方法②Botを作成する上での工夫ポイントや挑戦したことの共有③Slack Botの作成方法④自然言語処理を用いた対話Bot作成の方法 # カノジョのつくり方に学ぶSlack Bot開発##本発表の概要カノジョの作り方、その手順並びに工夫したポイントをご紹介します。## 本発表の詳細カノジョとは何かを定義した上で、カノジョ作成の意義を紹介します。その上で、Slackを組み合わせる時に全体のシステム構成や処理の流れといった概要を説明し、最後にSlack Bot作成の具体的な方法をご紹介します。### カノジョとはカノジョとは何か、なぜこのシステムを作ったかを語ります。具体的には理想のカノジョとは何かを定義し、どのような要素が重要か。また、カノジョがいなくて寂しい人に対する１つのソリューションとしての意義を説明します。### カノジョ作成の概要本カノジョ作成の概要です。システム全体の構成図やBot開発では、どのような処理をしているかを簡潔に説明します。### カノジョ作成の詳細####データ取得データ取得では、本を利用しました。  本を用いてどのように学習データを構築したかの紹介を行います。####データ加工データ加工部では、データ取得で得たデータを加工します。  基本的な形態素解析を用いた前処理の他に、カノジョBotならではの  工夫ポイントを実施しています。  例えば、カノジョが設定した名前を呼んでくれるような対応を行っています。  また、本カノジョBotでは、データ取得方法の都合上、十分なデータを得ることができませんでした。  そのような工夫・対応方法をご紹介します。####Neural Conversation Modelの構築「A Neural Conversational Model」（https://arxiv.org/pdf/1506.05869.pdf）を参考に、Deep Learningフレームワーク Chainerで実装します。本項目中で、そのモデルのご紹介をします。####Neural Conversation Modelの学習・保存Chainerを用いたNeural Conversational Modelの学習コードと学習モデル保存方法のご紹介です。####学習済みNeural Conversation ModelをSlack Bot組み込みこれまでの処理で得られたモデルをSlack Botに組み込みます。組み込み方と同時にSlack Botの作り方をご紹介します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/68/,Why you should do text analysis with Python (even if you don't want to) (en),"The explosion in Artificial Intelligence and Machine Learning is unprecedented now - and text analysis is likely the most easily accessible and understandable part of this. And with python, it is crazy easy to do this - python has been used as a parsing langauge forever, and with the rich set of text analysis tools, it works more than just well. Attendees will be able to now use the rich python NLP environment to parse and understand their textual data. python is an excellent scripting language and attendees will get to know the exact tricks used to clean and analyse textual data using python. The explosion in Artificial Intelligence and Machine Learning is unprecedented now - and text analysis is likely the most easily accessible and understandable part of this. And with python, it is crazy easy to do this - python has been used as a parsing langauge forever, and with the rich set of Natural Language Processing and Computational Linguistic tools, it's worth doing text analysis even if you don't want to.The purpose of this talk is to convince the python community to do text analysis - and explain both the hows and the whys. Python has traditionally been a very good parsing language, aruguably replacing perl for all text file handling tasks. Reading files, regular expressions, writring to files, crawling on the web for textual data have all been standard ways to use python - and now with the Machine Learning and AI explosion - we have a great set of tools in python to understand all the textual data we can so easily play with. I will be briefly talking aboubt the merits, de-merits and use-cases of the most popular text processing libraries. In particular, these will be spaCy, NLTK, gensim. I will also talk about how to use traditional Machine Learning libraries for text analysis, such as scikit-learn, Keras and TensorFlow.Pre-processing is the one of the most important steps of Text Analysis, and I will talk more about this - after all, garbage in, garbage out!The final part of the talk will be about where to get your data - and how to create your own textual data as well. You could analyse anything, from your own emails and whatsapp conversations to freely available British Parliament transcripts!",False
2017,https://pycon.jp/2017/ja/proposals/vote/72/,pythonで作るニュースレコメンドエンジン (ja),"毎日500万件以上のログデータを処理し、リアルタイムに記事を推薦するエンジンを作りました。ユーザーの行動分析, A/Bテストの分析などのフローも交えて紹介します。 データ処理方法や開発に関する注意点と苦労話と解決策を共有すること ニュース記事のレコメンドAPIをdjangoで実装しました。ユーザーの行動分析、レコメンドエンジンの開発、A/Bテストの分析までのフローと合わせてデータ処理時の苦労話や注意点を紹介します。主に以下のトピックについて話します.### アクセスログ収集- データの前処理アクセスログを活発に利用する環境が整っておらず、膨大なログをどのように扱うのかというノウハウがまったくない状態から地道にパーサーを作っていきました。分析フェーズでは、新聞ならではのユーザー行動に悩まされました。- 集計バッチ1日600万件以上のログを集計するための工夫について話します### レコメンドエンジン-  精度と計算量常に最新のニュースを推薦するために、事前バッチで記事を計算せず、リアルタイムレコメンドにこだわりました。-  負荷通勤時間などニュースサイトならではの集中的なアクセスに対してどのように対処したのかを話します。",False
2017,https://pycon.jp/2017/ja/proposals/vote/71/,Start Python Club - みんなの草の根コミュニティ (ja),"Start Python Club（Stapy）はPythonの初心者からマスターまで幅広いレベルの人が集まる草の根コミュニティです。2015年5月の設立以降、毎月勉強会を開催し、ウェブサイト上のメンバーは2,300名を超えました。オープンソースの開発を支えるには人と人のつながりが大切です。Stapyの活動を通じて得た経験から、Pythonコミュニティの発展のために、何ができるかを考えます。 Start Python Clubは初心者からマスターまでいろいろな人が集まり、様々な技術について気軽に語り合い、交流する場を提供しています。コミュニティに参加することで、新しい技術を学ぶだけでなく、悩みや志を共有する仲間を見つけることができます。人と人のつながりは、スキルレベルの向上やモチベーションの維持にとって良いサプリメントになります。この講演ではこれまでの活動を紹介した上で、日本のPython界の発展に必要な、草の根コミュニティの意義や価値をみなさんと共有します。 ### コミュニティ設立の経緯私はデータサイエンスとウェブ技術に関心を持ち、Pythonを学び始めました。教科書やオンラインコースを使い、独学で勉強を続けましたが、どのように実用に生かしたらよいかわからず、途方に暮れました。このとき自分のようにPythonのプログラミングを学びたいけれども、周りにメンターや仲間がいないで悩んでいる人は多いだろうと思いました。そこでPythonを気軽に学ぶ場としてStart Python Club（Stapy）を立ち上げました。### Stapyの活動Stapyは2015年5月の設立以降、Pythonに関するいろいろなトピックスを取り上げ、毎月、講演形式の勉強会を開催しています。勉強会の後には懇親会を開催し、参加者同士で交流しています。設立から2年経ち、Connpassのグループメンバーは2,300名を超えました。勉強会以外に、長野などの地方巡業、理研AIPとの合同研究会など、活動の範囲を広げています。### 草の根コミュニティの意義オープンソースソフトウェア（OSS）であるPythonの開発と普及を促進するには、その利益を享受するエコシステムが欠かせません。Stapyは、初心者からマスターまでいろいろな人が集まり、様々な技術やその応用について語り合い、交流する場を提供しています。講演を聞いて技術を学ぶだけではなく、プログラミングに関する悩みを相談したり、一緒に開発する仲間を探したりする、人と人のつながりを生み出しています。### Start Python Club サイト[http://startpython.connpass.com/](http://startpython.connpass.com/)",False
2017,https://pycon.jp/2017/ja/proposals/vote/41/,luigiによる機械学習データフロー (ja),"機械学習においては、実験等の再現性の重要であるためデータフローを明確に記述することが重要です。ここでデータフローとはデータの取得・整形から前処理、モデルの学習・ハイパパラメータ調整・評価、そしてデプロイまでの一連のフローをさします。発表では機械学習における一般的なデータフローについて導入したのち、実際のluigiによる記述方法について説明します。またluigiの利点や簡潔に書くためのテクニックについても述べます. 参加者が 機械学習のデータフロー(取得〜モデルの学習、評価〜デプロイ)までluigiを利用して記述できるようになること。 Pythonではscikit-learnやChainer, Theano, Tensorflowといった強力な機械学習ライブラリが充実しており、アカデミックの分野だけでなく, ビジネスの様々な分野においても活用が進んでいます。機械学習プロジェクトを進める上で最も重要なことは、再現性を保つことです。機械学習プロジェクトにおいて再現性を担保するには、データの取得・整形方法から前処理、モデルの理論・学習方法・ハイパパラメータ調整・評価までの一連のデータフローを明確に示すことが必要です。本発表では、このデータフローの構築の仕方について焦点をあて、Pythonのワークフローマネージャーであるluigiを利用し、機械学習データフローの構築方法について説明していきます。機械学習における一般的なデータフローについて導入したのち、実際のluigiによる記述方法について説明します。 伝統的なMakefileやシェルスクリプト、Apache Airflowなどと比較したときの 違いについて言及しつつ, luigiの紹介を行います。簡単な実例として 自然言語処理を題材として luigiでデータフローを記述しますこのとき、luigiの利点や簡潔に書くためのテクニックについても述べます。またPythonにおいてはpandasやjupyter notebook といった、他の強力なライブラリと連携することで、より簡潔・強力に集計作業やレポーティングを行うことができます。これらの連携方法についても発表で述べます。luigiを利用し、機械学習開発を進めることで前処理方式の比較、モデルやそのハイパーパラメータの比較、実際のデプロイ時のパフォーマンスチェックまで簡単に行えるようになります",False
2017,https://pycon.jp/2017/ja/proposals/vote/105/,neural networkを用いた多クラス分類への取り組み (ja),実際に、画像の多クラス分類を行った際の、neural networkのモデル構築までのプロセスについて。 機械学習を取り入れる際の現状の問題点を把握するための材料を提供する 我々が、取り組んだneural networkを用いた画像の多クラス分類問題において+ データ構造と認識への着想や仮定+ preprocessで発生した問題点と解決案+ モデル構造の選択+ 学習へのアプローチ+ 適切な損失関数の選択について、我々の考察とともに解説していく。,False
2017,https://pycon.jp/2017/ja/proposals/vote/84/,"Raspberry Pi Using Open CV which has The Installing ,making Programs And Performance (en)","HI, guys,  There are on ot the modules for being able to recognize shapes and faces in  camera and pictures or movies. It is the openCV  that it make them, and Tensorflow. So, I explain to all of you how to install and how to use  openCV from scratch and the demo for the performance of the devices in using Raspberry Pi for openCV,  and Tensorflow if I have a time. Thank-you.   For the recognizing the modules about open CV,  how to use and install from scratch  in some devices especially Raspberry pi. Using Open CV which Has the Installing , using and performance, you are able to recognize faces and so on in camera or pictures",False
2017,https://pycon.jp/2017/ja/proposals/vote/76/,Pythonで作る機械学習を使ったアプリケーションの開発と管理 (ja),"機械学習を使ったアプリケーションは、機械学習モデルを再現できなくなったり、バージョン管理ができていないと、技術的負債になる可能性があります。こうした課題に対して、本発表では、BrandSafe はてなを事例として、教師あり機械学習を利用したアプリケーションの開発時に行った取り組みについて紹介します。 教師あり機械学習を利用したアプリケーション開発・管理のケーススタディ 機械学習を使ったアプリケーションは、コードを読むだけでは挙動がわからないため、データやモデルの管理が不十分だと、開発した当人しかモデルを再現できず、いずれ技術的負債となります。機械学習システムを技術的負債にしないためには、過去行った取り組みをきちんと再現できるようにしておく必要があります。はてなでは、これまではてなブックマークの記事カテゴリ判定などの様々な場面で機械学習を活用した機能を開発してきましたが、中には、モデルの再現や改善ができずに技術的負債となってしまった機能も存在します。こうした課題があるなか、教師あり機械学習を使ったアプリケーションであり、Perlで書かれていたBrandSafe はてなを、scikit-learn, flaskなどを使いpythonでリプレースしました。BrandSafe はてなを事例として、機械学習システムを技術的負債にしないために、機械学習を使ったアプリケーション開発時に考慮した機械学習の再現性や、データやモデルのバージョン管理についての取り組みを紹介します。",False
2017,https://pycon.jp/2017/ja/proposals/vote/77/,Pythonで解く大学入試数学 (ja),近年Pythonコミュニティでも、データ分析や機械学習などの分野が注目を浴びています。ただこれらPyDataの領域は数学の知識が必要不可欠となっています。このTalkでは、sympy・numpy・scipyなどPythonを通して数学にふれる楽しさを紹介します。数学のレベルとしては、高校数学や大学入試問題を題材として基礎的なものを扱います。 sympy・numpy・scipyなどPythonを通して数学にふれる楽しさを感じてもらえればと思います。 数学系ライブラリの紹介・説明-  sympy- numpy- scipyライブラリの使い方とあわせて数学についての説明- 微積分- 行列- 線形代数- 確率・統計数学の問題を実際に解いてみる- センター試験- 大学入試問題,False
2017,https://pycon.jp/2017/ja/proposals/vote/66/,DjangoでできるリアルタイムWeb  (ja),Djangoがリリースされた2005年では、Webはリクエストとレスポンスの関係性のなかで完結していました。しかし、今やWebにリアルタイム性が求められることが当たり前の時代になりました。 このTalkでは、昨年秋にDjangoの公式プロジェクトに採用されたライブラリChannelsの解説を中心に、DjangoでWebSocketを用いたリアルタイムWebをどうやって実装するかを実例を交えて説明します。   この発表によって参加者が、DjangoでWebScoketを用いたリアルタイムWeb実装方法、ライブラリの活用方法を知り、実際の開発に役立ててもらえればと思います。 Channelsの以下のような特徴を、実際に簡単なアプリケーション作成を行いながら解説していきます。+ Djangoのviewに似た形式でのWebSocketハンドリング+ Tornadoなど他のwebサーバーを立てる必要がなく、Djangoのみで実現可能+ 低レイヤの非同期処理を開発者が実装しなくていいお手軽さ ,False
2017,https://pycon.jp/2017/ja/proposals/vote/129/,Polyphony: Python ではじめる FPGA と CNN への応用 (ja),Polyphony は Python で書かれたソースコードをそのまま Verilog HDL にコンパイルすることの出来る高位合成コンパイラです。Polyphony を使えば FPGA による並列処理をより身近に使うことが出来ます。難しいハードウェア用言語を覚える必要がありません。応用事例として RISC-V の実装、CNNへの応用 にも言及します。 FPGA に興味はあるし使ってみたことはないけど、Python なら書けるというソフトウェア技術者が、Polyphony を使うことでハードウェアの世界に足を踏み入れ、並列処理が書けるかもしれないし、やってみようと思うきっかけになれば最高です。 Polyphony は Python で書かれたソースコードをそのまま Verilog HDL にコンパイルすることの出来る高位合成コンパイラです。Polyphony を使えば FPGA による並列処理をより身近に使うことが出来ます。難しいハードウェア用言語を覚える必要がありません。もちろん、すべての Python で書かれたプログラムを FPGA 化できるわけではありません。例えば、ストリングを扱うことは FPGA にとって得意分野でもありませんし、現在の Polyphony には難しいことです。それでも、ちょっとしたコツをつかめば Python で書かれたプログラムを Polyphony によって  FPGA 上で動作させるようにすることはそう難しいことではありません。また、いくつかの Polyphony 特有の機能やライブラリを使えば FPGA が得意とする並列処理をうまく表現して使うことが可能です。実際の応用事例として RISC-V の実装、CNNへの応用 にも言及します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/93/,Docker と Python でつくるサンドボックス (ja),Docker を使うと，ユーザーが記述したコードを安全に実行するサンドボックスを簡単に作ることが出来ます．このトークでは，Docker を Python から操作してサンドボックスを作る方法を紹介した後，実際に作成したいくつかのサンドボックス環境について紹介します． Docker についての理解を深める・Docker を Python から利用する方法について理解する ## はじめに今回のトークで実現するサンドボックスについて簡単に紹介します．## Docker について- Docker とその仕組みについて簡単に紹介します- Docker が提供している API について紹介します## サンドボックスを作る- サンドボックスをつくるための Docker 環境の準備とポイントについて説明します- Docker を Python から操作するためのライブラリである docker-py について紹介します- docker-py を使ってサンドボックスを作成する方法について紹介します## サンドボックスの実例スピーカーが作成したサンドボックスについて紹介します- ユーザーが書いた Java のソースコードを安全にコンパイルするサンドボックス- [mypy を使った静的型チェックを行える Web サービス (mypy Playground)](https://play-mypy.ymyzk.com) のためのサンドボックス,False
2017,https://pycon.jp/2017/ja/proposals/vote/104/,Python3.0から3.6までの新機能全集 (ja),今日のPython界隈では当然の如くPython3を前提として話がされる。しかし、未だ3系に移行したものの違いがあまりわかっていないという人や、興味はあるが事情があり移行できず3系の流れについて行けないことを不安に思っている人も多いと思う。そんな方達のために現在リリースされているPython3系に追加された全ての機能について紹介する。 - Python3で追加された機能を知る 3年前のPyConJPにてPython2系と3系でどちらを使っているか、というアンケートが行われた時、2系が圧倒的多数だった。しかし、数年で3系への移行は一気に進み、現在Python界隈では当然の如く3系が前提とされている。私は長らくPythonを愛していたが、今年の春に転職するまで仕事の都合で2系から離れられずにいた。そんな私もようやく2.7から3.6に移行できる時が来たのだが、Pythonの最先端から離れていた数年の間に3系はどんなふうに進化を遂げたのか、とても気になりそれをまとめることにした。今回のトークでは、私のように最近3系に移行した人・移行したい気持ちはあるができずにいる人に向けて、「3系がどんな進化を遂げたのか」その全貌を俯瞰するために、簡単にではあるが3系に新たに追加された機能について全て紹介したいと思う。,False
2017,https://pycon.jp/2017/ja/proposals/vote/58/,サービス多言語対応を助ける翻訳ツールをつくる (ja),webサービスはその特性ゆえ様々な方がユーザーとなりえます。その中で、日本語以外のユーザーに対応したい機会もあるかと思います。そういったときに必要に迫られるのが、サービスの多言語対応です。そんな多言語対応の手助けとなる翻訳ツールの実装方法や運用方法の話をしたいと思います。 Python、pyramidを使った翻訳ツールの実装方法。PO・MOファイルの活用方法。このセッションでは、多言語対応に必要になる文言の翻訳を自動化して、よりスムーズにサービスを多言語化するためのノウハウを共有いたします。また、翻訳機能にかかわらずアプリケーションの基本的な機能であるCRUDの実装方法も学べるかと思います。 1. 翻訳ツールの作成・pyramidを用いたログイン機能からSQLAlchemyによるSQL操作など、基本的な機能の実装方法の説明・フレームワークpyramidを使ってみて分かった特徴とメリット、デメリット。2. PO・MOファイル作成し、利用する・PO・MOファイル自体の使い方などのおさらい・作成した翻訳ツールからPO・MOファイルを出力し、多言語化に利用する。・連携するともっと便利になるサービス、crowdinのご紹介環境Python3 gunicorn postgresql SQLAlchemy Pyramid chameleon,False
2017,https://pycon.jp/2017/ja/proposals/vote/107/,Primer on using WebSockets in Django (en),"I give a basic introduction to WebSockets and share some of my experiences from using Django-Channels for pushing data from external sources to the user in near real-time on a production scale.Rather than following the often-discussed example of a chatroom, I show how WebSockets can be used for handling backend events that origin at the models, querysets or even management commands level. Attendees should be able to implement WebSockets using Django-Channels in their new and existing projects even if they had no previous experience with the technology.Their understanding of the applicable use-cases should be significantly wider than if they only followed Django-Channel's documentation or tutorial The talk will start with a discussion of the standard request-response cycle in web application and show its current limitations in order to present how the WebSockets specification attempts to solve the problems.First, a brief history and current state of the WS spec is presented. Second, the choice of Django-Channels as a library to implement WS in a Django projects is defended.Third,  I show how the newly introduced concepts of Consumers, Channels, and Messages correspond to Django's Views, Request, and Responses.Finally, I walk the audience through a simple but working example putting all the presented concepts into practice.Finally, I want to point the audience to some common problems that we have encountered while running commercial, production-scale systems and share our solutions to them.",False
2017,https://pycon.jp/2017/ja/proposals/vote/130/,Virtual environments and dependency management in Python (en),"During my talk I'd like to present what Python has to offer when it comes to virtual environments and how to manage multiple Python environments with pyenv and virtualenv. In the second part I will focus on ways of managing dependencies and why it's worth to think about it. How to manage virtual environments, Python environments and why it's worth to think about managing dependencies.  TBD",False
2017,https://pycon.jp/2017/ja/proposals/vote/79/,プロダクト開発して分かったDjangoの深～いパーミッション管理の話 (ja),DjangoのView内でユーザーの権限を調べるif分岐がグチャグチャ、DjangoのPermissionがイマイチ使えない、チームやロール、契約プランが絡んだパーミッションが難しい。。。そんな人はこのトークを聞いてください。このトークではPyQ https://pyq.jp/ という製品開発を通して学んだパーミッション、権限管理、オブジェクト認可の方法を話します。ユーザーの権限や契約中のプラン、チームやロールによってパーミッションが変わるようなDjangoアプリケーションをどう作るかを、深〜く説明いたします。オブジェクトの単純な認可でなく、Viewにアクセスできるかの判定や、テンプレートの一部を表示非表示するような、アプリケーション開発に必要なパーミッションの知識を網羅します。 Djangoで画面やオブジェクトのパーミッション管理、設計方法を知れます。 DjangoのView内でユーザーの権限を調べるif分岐がグチャグチャ、DjangoのPermissionがイマイチ使えない、チームやロール、契約プランが絡んだパーミッションが難しい。。。そんな人はこのトークを聞いてください。## プロダクト開発を通して得た知識を共有しますこのトークではPyQ https://pyq.jp/ という製品開発を通して学んだパーミッション、権限管理、オブジェクト認可の方法を話します。ユーザーの権限や契約中のプラン、チームやロールによってパーミッションが変わるようなDjangoアプリケーションをどう作るかを、深〜く説明いたします。## ただのオブジェクトのパーミッションだけではありませんオブジェクトの単純な認可でなく、画面やワークフローなどアプリケーション開発に必要なパーミッションの知識を網羅します。DjangoのPermissionを使っても、単にモデルを追加、削除、編集する権限があるかないかしか管理できません。今回紹介する方法ではモデルだけでなく画面やそれ以外の場所にも使える汎用的なパーミッション管理の方法をお伝えします。例えば View にデコレータをつけるだけで、「この権限を持つユーザーのみ許可」とできたら素敵ではないですか？毎度Viewの中に書いたり、独自のデコレータを作って対応していませんか？その面倒な作業にサヨナラする方法を紹介します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/15/,エンジニアが身につけておきたい金融リテラシー 〜年金最適化編〜 (ja),"自分はエンジニアだから金融の知識は不要だと思っていませんか？これからはエンジニアがITを活用して金融情報をHackし、資産を守る時代です。Pythonを活用することをキッカケに金融リテラシーを身につけていきましょう。第二部ではPythonを活用した年金最適化の事例を紹介します。本講演以外ではなかなか聞くことができない話を聞くチャンスです。価格比較サイトでスマホの値段を調べるより遥かに有意義な30分を提供します。 Pythonのイベントに参加している内にエンジニアとしてのスキルはあるけど、お金のことはあまり詳しくないという方が結構いることがわかりました。ITのスキルをほんの少し金融に向けるだけで金融資産や今後のキャッシュフローが大きく改善する可能性があります。Pythonと最適化パッケージを活用することで年金のシミュレーションや最適化について知見を得ていただくことができます。 # 第一部: 有利な賭けをしよう(driller)## リスクとリターンリスクとリターンの関係を数値化するには様々な方法があります。今回は最も基本的な指標であるシャープレシオについて説明します。## ポートフォリオ一般的に分散投資がよいと言われていますが本当でしょうか？単純なモデルを題材にし、Pythonを用いてシミュレーションしてみます。# 第二部:年金を最適化してみよう(yosshii)## 公的年金シミュレーション年金の種類や仕組みについて簡単に説明し、Pythonで公的年金の収支シミュレーションを行います。将来年金がどのくらいもらえるのか、どんな受給方法がいいのかなどのヒントになります。## 個人型確定拠出年金（iDeCo）の最適化個人型確定拠出年金（iDeCo）対象商品でポートフォリオ最適化（最小分散、シャープレシオ最大化など）を行います。Pythonなら、数理計画問題（線形計画問題、２次計画問題）を解くパッケージScipy,openopt,cvxoptを使って手軽にポートフォリオ最適化を実施できます。",False
2017,https://pycon.jp/2017/ja/proposals/vote/157/,PILでの画像リサイズを読む (ja),PILの画像リサイズ実装をテキストに画像のデータ構造とそのアルゴリズム実装について説明します。数式は出しません。 画像のデータ構造とアルゴリズムについて 画像処理は様々な応用分野で使用されています。たいていのプログラミング言語には画像処理のライブラリが用意されており、少ない学習時間で画像を取り扱うことができます。Pythonもご多分に漏れずPILなどのライブラリがありますが、その中身がどうなっているかご存知でしょうか?このセッションでは、普段無造作に使っているであろうPILの画像リサイズ実装をテキストに画像のデータ構造とそのアルゴリズム実装について説明します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/152/,ディープニューラルネット確率的プログラミングライブラリEdward (ja),ディープニューラルネットで計算を行う，スケーラブルな確率的プログラミングライブラリEdwardの画期的な意義と使い方を本トークで紹介します．EdwardはTensorFlowの上に確率変数とベイズ推定を実装したPythonライブラリです．計算において確率的な情報を常に保持する確率的プログラミングライブラリですので，確率事象であるこの世の現象を合理的かつうまく計算することができます． 参加者はベイズ推定の概要と確率的プログラミングの意義を知り，Edwardの性質と使い方を学びます． 深層学習の怒濤の発展が続いています．2012年の世界的な画像認識コンペILSVRCにて，当時の最先端の機械学習アルゴリズムを押さえて圧倒的な性能を披露したあと，2015年には人間の認識性能をも凌駕する性能を発揮するようになっています．また2016年，2017年には囲碁のトップ棋士との対決，さらには将棋の名人との対決で圧倒的な勝利を収めました．2012年以降，深層学習の分野に世界中の優秀な才能が殺到し，さらには巨大IT企業や政府の巨額投資が行われ，深層学習の研究，応用が爆発的な勢いで発展しています．しかし，この深層学習，どうして性能が出るのかという点について，合理的に納得できる説明ができず，今は若い大学院生・ポスドクの勘と根性でガムシャラに試行錯誤して，個々の目的となる課題（オブジェクト認識，自然言語の応答，多言語機械翻訳等）に対して性能を上げあれれば勝利という，山勘ゲームになっている様相を呈しています．この風潮に再考を促す流れの一つが，ベイズ推定の考え方を深層学習に取り入れていこうというものです．2016年12月に行われた深層学習研究のトップカンファレンスであるNIPS 2016のシンポジウム “[Bayesian Deep Learning][1]” でもその問題意識は明かになってます．ベイズ推定に基づいた確率モデルを構築することで，深層ネットワークの振る舞いをコントロールしようというのが狙いです．[Edward][2]はこのような文脈で開発が進んでいるPythonライブラリです．ベイズ推定を行う「確率的プログラミングライブラリ」というものの一種ですが，計算部分にはGoogleの深層学習フレームワーク[TensorFlow][3]を使っています．このことにより，計算機を複数使った分散計算や，複数GPUによる並列計算や，Google Cloud Platformを利用した専用プロセッサTPUによる並列計算など，大規模にスケールする計算が可能になります．このTensorFlowの上に確率変数とベイズ推定の計算アルゴリズムを導入し，できるだけ確率的な情報を保持したまま，合理的な計算を行うのがEdwardです．本トークでは，ベイズ推定と確率的プログラミングの基礎を与えながら，このEdwardの性質と使い方を，実際のPythonコードで示しながらかつ実行しながら，紹介します．  [1]: http://bayesiandeeplearning.org/  [2]: http://edwardlib.org/  [3]: https://www.tensorflow.org/,False
2017,https://pycon.jp/2017/ja/proposals/vote/74/,scouty - スタートアップ1年生の開発スタイル (ja),日本初AIヘッドハンティングサービスを提供しているscoutyは、2017年9月でサービス提供開始から1年を迎えます。この1年間どのような開発スタイルを大切にしてきたのか、なぜPythonを採用して開発をしているのか、スタートアップならではの視点からお話しします。 新規事業をつくる時、大きな失敗をしないために小さく始めてリーンに開発を進めていく必要があります。そういった事業アイデアが容易に変わりうる状況下で、どのようなことに気をつけて技術選定し、コードを書くのが良いのか、そういった知見を共有します。 日本初AIヘッドハンティングサービスを提供している [scouty](https://scouty.co.jp/) は、2016年9月にクローズドβ、2017年5月にオープンβとなり、2017年9月でサービス提供開始から1年を迎えます。scouty は今年のテーマ  “Output & Follow” にふさわしく、ネット上に公開されている Output を収集し、それを Follow(注目/追跡/理解) することで人材のデータベースを作成し、人材と企業の最適なマッチングを行うヘッドハンティングサービスです。私が創業したてのスタートアップにジョインした理由から始まり、開発環境基盤を整え、エンジニア1.5人のリソースで1年間高速にサービスを開発してきた歴史を振り返ります。会社の致命傷になるような大きな失敗をしないために何に気をつけて開発をしてきたのか、少ない人的リソースでどのように効率良く開発を進めてきたのか、具体例を交えてお話しします。,False
2017,https://pycon.jp/2017/ja/proposals/vote/80/,Errbotによって得られる、ChatOpsライフ入門 (ja),Python製のChatbotフレームワークにErrbotというものがあります。ここでは、Errbotの基本機能やプラグインを用いた社内コミュニケーションの活性化に対するアプローチ例などを紹介します。 * Errbotの基本的な仕組み(構成概念、動かし方)* Errbotプラグインによる、Chatbotの拡張の仕方（サードパーティ・自作） Python製のChatbotフレームワークとして、[Errbot](http://errbot.io)というものが存在します。昨年も作者の方がトークで発表していましたが、ここでは実際に社内・個人での導入〜運用をベースに、ErrbotによるChatOptのケーススタディなどを紹介していきたいと思います。* Errbotってこんなもの* Errbotの使い方を知る* Errbotの機能を拡張する（プラグインを使う・作る）,False
2017,https://pycon.jp/2017/ja/proposals/vote/63/,banditアルゴリズムを使った自動ABテスト (ja),データ分析の初学者にもわかりやすいようにbanditアルゴリズムを解説し、疑似コード、pythonで書いた時のコード、簡単なデモをお見せします。banditアルゴリズム：複数台のスロットマシンを相手に報酬を最大化する目的で作られた。事前情報がない選択肢を探索的に試しながら、得た情報を使って各選択肢の期待報酬を計算し、報酬見合いの確率で選択できるよう自己調整し続ける。昨今自動ABテスト用に使われる。 banditアルゴリズムの概要、pythonを使った記述の仕方、実際に業務適用する際の注意点をお伝えし、参加者が自分でbanditアルゴリズムを使って効率的なABテストができるようサポートできればと思います CV最適なUIは個々人で違う、ということに反論する人は多くないでしょう。膨大な情報を詰め込んだwebサイト/アプリの中で、実際に個人が見ることのできる情報量には限界がありますが、どのように各個人に対してコンテンツを出すのが最適でしょうか。レコメンデーションのようなモデルを作って対応するのは妥当な方法ですが、もしデータがあまり使えない場合はどうでしょう。そうしたケースに対応できるのがbanditアルゴリズムです。banditアルゴリズムは事前のデータがない状態からでも適用でき、探索的に各コンテンツの報酬(CVR等)を推定、最適な各コンテンツの表示確率(期待報酬見合い)を自動で学習します。簡単に言えば自動でABテストを連続的に繰り返し、更新のたびに報酬見合いで各コンテンツの表示確率を調整するという動きを繰り返します。また、banditアルゴリズムはABテストという文脈であれば概ね良好に機能します。例えばレコメンデーションのアルゴリズムを2つ以上比較検討したい、というケースにも使えますし、2つ以上のbanditアルゴリズムを競わせることもできます。本発表は上記のような概要に加え、疑似コード、pythonコード、デモ、実用上の注意点をお伝えし、実際に個人でbanditアルゴリズムを動かしoutputを作れるようなセッションにしたいと思っています。ただし僕自身の専門範囲ではないので、インフラの話はあまり厚く扱う予定はありません。,False
2017,https://pycon.jp/2017/ja/proposals/vote/83/,Sphinxで本を書く (ja),Real World HTTPを題材に、Python製のドキュメントツールであるSphinxを使って書籍・電子書籍・薄い本を書いてみよう、という内容です。テーマの決め方から紹介します。 テーマの選び方。Sphinxを使った本の書き方。PDF/ePub/mobiのビルド。gitlabを使ったCI。 - テーマの選び方-  Sphinxとは？- コワくないreStructuredText- PDF/ePub/mobiのビルド- gitlabを使った自動ビルド,False
2017,https://pycon.jp/2017/ja/proposals/vote/108/,"Thick QuerySets and Thin Models, or Where to Put Business Logic in Django (en)","In this talk, I want to formulate a number of arguments against following the ""Fat Models"" and rival approaches to organising Business Logic code in a Django project codebase and ultimately suggest a better solution and outline possible advantages and disadvantages of each approach. Attendees should get a clear image of all the possible problems stemming from following the ""Fat Models"" approach to organising Business Logic in a Django project and be able to evaluate how alternative approaches fit their needs. In this talk, I want to formulate a number of arguments against following the ""Fat Models"" and rival approaches to organising Business Logic code in a Django project codebase and ultimately suggest a better solution and outline possible advantages and disadvantages of each approach.The discussed approaches include:1. Organising BL code as Models' methods.2.  Dispersing BL code between a number of Views, Forms, and/or Serializers.3. Defining a completely separate Services layer.4. Organising BL code as QuerySets'/Managers' methods.An example for each approach is given by a way of a real-world example and each approach is evaluated according to the following criteria:- Code repetition (violation of the DRY principle);- code testability;- code readability.Finally, a discussion of new challenges stemming from the proposed Approach no 4 - especially concerning business rules validation and integration with external APIs - is presented.",False
2017,https://pycon.jp/2017/ja/proposals/vote/85/,Reactive Programming on AWS Lambda in Python (en),"Instead of common tutorials of Python Reactive Programming and AWS Lambda, fun, real coding practice, and benefits of ""Reactive AWS Lambda"" will be focused in this talk by discomposed actions in different aspects and tools, including serverless, functional programming, deployment, testing, debug, and activity/performance monitor. PyFunctional, RxPy, Zappa, and Apex will also be looked at. As ""Details Abstract"" section below. #### ObjectivesTo paint the blueprint and show real use cases to audience for following perspectives:- How fun and pain Reactive Programming can be- The devil detail of ""Reactive""- How fun and pain the serverless can be- The pitfalls of ""Serverless""- How it looks like running Reactive Python on AWS Lambda- How to decide when it's good to apply these paradigms- How to dig in the detail of used paradigm on debug, test, development, and deployment.",False
2017,https://pycon.jp/2017/ja/proposals/vote/87/,Geospatial data analysis and visualization in Python (en),"In this talk I will introduce you to some very useful libraries for geospatial data visualization and analysis. I will show you how to create your own maps and how I solved the problems that I ran into. I will use data from 食べログ and SafeCast. If you are interested in data mining, visualization and, of course maps, then this talk is for you. In this talk you will learn how to create great looking interactive maps for visualizing datasets with geospatial coordinates. I'll show you what Python libraries make it easy to create such maps. My aim is that you will leave the talk with a desire and knowhow to start making interactive maps using Python on your own. Interactive maps are great for exploring and getting a quick intuition of datasets containing location information. In this talk I will show that you don't have to be a data scientist or a JavaScript expert to create such maps. More concretely, I will give a quick intro to some great libraries and show you how to:- use `osmnx` to download map data and convert it into a street graph- use `geopandas`, `networkx` and `shapely` to manipulate street graphs and assign data points to areas- use `pyproj` and `geopy` for changing between coordinate reference systems and measuring distances (I'll give you a short demonstration of how important this can be)- use  `folium` for creating beautiful and responsive maps that are rendered to HTML and JavaScriptThis covers the basic part of the talk, and I will then move into the second part,  talking about some of the more difficult issues that I encountered while creating maps:- how to deal with lack of geojson/shapefile boundary data for small areas- how to deal with geospatial data that changes over timeApart from the tools mentioned above, I'll show you how `networkx`, `scikit-learn`, and good old plain Python can be used to solve these problems.英語で発表しますが、質問は日本語で受け取ります。",False
2017,https://pycon.jp/2017/ja/proposals/vote/86/,Pycoinを使ってビットコインウォレットをつくろう (ja),最近何かと話題のビットコイン。P2Pの特徴を持つからこそ、仕組みを理解し自分で管理することが大切です。PythonライブラリのPycoinを使えば簡単にビットコインネットワークに接続することができ、ウォレットやトランザクションの作成が可能になります。 ビットコインの基本的な仕組みから解説します。ビットコインとは？ブロックチェーンって何がすごいの？って方にも楽しめる内容になるかと思います。また、ビットコインネットワークを利用してPythonでアプリケーションを開発したいと考えるエンジニアならば、Pycoinを使えば開発環境の構築が容易になります。 ・ビットコイン、ブロックチェーンについて・ウォレットやトランザクションとは・ビットコインの開発環境、メインネットとテストネット・Pycoinの使い方,False
2017,https://pycon.jp/2017/ja/proposals/vote/109/,OpenAPIを利用したPythonWebアプリケーション開発 (ja),APIを記述するためのフォーマットであるOpenAPI（Swagger）を用いたPythonWebアプリケーション開発についてご紹介致します。 下記の知識を学ぶことができます。- OpenAPI（Swagger）とそのエコシステムに関する知識- OpenAPIで利用することができるツール- OpenAPIを利用したプロジェクトケーススタディ OpenAPI（Swagger）を利用したアプリケーション開発についてご紹介致します。OpenAPIとは何か？といった基礎的なことから、OpenAPIのための様々なツール群の紹介、またそれらを利用して開発している弊社プロジェクトの実例紹介を行います。,False
2017,https://pycon.jp/2017/ja/proposals/vote/131/,5年戦ってわかったマイクロサービスアーキテクチャのつらみ(と良さ) (ja),最近はやりのマイクロサービスアーキテクチャですが、利点に対してつらい部分についての情報があまりありません。この発表では5年間マイクロサービスアーキテクチャを運用して起きたトラブルなどの事例を紹介します。 マイクロサービスアーキテクチャを導入するにあたって気を付ける必要がある点。モノリシックアーキテクチャと天秤にかけるにあたって必要な知識。 弊社サービスでは2012年のリリース当初からPython + RabbitMQ + MySQLを使ってマイクロサービスアーキテクチャでサーバAPIを構成しており、現在もこのアーキテクチャのまま運用を続けています。マイクロサービスアーキテクチャは肥大化するサービスへの銀の弾丸のように語られますが、実際には実装時にも運用時にも独特の感覚が求められ、ちょっとしたトラブルでアプリケーション全体が不安定になることもあります。この発表ではサービスを運用するにあたって発生したトラブルなどをもとにマイクロサービスアーキテクチャのつらみとそれらとの付き合い方についてお話しします。,False
2017,https://pycon.jp/2017/ja/proposals/vote/89/,djangoのmigrationはどう動いているのか？ (ja),"djangoのmigrationはたいへん強力です。makemigration, migrateコマンドを実行したとき、内部でなにが起きているのか解説します。最低限これを理解していれば、なにかトラブルが置きたときの対応に困らなくなるでしょう。同時に我々のチームで行っているmigrationのクリーンアップ作業についてもデモを交えながら説明します。 djangoのmigration機能について理解を深め、種々のトラブルに対応できるようにな知識を身につける 以下の内容に関して解説します。 - djangoとはなにか - webアプリ開発においてなぜDB管理は厄介なのか - migrationはどのような機能か - makemigration, migrateを実行するとなにがおきるのか？ - migrationのクリーンアップについて - 様々なトラブルへの対応方法について - デモ",False
2017,https://pycon.jp/2017/ja/proposals/vote/82/,ドローンのフライトコントローラをPythonで制御してみた話 (ja),著しい進化を遂げたドローン（マルチコプター）が近年では流通しているが、その中核となるフライトコントローラはブラックボックスである。機体の価格も下がり今では個人で購入できるものもあり、気軽に飛ばせるようになったが、まだまだ発展途上であるガジェット。今回はPythonや航空力学などを用いてドローンのフライトコントローラとなる部分を開発し、その経緯を報告する。 Pythonを使った飛行ロボットの一部を知ることができます。Pythonを使った実例をディスプレイ上でなく、実物の機器として触れることができます。Pythonの未来を見ることができます。 著しい進化を遂げたドローン（マルチコプター）が近年では広まっているが、その中核となるフライトコントローラはブラックボックスである。ホビードローンの世界トップシェアは中国（香港）だが、日本製のドローンも世界に並んで欲しく、その技術の一つとしてPythonを使い新しい機体開発に試みたい。機体の価格も下がり今では個人で購入できるものもあり、気軽に飛ばせるようになったが、まだまだ発展途上であるガジェット。今回はPythonや航空力学などを用いてドローンのフライトコントローラとなる部分を開発し、その経緯を報告する。,False
2017,https://pycon.jp/2017/ja/proposals/vote/110/,実践サーバーレス: 超メンテナンスフリーな「ログ基盤」の作り方 (ja),JX通信社では、ログ収集基盤をサーバーレスアーキテクチャで構築し、約一年半運用しています。サーバーレスアーキテクチャの紹介や、実運用上のトラブルの話、サーバーレスログ基盤の事例紹介、データ可視化環境をDocker上で構築していた事例の紹介などをします。 サーバーレスアーキテクチャに関する知識や、メンテナンスフリーなログ基盤の作り方を学べます。 - サーバーレスアーキテクチャとは何か  - サーバーレスで実現できること  - サーバーレスのメリット・デメリット- ログの収集基盤をどのように構築したか  - ログ収集基盤の設計  - 実運用した上でのトラブル  - 実運用して感じたメリット- ログの集計・可視化基盤  - 可視化基盤をDocker上で運用する方法,False
2017,https://pycon.jp/2017/ja/proposals/vote/143/,Deploying Python web applications in containers using OpenShift. (en),"Containers are taking off as a way for deploying applications. Containers alone are not enough. You need a platform like OpenShift to ease the task of building, deploying and managing them. In this talk you will learn how to deploy Python web applications to OpenShift, a next generation PaaS and CaaS for running applications across a cluster of machines and at scale. Attendees will learn about OpenShift, its relationship to containers, and projects like Kubernetes and Moby (Docker). They will see how OpenShift can be used to deploy front end web applications, as well as back end services, and databases. Container technology has been around for well over a decade. It has only been in the last few years though that containers have become more accessible through easy to use tooling. When deploying applications, containers still aren't enough though. When needing to deploy applications across multiple hosts and at scale, you need an orchestration and scheduling system to help you manage them. This is where Kubernetes comes into the picture.Kubernetes was born out of Google and provides a management platform for deploying applications in containers across a cluster of machines. Although Kubernetes helps to manage containers, it lacks a developer friendly interface for building and deploying an application. To address this, OpenShift has been implemented as a layer around Kubernetes and adds a developer friendly interface for deploying applications.The result in OpenShift, is a container application platform, providing both PaaS (Platform as a Service) and CaaS (Container as a Service) functionality, as well as being an enabling platform for SaaS (Software as a Service).In this talk you will learn about these technologies. It will be demonstrated how you can deploy a Python web application to OpenShift. For your persistent data, you will see how you can also deploy a database in OpenShift and link it to your application, as well as how to mount persistent storage direct into your application.You will also learn about the options you have available for deploying OpenShift as a platform for running your applications, or if you don't want to install and administer the platform, how you can use OpenShift Online, a publicly hosted version of OpenShift.",False
2017,https://pycon.jp/2017/ja/proposals/vote/91/,pythonで始めるビットコイン自動取引システムの開発 (ja),"ビットコインの自動取引システムをpythonで作る方法について学びます。「取引Bot?なんだか難しそう」と思っている方が主な対象です。トーク中にはまず、実際に簡単な取引アルゴリズムを実装し、次にトレード結果をjupyter上で分析、考察して取引アルゴリズムを改良します。一連のフローを通して、Bot開発の進め方、楽しさが伝わればと思います。 - 仮想通貨取引に馴染みがない方でも、自分のBotが作れるようになる 。 - Bot開発のデモを通して、仮説検証プロセスの楽しさを体感する。 - 頑張ってちょっとしたお小遣いを稼げるようになる   ## 発表の流れ### 1) ツールについての説明使用するライブラリについて説明したのち、仮想通貨について、チャートの読み方など、基本的な事項を軽くおさらいします。### 2) Bot作成実践実際にチャートを見て仮説を立てるところからスタートして、取引アルゴリズムをデモ実装します。時間があればサーバーに上げるところまでやれたら良いなと思っています。### 3) 取引データの分析・評価あらかじめデモ実装と同じロジックで取引した結果をpandas, matplotlibで分析、可視化し、取引アルゴリズムを評価します。その後、考察結果をもとにロジックを改良する過程を示します。## こういう人が対象* 取引Botに興味があるが作ったことはない* 仮想通貨に興味がある* お小遣いを増やしたい！## こういう人は対象ではない* プロのトレーダー* 稼げるアルゴリズムを知りたい人",False
2017,https://pycon.jp/2017/ja/proposals/vote/132/,活用 prompt-toolkit: リッチな対話プロンプトの開発 (ja),python-prompt-toolkitというツールをご存知でしょうか。このライブラリはリッチな対話プロンプトツールをサクッと作ることができ、ipythonなどでも利用されています。業務ツールの開発などにも非常に便利で、日頃の不便な処理をサクッと効率化できます。本発表では、ライブコーディングを交えてprompt-toolkitの使い方を解説します。 日頃の不便な作業をサクッと便利にするリッチな対話プロンプトツールが作れるようになる リッチな対話プロンプトが便利なシーンは多くあります。大量のオプションがある場合、helpを見て探すのも一苦労ですが、賢い対話プロンプトがサジェストしてくれれば楽ができます。例えばRDBのSQLを発行するだけでもテーブル名やカラム名を便利に補完してくれると少し楽ができるかもしれません。HTTPのリクエストを送る際にヘッダ情報を補完してくれると便利かもしれません。こういったすでによくあるツールはすでに公開され多くのstarがつけられています。* https://github.com/dbcli/mycli* https://github.com/dbcli/pgcli* https://github.com/eliangcs/http-promptpython-prompt-toolkitはリッチな対話プロンプトツールをサクッと作ることができるライブラリです。そんなときにサクッと便利なCLIツールを用意出来ると、日頃の面倒な処理を効率化することができるでしょう。複雑なオプションも賢く補完してくれることで、利用者の負担は非常に楽になります。本発表では、prompt-toolkitの概要、活用事例、使い方をお話します。また非常に手軽に作れることを知ってもらうためにも、ライブコーディングを交えて15分程度で実際に便利な対話プロンプトツールを作る様子もお見せします。このトークを通して、日頃の不便な作業をサクッと便利にするツールを作ってみてください。,False
2017,https://pycon.jp/2017/ja/proposals/vote/153/,Clearer Code at Scale: Static Types at Zulip and Dropbox (en),"Python now offers static types! Companies like Dropbox and open-source projects like Zulip now use static types (with PEP 484 and mypy) to make Python more productive and fun to work with — in existing codebases from 40k lines to 4 million, in Python 2 and 3, and while preserving the conciseness and flexibility that make Python a great language in the first place. I’ll describe how. 1. ​They’ll know what Python’s static typing (PEP 484) does and how it makes Python programs more fun and productive to work in.2. They’ll know how large companies like Dropbox and open-source projects like Zulip have adopted static typing, and best practices for adopting it in an existing codebase.3. ​Hopefully, they’ll be excited to go try static types out in their own Python programs! ​😃​​ Reading and understanding code is a huge part of what we do as software developers. If we make it easier to understand our codebases, we make everyone more productive and help each other write fewer bugs. This is doubly important for an open-source project: the easier it is to understand your code, the less of a barrier you have for new contributors.That’s why Python now features *optional static types*, described in [PEP 484][1] and implemented with the [mypy type-checker][2]; why Dropbox has adopted static types on 750+ kLOC of its Python codebase and supports a full-time mypy core team; and why Zulip, an open source group chat project, [has adopted it][3] on 100% of its 90 kLOC Python codebase. Dropbox, Zulip, and others have found static types work for making code easier to understand and more fun and productive to work in.Adopting static types in your own code is easier than you might think: you can start on a small piece of a big codebase and start getting the benefits from the beginning. And it works great on Python 2.7 and 3.3+ — both Dropbox and Zulip were entirely on Python 2.7 when they started.In this talk, I’ll share lessons from Zulip’s and Dropbox’s experience — having led the mypy team at Dropbox and working now on the Zulip core team — for how you can start using static types in your own codebases, large or small. We’ll discuss how to make it a seamless part of your project’s tooling; what order to approach things in; and some powerful new tools that make it even easier today to add static types to your Python codebase than ever before.  [1]: https://www.python.org/dev/peps/pep-0484/  [2]: https://github.com/python/mypy  [3]: https://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/",False
2017,https://pycon.jp/2017/ja/proposals/vote/162/,Pythonicで高速・クリーンな深層学習ライブラリ PyTorch (ja),"現在，赤丸人気急上昇中の深層学習ライブラリであるPyTorchの特徴と基本について解説します．老舗ライブラリであるTorchの資産をそのまま利用できる拡張性，CPU計算とGPU計算とほぼ同じコード書くことができるテンソル，高速で効率的な自動微分，そして長大なニューラルネットモデルであっても非常に簡潔にかつPythonicに記述できる気持ちよいシンタックス，これらの特徴をもつライブラリがPyTorchです． 参加者はPyTorchの特徴と基本を学び，自分の深層学習ツールとして利用できる切っ掛けを得ます． 深層学習の爆発的ブームにより，数々の深層学習ライブラリが生まれては消えて行きました．そして2017年現在でメジャーに利用されている深層学習ライブラリは，Googleの[TensorFlow][1]を筆頭に，この分野の草分けであるモントリオール大の[Theano][2], [Caffe][3], FaceBookの[Torch][4], Microsoftの[CNTK][5], AWSが採用した[mxnet][6]，そして日本のプリファード・ネットワークスの[Chainer][7]，というように収斂されてきました．[PyTorch][8]は2016年後半にその一角に突然割り込んできたライブラリです．PyTorchは元々，TorchをPythonで利用するという目的で2012年からひっそりと開発されていましたが，パッとしない存在でした．それがChainerの登場により，全面的に実装を書き換えてました．つまり，Chainerが提唱した”define-by-run”のパラダイムと実装を「パク」ったのです．それがターニングポイントになり，2016年後半から急速にユーザ数を伸ばしてきました．Torchの豊かな深層ニューラルネット資産，ニューラルネットモデルや最適化アルゴリズム，を利用でき，CPUとGPUのコードをほぼ同じように書けるテンソル，更には高速で効率的な自動微分ライブラリ，巨大なニューラルネットモデルであってもクリーンで簡潔にPythonicに書けるシンタックス，これらの特徴を持つライブラリに進化しました．講演者本人も現在一番利用しているのはPyTorchです．コードを気持ちよく速く簡潔に書け，デバグも容易なのに，学習もテストもそこそこ速い，それがPyTorchです．また，クリーンに簡潔に書けるという特徴から研究者が実際の研究に利用していることが多いことがPyTorchの利点の一つとなっています．実際に論文公開と同時に公開されるPyTorchコードが多いと共に，例え研究者がソースコードを公開しなくても，arXivなどのプレプリントサーバで発表され話題になった論文の実装は，有志の手によりPyTorchのコードとして1週間程度でGitHub等で公開されることが多いのです．このためにPyTorchを読めれば研究の最前線にキャッチアップすることが容易になります．本トークでは，PyTorchの特徴と基本について，わかりやすい実例をPythonコードで示しかつ実行しながら，解説いたします．TensorFlowやChainerなどの他の深層学習ライブラリとの比較についても取り上げます．  [1]: https://www.tensorflow.org/  [2]: http://deeplearning.net/software/theano/  [3]: http://caffe.berkeleyvision.org/  [4]: http://torch.ch/  [5]: https://www.microsoft.com/en-us/cognitive-toolkit/  [6]: http://mxnet.io/  [7]: https://chainer.org/  [8]: http://pytorch.org/",False
2017,https://pycon.jp/2017/ja/proposals/vote/90/,Erlang VM上で動くPython風言語『Mochi 2』のご紹介 (ja),近年、並列処理、分散処理を記述しやすく、耐障害性のあるプログラムを作りやすいという理由で、Erlangが注目を集めています。その一方で、Erlang言語のシンタックスは普及している言語(たとえばPythonやJava、Rubyなど）のそれとは大きく異なるため、多くのプログラマーにとってErlangの習得は容易ではないようです。その問題を解消するため、さまざまな言語がErlangのVM上で実現されています(Reia、LFE、Elixir、ErRubyなど）。本発表では、Erlang VM上で動作するPython風言語『Mochi 2』のご紹介と、Mochi 2 とcPythonなどとの連携方法についてお話しします。 - ErlangとMochi 2を知る。- Mochi 2を使って、Erlang VM上で動作するプログラムを作れるようになる。- ErlangとMochi 2とPythonの連携方法を知る。 近年、並列処理、分散処理を記述しやすく、耐障害性のあるプログラムを作りやすいという理由で、Erlangが注目を集めています。その一方で、Erlang言語のシンタックスは普及している言語(たとえばPythonやJava、Rubyなど）のそれとは大きく異なるため、多くのプログラマーにとってErlangの習得は容易ではないようです。その問題を解消するため、さまざまな言語がErlangのVM上で実現されています(Reia、Joxa、LFE、Elixir、ErRubyなど）。しかし、私の知る限り、今をときめくPythonの処理系もしくはPython風言語の処理系は存在していません。そこで、Python風言語をErlangのASTに変換する言語処理系『Mochi 2』を作ることにしました。名前を『Mochi 2』とした理由は、その言語仕様が私が以前から作っているPython風言語『[Mochi][1]』に似ていることと、Mochiという名前が気に入っているからです。本発表では、以下を説明します。- Erlang/OTP、Erlang VMの特徴 - Pythonとの相違を中心にお話しします。- Mochi 2の言語仕様 - PythonやErlangとの相違を中心にお話しします。     - Mochi 2はPythonのサブセット+αにしたいと考えています。     - +αの部分はErlangの持つパターンマッチや軽量プロセスに関する機能です。     - ただし、現状は、Pythonのサブセットになりきっていません。その辺の相違についてもご説明したいと思います。- Mochi 2の処理概要 - クラスとインスタンス、メソッド呼び出し（同期、非同期）、iteratorプロトコルなどの実現方法をお話しします。- Mochi 2とcPythonとPyPyとの連携方法 - Mochi 2で実装されたプログラムとPythonで実装されたプログラムの連携方法をご説明します。＊ Mochi 2はMochiの後継ではありません。Mochiの開発も継続します。＊ Mochiは餅です。餅はモチモチとした弾力性のある食べ物なので、耐障害性のある弾力性のあるプログラムの開発に向いているErlang VM用言語の名前に合っているとも思いました。＊ Mochi 2は既に公開している[Manju][2]とは異なる方法でクラスとインスタンスを実現しており、別リポジトリとして公開予定です。  [1]: https://github.com/i2y/mochi   [2]: https://github.com/i2y/manju,False
2017,https://pycon.jp/2017/ja/proposals/vote/142/,PyCon JP Bot: Python ベースの chatbot が支える PyCon JP (ja),PyCon JP を開催するためにスタッフはSlackを中心にさまざまツールを使ってイベント開催に向けて作業を進めています。このトークでは、日々の作業を楽にしたり、繰り返し作業を自動化するために作成したたさまざまな Python 製の chatbot について、その技術的なポイント紹介します。 slackとの連携する chatbot の作る方法を学びます。また、chatbot からりようできる各種APIとの連携方法やハマりどころについても紹介します。 ## slackと連携したプログラムの作成方法slackにメッセージを送信する方法、slackとやりとりするbotの作成の基本を、Python製のライブラリを使用して解説します。以下の要素を説明します。* Slack API: Incoming Webhooks* Slack API: Bot users* [slacker][4]* [slackbot][5]##  定期実行系bot定期的に実行する作業を、cron で動作するbotプログラムによって効率化します。以下ような機能と要素をコードを交えて解説します。* SNS通知を楽にしたい: Google SpreadsheetとTwitter/Facebook連携* PyCon JP カレンダーのメンテナンス: connpass API と Google Calendar連携* チケットの棚卸し: JIRA API連携* ソースコード: [https://github.com/pyconjp/pyconjp-cron][1]* ソースコード: [https://github.com/pyconjp/jira-issue-report][2]## PyCon JP BotPyCon JP Slackに常駐しているchatbotの各種機能と、その機能をどのように実現しているかをコードを交えて解説します。* 感謝の++したい: O/R Mapper* あいさつとリアクション: slackbot* 割り勘計算したい: SymPy* JIRAのissue探したい: JIRA API* JIRAのissueをまとめて作りたい: JIRA API* Google driveからファイル探したい: Google Drive API* Google Suite のアカウント管理したい: Google Directory API* ソースコード: [https://github.com/pyconjp/pyconjpbot][3]  [1]: https://github.com/os/slacker  [2]: https://github.com/pyconjp/jira-issue-report  [3]: https://github.com/pyconjp/pyconjpbot  [4]: https://github.com/os/slacker  [5]: https://github.com/lins05/slackbot,False
2017,https://pycon.jp/2017/ja/proposals/vote/42/,after VOEZ launch: how to resolve problems of mobile game server development and service maintenance (en),"As startup game makers, many people might be able to build a workable mobile game service, but have no experience in building a stable, reliable, high performance mobile game service due to some reasons such as race condition and lack of caching.We will introduce how we built a commercial game server by showing the essence of Python code and Flask usage from VOEZ game server. ゲームサーバーを作動できたとしても、安定性、高信頼性、高性能を保証するものではありません。私たちもそれに困りました、だからその経験をみんなにシェアしなきゃって思います。このトークで、Python と Google Cloud Platform を使え、(大体)一人で安定、高信頼、高性能なゲームサービスの作り方を教えるつもりです。目的は二つ:1. 安定性、高性能を持つモバイルゲームサービスを構築したいと望むバックエンド初心者に、経験をシェアするつもり。2. 経験があるシニア開発者から私たちの方法の欠点を遠慮なく指摘してもらいたい。私たちはもっと優秀な構築経験を学びたい。 What we will share at below chapter:*  VOEZ game play demo and connection layout* Python code:  * database/storage layout & API for distributing current seasonal event revision and corresponding assets  * Redis database operation  * order of inter-server request and related state transition  * publish event game data to Amazon S3/Google Cloud Storage  * append header ""Cache-Control"" under Flask framework, and make cache expire precisely at seasonal event switching  * calculate how many consecutive days a player logged in* Cloud platform setting tips:  * What happened when we met DDoS* Appendix:  * game play authorization and score uploading",False
2017,https://pycon.jp/2017/ja/proposals/vote/88/,3次元データへのDeepLearningの適用方法 (ja),arXiVやDeepLearningフレームワーク、githubによってDeepLearningにおける技術の取得やノウハウ、コードの取得は容易になってきています。しかしいざ自社のプロダクトとして使いたい場合の戦略やアプローチについて得られる情報は少ないです。本公演では第一部でノウハウ、データ、経験が少ない中で3ヶ月程度でvalidationデータで論文の性能83％を超える85％の性能（40カテゴリ分類）を達成した戦略について第2部では具体的なケースとして3次元物体へのDeepLearning適用についての詳細についてお話します。この公演であなたはExampleケースのDeepLearningしかできない状態から抜け出し、プロダクトへ適用するための確かな一歩が踏み出せることの手助けになる情報を取得できるはずです。 DeepLearningプロダクト作成のためのアプローチ方法の取得、短い期間でDeepLearningプロダクトを作成するための方法の取得 # 第1部 3次元物体へのDeepLearning適用のための戦略## 情報源の取得 今は情報源が多くあり、選択基準が定まっていないと適切な情報が得られない状態になっています。私が実際に行っている情報収集の方法をシェアすることで情報選択の基準や方法の参考になると思います。## 学習データの取得DeepLearningの学習データは公開されているものもあるので私が実際に行ったデータの探し方をシェアいたします。##  小さく始める 大きなデータセットでいきなり学習を初めて数日後に悪い結果でしたとなると目も当てられません。ここでは具体的にどのようなアプローチで小さなデータで始めたかをお伝えします。## 注力する方向を決めるDeepLearningはやるべきことが多いです。不確実性の多いなかですべてのことはできないのである程度の割り切りが必要になります。私が行った取捨選択のアプローチをお伝えします。# 第2部 3次元データへのDeepLearning適用## Deep Learning で制御できるものDeep Learningは結果が精度の出るまで分からないものが多いですが精度の結果が出る前に分かることがあります。その点をシェアします。## 挑戦する回数を増やす機械学習は失敗する回数が多いです。失敗の数を増やせることが成功への近道になります。失敗の数を増やすために行ったアプローチをシェアします。## パラメータチューニング精度向上においてモデルの汎用性を向上とデータの汎用性を向上にお伝えします。## プロダクトここまで来るとプロダクト化に入ります。現行の機能に影響を及ぼしにくくアップデートしやすい仕組みについてお伝えします。,False
2017,https://pycon.jp/2017/ja/proposals/vote/69/,PyLadiesへ参加しよう〜hack-a-thon結果報告からPyLadies Tokyoを知る (ja),日本のPyLadies拠点の説明や活動概要をお話しします。6月に行ったPyLadies Hack-a-thonで作成したシステム(国際政治ニュースを楽しく読もう！ギャル語ニュース)の発表メインに行います。「女性しか参加できないイベントってどんな雰囲気でやっているんだろう？」と興味ある男性の方もぜひご参加ください。 PyLadies という女性Pythonistaコミュニティの意義と活動内容について知ることができます。 日本のPyLadies拠点の説明や活動概要をお話しします。   本セッションはPyLadies Tokyoスタッフの一人である @maaya8585 を中心に、hack-a-thon参加者4名で発表をする予定です。## PyLadies についてPyLadies Tokyoの設立や活動方針の説明を写真を使いながら説明します。- PyLadies とは- PyLadies Japan活動拠点について- PyLadies Tokyoについて- PyLadies Tokyo 過去イベントの雰囲気## Hack-a-thon成果発表2017年6月7月に行うPyLadies Hack-a-thonで作成したシステムの発表を行います。   Webシステムフロントエンドからバックエンド、分析システムまで全てPyLadiesメンバーで協力して作りました。本システムのアーキテクチャ・利用技術・苦労したところetc. 話したいと思います。###「国際政治ニュースを楽しく読もう！ギャル語ニュース」Chromeから国際政治ニュースのページ(現在は[産経ニュース](http://www.sankei.com/world/world.html)のみ)にアクセスすると自動で記事がギャル語に書き換わる自作Chromeアプリシステムです。- 使った技術    - MeCab    - Tornado    - Azure         - Data Sience Virtual Machine (Ubuntu)        - Redis Cache        - (Functions)     - JavaScript(Chromeアプリ作成) ,False
2017,https://pycon.jp/2017/ja/proposals/vote/81/,PythonとHadoopで作るデータ分析環境 (ja),データ処理に関わるツールはPythonで実装されていることが多いですが、HadoopエコシステムはJVM上で動作するものが多いためPythonからの接続が問題になることが多いです。このTalkではHadoopエコシステムを中心としたデータ分析環境においてPythonを利用した場合におきる問題とその対策をお伝えします。 データエンジニアが抱える課題をPythonを使ってどのように解決するのか?とくに、「PythonからHDFS(secure mode)に接続するときに起きる問題とその対策」と「データ処理を並列化するためにコンテナをどう使えばよいか」について理解が深まります。 株式会社サイバーエージェント アドテクスタジオでは、マルチテナント化されたHadoopを中心としたデータ分析環境を構築おり、分析環境を利用するためのCLIツールやワークフロー管理など多くのシステムでPythonを利用しています。このtalkでは、データ分析環境の構築に際して生じるデータに関わる問題の解決にむけてPythonをどのように利用したかを事例を交えながら解説します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/37/,PythonでOAuth『サーバ』を構築した話 (ja),"TwitterやFacebookでもおなじみ、認可機能であるOAuth。  携わったことがある方も多いと思いますが、ほとんどはクライアントであって、認可を行うサーバ側を実際に実装した方は少ないのではないでしょうか。  今回のトークではOAuthとは何ぞやというところから、Pythonを使ってOAuthサーバを構築した方法や注意点、工夫した箇所を話します。 Pythonを使ったWebアプリケーション、WebAPIの実装方法。OAuthの国際基準。クライアント、サーバ問わずOAuthを実装する方法を知ることができる。 １．OAuthについての説明  ２．PythonでOAuthを実現する方法  ３．OAuth認可トークンの使い方  端的に言うと上記3つを説明するトークとなります。  １については2,3を理解しやすくするための前知識として話します。ググれば出てくる情報になりますが、要点をかいつまんで、以降の説明を頭に入りやすいようにします。  2についてはPython、及び関連するフレームワーク、ライブラリを利用して実装する方法、注意点を説明します。  3については発行したOAuthトークンの利用方法を具体的に説明します。",False
2017,https://pycon.jp/2017/ja/proposals/vote/43/,Pythonではじめる数理最適化-ケーススタディを通して- (ja),数理最適化の歴史は古く、George Dantzigが線形計画問題に対する解法として単体法を発明したのは1947年。理論とマシンパワーの進歩とともに多くの実用的な問題が解けるようになり、最近ではPythonをインターフェースとした数理最適化ソルバーが登場し、広くエンジニアが利用できる技術の１つになりました。今年はDantzigの革命的な発明から70周年、この機会に数理最適化をはじめてみませんか？ 聴講者の方に①数理モデリングの考え方、②数理最適化問題とは何か、③数理最適化問題をPythonで解く方法、④数理最適化の応用事例、をお伝えします。聴講後には数理最適化をはじめてみたくなるはず！ 数理最適化の入門とケーススタディについてお話します。前半の入門編では数理モデリングの考え方から数理最適化の解説、およびPythonライブラリPuLPを使って数理最適化問題を解く方法を紹介します。後半のケーススタディ編ではRettyにおける数理最適化技術の応用事例を紹介します。具体的には現実の問題を0-1整数計画問題に落とし込み、ロジックの要件に合わせて柔軟に数理モデルを改善していくプロセスをお話します。,False
2017,https://pycon.jp/2017/ja/proposals/vote/165/,async/awaitとasyncioによる非同期処理 (ja),Python3.6の登場で非同期処理に関する機能は概ね整ってきました。このセッションではPythonが持つ非同期処理の機能について解説していきます。 非同期処理の書き方について知る Python3.5でasync/awaitの構文、Python 3.6で非同期内包表記や非同期ジェネレータが追加され、非同期周りの機能が充実してきました。今後、追加/変更の予定がありそうなものは、私が知る限り次の2つです。* https://www.python.org/dev/peps/pep-0492/#async-lambda-functions* https://www.python.org/dev/peps/pep-0525/#aiter-and-anext-builtinsasyncioの勉強をしていると、コルーチン(Coroutine)とタスク(Task)、Futureなど似たような単語が並び概念をつかむのが難しいものです。一方でそれらの用語を知っておくとスムーズに理解できる点も多くあります。それらの用語や要点を押さえながら非同期処理の機能について覚えていきましょう。,False
2017,https://pycon.jp/2017/ja/proposals/vote/106/,Sharding with SQLAlchemy (ja),"マルチテナント型のサービスを開発・運用する際に使われるテクニックの一つとして、データベースの Sharding があります。本トークでは、Pythonの代表的なORMであるSQLAlchemyを使って、単一のアプリケーションインスタンスから、Sharding されたデータベースへのアクセスを実現するテクニックを説明します。 SQLAlchemyを使用してMaster/Slave 型や Sharding されたDBを扱うための実践的なテクニック マルチテナント型のサービスを開発・運用する際に使われるテクニックの一つとして、データベースの Sharding があります。マルチテナント型のサービスを運用する方式としては、テナント毎に完全にアプリケーションインスタンスを分割するようなものから、単一のアプリケーションインスタンスで全てのテナントをカバーするようなものまで多数の方式があります。その中でもデータベースをどのように配置するかは、アーキテクチャ設計の上で常に開発者を悩ませるポイントの一つです。本トークでは、このような状況下でよく利用される以下のようなDB配置パターンに対して Python の代表的なORMの一つである SQLAlchemy を活用するテクニックをご紹介します。* 書き込み系と読み込み系のDBインスタンスを分離するパターン (master - slave)*  テナント毎にDBのスキーマやインスタンスを分離するパターン (sharding)トーク中では、 Engine, Pool, Connection といったSQLAlachemy の core 機能だけではなく、DB設計パターンに合わせて、declarative base や autoload をどうするかといった、ORM側の機能にも触れます。",False
2017,https://pycon.jp/2017/ja/proposals/vote/119/, Find the Farm (Data Science Insights into Real Estate Pricing) (en),"Real estate transactions are geographically and temporally sparse. There is often both a listing and a selling agent. Pricing models typically rely on physical parameters; there has been little work done in assessing the contribution of the realtor. A realtor 'farm' may be discoverable by cluster identification, and analyzed for negotiation strength in listing and sales prices.  This talk is for anyone interested in simple yet novel approaches to data science. Relatively little Python knowledge is expected, as the work consists of importing modules, plotting, comprehensions, and leveraging simple data science concepts. It should be a great intro to the person who would like a concrete approach to data exploration. Home ownership is the largest transaction, yet there is little data available concerning the effectiveness of realtors. The audience will have a start on utilizing geographic tools for discovering relationships that might not be otherwise obvious. Using gmplot, geopy, and Python data science tools we'll discover realtor farms, and assess the characteristics of sales vs listing price. Real estate transactions tend to be geographically sparse and temporally rare. There is often both a listing and a selling agent in the representing a given property. The sales price is determined by a number of factor. While there has been considerable interest in building pricing models relying on physical parameters, there has been little work done in assessing the contribution of the realtor. The discovery of a 'farm' uses cluster identification methods. These farms can then be analyzed for imputed listing prices and the sales price, both of which are negotiated.The problem: Most real estate analytics deal only with property description and location. Markets can swing quickly from buyer's to seller's advantage, so timing and days on market is important. Agent effects are not well understood and can be a significant factor in determining the actual price. Data source are examined . Python Modules utilized. Application of data science, e.g. modules pycluster, pyclustering, scikit-learn. (the talk is primarily application, not theory)Examples of geographic and hidden affinity Analysis of listing price to appraisal and listing agent effect Analysis of over/under-performance of sales price to listing price Determination of listing agent vs selling agent negotiation skills. Effect of dual agency on pricing. Effect of listing agent Farms on neighborhood pricing.Consideration as a Machine Learning project using Theano or TensorFlow , Keras, Sonnet tflearnConclusions and future directions Questionsdata, code, notebooks, and graphics will be includedThe methodology presented is likely applicable to other low-volume high-value facilitated transactions.",False
2017,https://pycon.jp/2017/ja/proposals/vote/21/,Djangoフレームワークのユーザーモデルと認証 (ja),Djangoフレームワークにはユーザーモデルと認証の仕組みが組み込まれています。これらの紹介とカスタマイズのポイント、ハマりどころなどについて話します。 Djangoフレームワークの認証の仕組み、カスタマイズ方法 Webアプリケーションでは、利用者の認証してアプリケーションを利用させる仕組みが広く使われています。Djangoフレームワークを利用してアプリケーションを開発する際は、利用者の情報は認証フレームワーク(django.contrib.auth)を利用します。この認証フレームワークはすぐに利用可能なものですが、次のようなカスタマイズもできます。- 保存しておく利用者情報の追加(Userモデルの変更、拡張)- 認証アルゴリズムの変更,False
2017,https://pycon.jp/2017/ja/proposals/vote/161/,クイズ・python勝ち抜きバトル!! (ja),pythonの言語仕様、標準ライブラリに対して新しい発見が得られます!!現在、機械学習、Web、データ分析など、様々な分野で活用されていますが、もともとのpythonの魅力は、「Batteries Included」と言われており、言語仕様や標準ライブラリの先進性が評価されていました。このセッションは、言語仕様、標準ライブラリに対する3択形式のクイズに挑戦する参加型のセッションです。そして!! 勝者には素敵なプレゼントもあるよ!! 現在、pythonは、機械学習、Web、データ分析など、様々な分野で活用されています。一方で、抽象化度の高いライブラリの充実と共に、ツールとしての利用者が増え、真のpythonの魅力に気がつくことができていない人も生まれています。このセッションでは、単なるツールとしてではなくプログラミング言語pythonとしての新しい魅力を伝えたいと思います。 ## クイズ・python勝ち抜きバトル!!このセッションでは、pythonの言語仕様、標準ライブラリに対するクイズを、初級~上級まで、勝ち抜き形式で行う、参加型のセッションです。## どんなクイズ?例えば...* [初級] lambda関数の書き方として適切なものは?* [中級] 次のうち、偽と評価されるものは?* [上級] 次のうち、メモリ使用量が最も高いものは?このようなクイズに、3択形式で勝ち抜き戦を行います。## 何が得られるの?機械学習、Web、データ分析など、様々な分野で活用されていますが、もともとのpythonの魅力は、「Batteries Included」と言われており、言語仕様や標準ライブラリの先進性が評価されてきました。このセッションをきっかけに、ライブラリを組み合わせることを主体としたツールとしての利用を脱却し、真にpythonの魅力に気がつくことが出来ます。そして!! 勝者には素敵なプレゼントもあるよ!!,False
